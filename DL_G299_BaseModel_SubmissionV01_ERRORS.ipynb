{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_history \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbilba\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDL_Small\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mhistory.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m train_behaviors \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbilba\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDL_Small\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbehaviors.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marticle_id\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      4\u001b[0m val_history \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbilba\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDL_Small\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mhistory.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "train_history = pd.read_parquet(r\"C:\\Users\\bilba\\Downloads\\DL_Small\\history.parquet\", engine='pyarrow')\n",
    "train_behaviors = pd.read_parquet(r\"C:\\Users\\bilba\\Downloads\\DL_Small\\behaviors.parquet\", engine='pyarrow').dropna(subset=[\"article_id\"])\n",
    "\n",
    "val_history = pd.read_parquet(r\"C:\\Users\\bilba\\Downloads\\DL_Small\\validation\\history.parquet\", engine='pyarrow')\n",
    "val_behaviors = pd.read_parquet(r\"C:\\Users\\bilba\\Downloads\\DL_Small\\validation\\behaviors.parquet\", engine='pyarrow').dropna(subset=[\"article_id\"])\n",
    "\n",
    "articles_ = pd.read_parquet(r\"C:\\Users\\bilba\\Downloads\\DL_Small\\articles.parquet\", engine='pyarrow')\n",
    "\n",
    "word2vec_file = pd.read_parquet(r\"C:\\Users\\bilba\\Downloads\\Ekstra_Bladet_word2vec\\document_vector.parquet\", engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_path : ~/ebnerd_data\n",
      "seed : 42\n",
      "datasplit : train_test_split\n",
      "debug : False\n",
      "bs_train : 32\n",
      "bs_test : 32\n",
      "batch_size_test_wo_b : 32\n",
      "batch_size_test_w_b : 4\n",
      "history_size : 20\n",
      "npratio : 4\n",
      "epochs : 10\n",
      "train_fraction : 0.8\n",
      "fraction_test : 0.2\n",
      "nrms_loader : NRMSDataLoaderPretransform\n",
      "n_chunks_test : 10\n",
      "chunks_done : 0\n",
      "transformer_model_name : bert-base-uncased\n",
      "max_title_length : 30\n",
      "head_num : 20\n",
      "head_dim : 20\n",
      "attention_hidden_dim : 200\n",
      "optimizer : adam\n",
      "loss : cross_entropy_loss\n",
      "dropout : 0.2\n",
      "learning_rate : 0.0001\n",
      "Initiating articles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title_size: 30\n",
      "history_size: 20\n",
      "head_num: 20\n",
      "head_dim: 20\n",
      "attention_hidden_dim: 200\n",
      "optimizer: adam\n",
      "loss: cross_entropy_loss\n",
      "dropout: 0.2\n",
      "learning_rate: 0.0001\n",
      "newsencoder_units_per_layer: None\n",
      "newsencoder_l2_regularization: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bilba\\AppData\\Local\\Temp\\ipykernel_10028\\233790892.py:344: DeprecationWarning: `DataFrame.with_row_count` is deprecated. Use `with_row_index` instead. Note that the default column name has changed from 'row_nr' to 'index'.\n",
      "  df = df.with_row_count(name=GROUPBY_ID)\n",
      "C:\\Users\\bilba\\AppData\\Local\\Temp\\ipykernel_10028\\2511132047.py:670: DeprecationWarning: `DataFrame.with_row_count` is deprecated. Use `with_row_index` instead. Note that the default column name has changed from 'row_nr' to 'index'.\n",
      "  df = df.with_row_count(GROUPBY_ID)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating training-dataloader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bilba\\AppData\\Local\\Temp\\ipykernel_10028\\1678632858.py:108: DeprecationWarning: The `default` parameter for `replace` is deprecated. Use `replace_strict` instead to set a default while replacing values.\n",
      "  .with_columns(pl.col(behaviors_column).replace(mapping, default=None))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, None, 30)]   0           []                               \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 20, 30)]     0           []                               \n",
      "                                                                                                  \n",
      " time_distributed_3 (TimeDistri  (None, None, 400)   10040400    ['input_7[0][0]']                \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      " user_encoder (Functional)      (None, 400)          10600800    ['input_6[0][0]']                \n",
      "                                                                                                  \n",
      " dot_2 (Dot)                    (None, None)         0           ['time_distributed_3[0][0]',     \n",
      "                                                                  'user_encoder[0][0]']           \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, None)         0           ['dot_2[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 10,600,800\n",
      "Trainable params: 10,600,800\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... \n",
      "Layer AttLayer2 has arguments ['dim', 'seed']\n",
      "in `__init__` and therefore must override `get_config()`.\n",
      "\n",
      "Example:\n",
      "\n",
      "class CustomLayer(keras.layers.Layer):\n",
      "    def __init__(self, arg1, arg2):\n",
      "        super().__init__()\n",
      "        self.arg1 = arg1\n",
      "        self.arg2 = arg2\n",
      "\n",
      "    def get_config(self):\n",
      "        config = super().get_config()\n",
      "        config.update({\n",
      "            \"arg1\": self.arg1,\n",
      "            \"arg2\": self.arg2,\n",
      "        })\n",
      "        return config\n",
      "Epoch 1/10\n",
      " 861/9707 [=>............................] - ETA: 7:37 - loss: 1.5040 - auc: 0.6472"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 221\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitiating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, start training...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;66;03m# =>\u001b[39;00m\n\u001b[1;32m--> 221\u001b[0m hist \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_WEIGHTS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    229\u001b[0m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mload_weights(MODEL_WEIGHTS)\n",
      "File \u001b[1;32mc:\\Users\\bilba\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\bilba\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1570\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1568\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs\n\u001b[0;32m   1569\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[1;32m-> 1570\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[0;32m   1572\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bilba\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:470\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \n\u001b[0;32m    465\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 470\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bilba\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:317\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 317\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\bilba\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:340\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    337\u001b[0m     batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 340\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    343\u001b[0m     end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[1;32mc:\\Users\\bilba\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:388\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m    387\u001b[0m     hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> 388\u001b[0m     \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[1;32mc:\\Users\\bilba\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:1081\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1081\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bilba\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:1157\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1156\u001b[0m     \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\bilba\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\tf_utils.py:635\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m--> 635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bilba\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mc:\\Users\\bilba\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mc:\\Users\\bilba\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\tf_utils.py:628\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 628\u001b[0m         t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;66;03m# as-is.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
      "File \u001b[1;32mc:\\Users\\bilba\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1157\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \n\u001b[0;32m   1136\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[1;32mc:\\Users\\bilba\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1123\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1122\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1124\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#examples/reproducibility_scripts/ebnerd_nrms.py\n",
    "\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import datetime as dt\n",
    "import polars as pl\n",
    "import shutil\n",
    "import gc\n",
    "import os\n",
    "import pyarrow\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "args = get_args()\n",
    "\n",
    "for arg, val in vars(args).items():\n",
    "    print(f\"{arg} : {val}\")\n",
    "\n",
    "PATH = Path(args.data_path).expanduser()\n",
    "# Access arguments as variables\n",
    "SEED = args.seed\n",
    "DATASPLIT = args.datasplit\n",
    "DEBUG = args.debug\n",
    "BS_TRAIN = args.bs_train\n",
    "BS_TEST = args.bs_test\n",
    "BATCH_SIZE_TEST_WO_B = args.batch_size_test_wo_b\n",
    "BATCH_SIZE_TEST_W_B = args.batch_size_test_w_b\n",
    "HISTORY_SIZE = args.history_size\n",
    "NPRATIO = args.npratio\n",
    "EPOCHS = args.epochs\n",
    "TRAIN_FRACTION = args.train_fraction if not DEBUG else 0.0001\n",
    "FRACTION_TEST = args.fraction_test if not DEBUG else 0.0001\n",
    "\n",
    "NRMSLoader_training = (\n",
    "    NRMSDataLoaderPretransform\n",
    "    if args.nrms_loader == \"NRMSDataLoaderPretransform\"\n",
    "    else NRMSDataLoader\n",
    ")\n",
    "\n",
    "# =====================================================================================\n",
    "#  ############################# UNIQUE FOR NRMSModel ################################\n",
    "# =====================================================================================\n",
    "\n",
    "# Model in use:\n",
    "model_func = NRMSModel\n",
    "hparams = hparams_nrms\n",
    "\n",
    "## NRMSModel:\n",
    "TEXT_COLUMNS_TO_USE = [DEFAULT_TITLE_COL, DEFAULT_SUBTITLE_COL, DEFAULT_BODY_COL]\n",
    "\n",
    "TRANSFORMER_MODEL_NAME = args.transformer_model_name\n",
    "MAX_TITLE_LENGTH = args.max_title_length\n",
    "hparams.title_size = MAX_TITLE_LENGTH\n",
    "hparams.history_size = args.history_size\n",
    "hparams.head_num = args.head_num\n",
    "hparams.head_dim = args.head_dim\n",
    "hparams.attention_hidden_dim = args.attention_hidden_dim\n",
    "hparams.optimizer = args.optimizer\n",
    "hparams.loss = args.loss\n",
    "hparams.dropout = args.dropout\n",
    "hparams.learning_rate = args.learning_rate\n",
    "\n",
    "#\n",
    "hparams.newsencoder_units_per_layer = None  # [400, 400, 400]\n",
    "\n",
    "# =============\n",
    "print(\"Initiating articles...\")\n",
    "df_articles = pl.read_parquet(r\"C:\\Users\\bilba\\Downloads\\DL_Small\\articles.parquet\")\n",
    "\n",
    "# LOAD HUGGINGFACE:\n",
    "transformer_model = TFAutoModel.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "transformer_tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "\n",
    "word2vec_embedding = get_transformers_word_embeddings(transformer_model)\n",
    "#\n",
    "df_articles, cat_cal = concat_str_columns(df_articles, columns=TEXT_COLUMNS_TO_USE)\n",
    "df_articles, token_col_title = convert_text2encoding_with_transformers(\n",
    "    df_articles, transformer_tokenizer, cat_cal, max_length=MAX_TITLE_LENGTH\n",
    ")\n",
    "article_mapping = create_article_id_to_value_mapping(\n",
    "    df=df_articles, value_col=token_col_title\n",
    ")\n",
    "\n",
    "# =====================================================================================\n",
    "#  ############################# UNIQUE FOR NRMSDocVec ###############################\n",
    "# =====================================================================================\n",
    "\n",
    "print_hparams(hparams)\n",
    "\n",
    "# Dump paths:\n",
    "DUMP_DIR = Path(\"ebnerd_predictions\")\n",
    "DUMP_DIR.mkdir(exist_ok=True, parents=True)\n",
    "#\n",
    "DT_NOW = dt.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")  # Sanitize the timestamp\n",
    "#\n",
    "MODEL_NAME = model_func.__name__\n",
    "MODEL_OUTPUT_NAME = f\"{MODEL_NAME}-{DT_NOW}\"\n",
    "#\n",
    "ARTIFACT_DIR = DUMP_DIR.joinpath(\"test_predictions\", MODEL_OUTPUT_NAME)\n",
    "# Model monitoring:\n",
    "MODEL_WEIGHTS = DUMP_DIR.joinpath(f\"state_dict/{MODEL_OUTPUT_NAME}/weights\")\n",
    "LOG_DIR = DUMP_DIR.joinpath(f\"runs/{MODEL_OUTPUT_NAME}\")\n",
    "# Evaluating the test test can be memory intensive, we'll chunk it up:\n",
    "TEST_CHUNKS_DIR = ARTIFACT_DIR.joinpath(\"test_chunks\")\n",
    "TEST_CHUNKS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "N_CHUNKS_TEST = args.n_chunks_test\n",
    "CHUNKS_DONE = args.chunks_done  # if it crashes, you can start from here.\n",
    "# Just trying keeping the dataframe slime:\n",
    "COLUMNS = [\n",
    "    DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_USER_COL,\n",
    "]\n",
    "# Store hparams\n",
    "write_json_file(\n",
    "    hparams_to_dict(hparams),\n",
    "    ARTIFACT_DIR.joinpath(f\"{MODEL_NAME}_hparams.json\"),\n",
    ")\n",
    "write_json_file(vars(args), ARTIFACT_DIR.joinpath(f\"{MODEL_NAME}_argparser.json\"))\n",
    "\n",
    "# =====================================================================================\n",
    "# We'll use the training + validation sets for training.\n",
    "df = (\n",
    "    pl.concat(\n",
    "        [\n",
    "            ebnerd_from_path(\n",
    "                PATH.joinpath(DATASPLIT, \"train\"),\n",
    "                history_size=HISTORY_SIZE,\n",
    "                padding=0,\n",
    "            ),\n",
    "            ebnerd_from_path(\n",
    "                PATH.joinpath(DATASPLIT, \"validation\"),\n",
    "                history_size=HISTORY_SIZE,\n",
    "                padding=0,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    .sample(fraction=TRAIN_FRACTION, shuffle=True, seed=SEED)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(\n",
    "        sampling_strategy_wu2019,\n",
    "        npratio=NPRATIO,\n",
    "        shuffle=True,\n",
    "        with_replacement=True,\n",
    "        seed=SEED,\n",
    "    )\n",
    "    .pipe(create_binary_labels_column)\n",
    ")\n",
    "\n",
    "# We keep the last day of our training data as the validation set.\n",
    "last_dt = df[DEFAULT_IMPRESSION_TIMESTAMP_COL].dt.date().max() - dt.timedelta(days=1)\n",
    "df_train = df.filter(pl.col(DEFAULT_IMPRESSION_TIMESTAMP_COL).dt.date() < last_dt)\n",
    "df_validation = df.filter(pl.col(DEFAULT_IMPRESSION_TIMESTAMP_COL).dt.date() >= last_dt)\n",
    "\n",
    "# =====================================================================================\n",
    "print(f\"Initiating training-dataloader\")\n",
    "train_dataloader = NRMSLoader_training(\n",
    "    behaviors=df_train,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=False,\n",
    "    batch_size=BS_TRAIN,\n",
    ")\n",
    "\n",
    "val_dataloader = NRMSLoader_training(\n",
    "    behaviors=df_validation,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=False,\n",
    "    batch_size=BS_TRAIN,\n",
    ")\n",
    "\n",
    "# =====================================================================================\n",
    "# CALLBACKS\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=LOG_DIR,\n",
    "    histogram_freq=1,\n",
    ")\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_auc\",\n",
    "    mode=\"max\",\n",
    "    patience=4,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "modelcheckpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=MODEL_WEIGHTS,\n",
    "    monitor=\"val_auc\",\n",
    "    mode=\"max\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    verbose=1,\n",
    ")\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_auc\",\n",
    "    mode=\"max\",\n",
    "    factor=0.2,\n",
    "    patience=2,\n",
    "    min_lr=1e-6,\n",
    ")\n",
    "callbacks = [tensorboard_callback, early_stopping, modelcheckpoint, lr_scheduler]\n",
    "\n",
    "# =====================================================================================\n",
    "model = model_func(\n",
    "    hparams=hparams,\n",
    "    seed=42,\n",
    ")\n",
    "model.model.compile(\n",
    "    optimizer=model.model.optimizer,\n",
    "    loss=model.model.loss,\n",
    "    metrics=[\"AUC\"],\n",
    ")\n",
    "print(model.model.summary())\n",
    "f\"Initiating {MODEL_NAME}, start training...\"\n",
    "# =>\n",
    "hist = model.model.fit(\n",
    "    train_dataloader,\n",
    "    validation_data=val_dataloader,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "print(f\"loading model: {MODEL_WEIGHTS}\")\n",
    "model.model.load_weights(MODEL_WEIGHTS)\n",
    "\n",
    "# =====================================================================================\n",
    "print(\"Initiating testset...\")\n",
    "df_test = (\n",
    "    ebnerd_from_path(\n",
    "        PATH.joinpath(\"ebnerd_testset\", \"test\"),\n",
    "        history_size=HISTORY_SIZE,\n",
    "        padding=0,\n",
    "    )\n",
    "    .sample(fraction=FRACTION_TEST)\n",
    "    .with_columns(\n",
    "        pl.col(DEFAULT_INVIEW_ARTICLES_COL)\n",
    "        .list.first()\n",
    "        .alias(DEFAULT_CLICKED_ARTICLES_COL)\n",
    "    )\n",
    "    .select(COLUMNS + [DEFAULT_IS_BEYOND_ACCURACY_COL])\n",
    "    .with_columns(\n",
    "        pl.col(DEFAULT_INVIEW_ARTICLES_COL)\n",
    "        .list.eval(pl.element() * 0)\n",
    "        .alias(DEFAULT_LABELS_COL)\n",
    "    )\n",
    ")\n",
    "# Split test in beyond-accuracy TRUE / FALSE. In the BA 'article_ids_inview' is 250.\n",
    "df_test_wo_beyond = df_test.filter(~pl.col(DEFAULT_IS_BEYOND_ACCURACY_COL))\n",
    "df_test_w_beyond = df_test.filter(pl.col(DEFAULT_IS_BEYOND_ACCURACY_COL))\n",
    "\n",
    "df_test_chunks = split_df_chunks(df_test_wo_beyond, n_chunks=N_CHUNKS_TEST)\n",
    "df_pred_test_wo_beyond = []\n",
    "print(\"Initiating testset without beyond-accuracy...\")\n",
    "for i, df_test_chunk in enumerate(df_test_chunks[CHUNKS_DONE:], start=1 + CHUNKS_DONE):\n",
    "    print(f\"Test chunk: {i}/{len(df_test_chunks)}\")\n",
    "    # Initialize DataLoader\n",
    "    test_dataloader_wo_b = NRMSDataLoader(\n",
    "        behaviors=df_test_chunk,\n",
    "        article_dict=article_mapping,\n",
    "        unknown_representation=\"zeros\",\n",
    "        history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "        eval_mode=True,\n",
    "        batch_size=BATCH_SIZE_TEST_WO_B,\n",
    "    )\n",
    "    # Predict and clear session\n",
    "    scores = model.scorer.predict(test_dataloader_wo_b)\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # Process the predictions\n",
    "    df_test_chunk = add_prediction_scores(df_test_chunk, scores.tolist()).with_columns(\n",
    "        pl.col(\"scores\")\n",
    "        .map_elements(lambda x: list(rank_predictions_by_score(x)))\n",
    "        .alias(\"ranked_scores\")\n",
    "    )\n",
    "\n",
    "    # Save the processed chunk\n",
    "    df_test_chunk.select(DEFAULT_IMPRESSION_ID_COL, \"ranked_scores\").write_parquet(\n",
    "        TEST_CHUNKS_DIR.joinpath(f\"pred_wo_ba_{i}.parquet\")\n",
    "    )\n",
    "\n",
    "    # Append and clean up\n",
    "    df_pred_test_wo_beyond.append(df_test_chunk)\n",
    "\n",
    "    # Cleanup\n",
    "    del df_test_chunk, test_dataloader_wo_b, scores\n",
    "    gc.collect()\n",
    "\n",
    "df_pred_test_wo_beyond = pl.concat(df_pred_test_wo_beyond)\n",
    "df_pred_test_wo_beyond.select(DEFAULT_IMPRESSION_ID_COL, \"ranked_scores\").write_parquet(\n",
    "    TEST_CHUNKS_DIR.joinpath(\"pred_wo_ba.parquet\")\n",
    ")\n",
    "# =====================================================================================\n",
    "print(\"Initiating testset with beyond-accuracy...\")\n",
    "test_dataloader_w_b = NRMSDataLoader(\n",
    "    behaviors=df_test_w_beyond,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=True,\n",
    "    batch_size=BATCH_SIZE_TEST_W_B,\n",
    ")\n",
    "scores = model.scorer.predict(test_dataloader_w_b)\n",
    "df_pred_test_w_beyond = add_prediction_scores(\n",
    "    df_test_w_beyond, scores.tolist()\n",
    ").with_columns(\n",
    "    pl.col(\"scores\")\n",
    "    .map_elements(lambda x: list(rank_predictions_by_score(x)))\n",
    "    .alias(\"ranked_scores\")\n",
    ")\n",
    "df_pred_test_w_beyond.select(DEFAULT_IMPRESSION_ID_COL, \"ranked_scores\").write_parquet(\n",
    "    TEST_CHUNKS_DIR.joinpath(\"pred_w_ba.parquet\")\n",
    ")\n",
    "\n",
    "# =====================================================================================\n",
    "print(\"Saving prediction results...\")\n",
    "df_test = pl.concat([df_pred_test_wo_beyond, df_pred_test_w_beyond])\n",
    "df_test.select(DEFAULT_IMPRESSION_ID_COL, \"ranked_scores\").write_parquet(\n",
    "    ARTIFACT_DIR.joinpath(\"test_predictions.parquet\")\n",
    ")\n",
    "\n",
    "if TEST_CHUNKS_DIR.exists() and TEST_CHUNKS_DIR.is_dir():\n",
    "    shutil.rmtree(TEST_CHUNKS_DIR)\n",
    "\n",
    "write_submission_file(\n",
    "    impression_ids=df_test[DEFAULT_IMPRESSION_ID_COL],\n",
    "    prediction_scores=df_test[\"ranked_scores\"],\n",
    "    path=ARTIFACT_DIR.joinpath(\"predictions.txt\"),\n",
    "    filename_zip=f\"{MODEL_NAME}-{SEED}-{DATASPLIT}.zip\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#examples/reproducibility_scripts/args_nrms.py\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "sys.argv = [\n",
    "    \"--data_path\", \"path/to/data\",\n",
    "    \"--datasplit\", \"train_test_split\",\n",
    "    \"--transformer_model_name\", \"bert-base-uncased\",\n",
    "    \"--seed\", \"42\",\n",
    "    \"--bs_train\", \"32\",\n",
    "    \"--bs_test\", \"32\",\n",
    "    \"--epochs\", \"10\",\n",
    "    \"--fraction_test\", \"0.2\",\n",
    "    \"--train_fraction\", \"0.8\",\n",
    "    # Add any other arguments you need\n",
    "]\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Argument parser for NRMSModel training\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--data_path\",\n",
    "        type=str,\n",
    "        default=str(\"~/ebnerd_data\"),\n",
    "        help=\"Path to the data directory\",\n",
    "    )\n",
    "\n",
    "    # General settings\n",
    "    parser.add_argument(\"--seed\", type=int, default=123, help=\"Random seed\")\n",
    "    parser.add_argument(\n",
    "        \"--datasplit\", type=str, default=\"ebnerd_small\", help=\"Dataset split to use\"\n",
    "    )\n",
    "    parser.add_argument(\"--debug\", action=\"store_true\", help=\"Enable debug mode\")\n",
    "\n",
    "    # Batch sizes\n",
    "    parser.add_argument(\n",
    "        \"--bs_train\", type=int, default=32, help=\"Batch size for training\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--bs_test\", type=int, default=32, help=\"Batch size for testing\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch_size_test_wo_b\",\n",
    "        type=int,\n",
    "        default=32,\n",
    "        help=\"Batch size for testing without balancing\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch_size_test_w_b\",\n",
    "        type=int,\n",
    "        default=4,\n",
    "        help=\"Batch size for testing with balancing\",\n",
    "    )\n",
    "\n",
    "    # History and ratios\n",
    "    parser.add_argument(\n",
    "        \"--history_size\", type=int, default=20, help=\"History size for the model\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--npratio\", type=int, default=4, help=\"Negative-positive ratio\"\n",
    "    )\n",
    "\n",
    "    # Training settings\n",
    "    parser.add_argument(\"--epochs\", type=int, default=5, help=\"Number of epochs\")\n",
    "    parser.add_argument(\n",
    "        \"--train_fraction\",\n",
    "        type=float,\n",
    "        default=1.0,\n",
    "        help=\"Fraction of training data to use\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--fraction_test\",\n",
    "        type=float,\n",
    "        default=1.0,\n",
    "        help=\"Fraction of testing data to use\",\n",
    "    )\n",
    "\n",
    "    # Model and loader settings\n",
    "    parser.add_argument(\n",
    "        \"--nrms_loader\",\n",
    "        type=str,\n",
    "        default=\"NRMSDataLoaderPretransform\",\n",
    "        choices=[\"NRMSDataLoaderPretransform\", \"NRMSDataLoader\"],\n",
    "        help=\"Data loader type (speed or memory efficient)\",\n",
    "    )\n",
    "\n",
    "    # Chunk processing\n",
    "    parser.add_argument(\n",
    "        \"--n_chunks_test\", type=int, default=10, help=\"Number of test chunks to process\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--chunks_done\", type=int, default=0, help=\"Number of chunks already processed\"\n",
    "    )\n",
    "\n",
    "    # =====================================================================================\n",
    "    #  ############################# UNIQUE FOR NRMSDocVec ###############################\n",
    "    # =====================================================================================\n",
    "    # Transformer settings\n",
    "    parser.add_argument(\n",
    "        \"--transformer_model_name\",\n",
    "        type=str,\n",
    "        default=\"FacebookAI/xlm-roberta-large\",\n",
    "        help=\"Transformer model name\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_title_length\",\n",
    "        type=int,\n",
    "        default=30,\n",
    "        help=\"Maximum length of title encoding\",\n",
    "    )\n",
    "\n",
    "    # Hyperparameters\n",
    "    parser.add_argument(\n",
    "        \"--head_num\", type=int, default=20, help=\"Number of attention heads\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--head_dim\", type=int, default=20, help=\"Dimension of each attention head\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--attention_hidden_dim\",\n",
    "        type=int,\n",
    "        default=200,\n",
    "        help=\"Dimension of attention hidden layers\",\n",
    "    )\n",
    "\n",
    "    # Optimizer settings\n",
    "    parser.add_argument(\n",
    "        \"--optimizer\", type=str, default=\"adam\", help=\"Optimizer to use\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--loss\", type=str, default=\"cross_entropy_loss\", help=\"Loss function\"\n",
    "    )\n",
    "    parser.add_argument(\"--dropout\", type=float, default=0.20, help=\"Dropout rate\")\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\", type=float, default=1e-4, help=\"Learning rate\"\n",
    "    )\n",
    "\n",
    "    # Parse known args to avoid Jupyter kernel args\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT License.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, Input, Dropout, Dense, BatchNormalization\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "#ebrec/models/newsrec/nrms.py\n",
    "\n",
    "class NRMSModel:\n",
    "    \"\"\"NRMS model(Neural News Recommendation with Multi-Head Self-Attention)\n",
    "\n",
    "    Chuhan Wu, Fangzhao Wu, Suyu Ge, Tao Qi, Yongfeng Huang,and Xing Xie, \"Neural News\n",
    "    Recommendation with Multi-Head Self-Attention\" in Proceedings of the 2019 Conference\n",
    "    on Empirical Methods in Natural Language Processing and the 9th International Joint Conference\n",
    "    on Natural Language Processing (EMNLP-IJCNLP)\n",
    "\n",
    "    Attributes:\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hparams: dict,\n",
    "        word2vec_embedding: np.ndarray = None,\n",
    "        word_emb_dim: int = 300,\n",
    "        vocab_size: int = 32000,\n",
    "        seed: int = None,\n",
    "    ):\n",
    "        \"\"\"Initialization steps for NRMS.\"\"\"\n",
    "        self.hparams = hparams\n",
    "        self.seed = seed\n",
    "\n",
    "        # SET SEED:\n",
    "        tf.random.set_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        # INIT THE WORD-EMBEDDINGS:\n",
    "        if word2vec_embedding is None:\n",
    "            # Xavier Initialization\n",
    "            initializer = GlorotUniform(seed=self.seed)\n",
    "            self.word2vec_embedding = initializer(shape=(vocab_size, word_emb_dim))\n",
    "            # self.word2vec_embedding = np.random.rand(vocab_size, word_emb_dim)\n",
    "        else:\n",
    "            self.word2vec_embedding = word2vec_embedding\n",
    "\n",
    "        # BUILD AND COMPILE MODEL:\n",
    "        self.model, self.scorer = self._build_graph()\n",
    "        data_loss = self._get_loss(self.hparams.loss)\n",
    "        train_optimizer = self._get_opt(\n",
    "            optimizer=self.hparams.optimizer, lr=self.hparams.learning_rate\n",
    "        )\n",
    "        self.model.compile(loss=data_loss, optimizer=train_optimizer)\n",
    "\n",
    "    def _get_loss(self, loss: str):\n",
    "        \"\"\"Make loss function, consists of data loss and regularization loss\n",
    "        Returns:\n",
    "            object: Loss function or loss function name\n",
    "        \"\"\"\n",
    "        if loss == \"cross_entropy_loss\":\n",
    "            data_loss = \"categorical_crossentropy\"\n",
    "        elif loss == \"log_loss\":\n",
    "            data_loss = \"binary_crossentropy\"\n",
    "        else:\n",
    "            raise ValueError(f\"this loss not defined {loss}\")\n",
    "        return data_loss\n",
    "\n",
    "    def _get_opt(self, optimizer: str, lr: float):\n",
    "        \"\"\"Get the optimizer according to configuration. Usually we will use Adam.\n",
    "        Returns:\n",
    "            object: An optimizer.\n",
    "        \"\"\"\n",
    "        # TODO: shouldn't be a string input you should just set the optimizer, to avoid stuff like this:\n",
    "        # => 'WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.'\n",
    "        if optimizer == \"adam\":\n",
    "            train_opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        else:\n",
    "            raise ValueError(f\"this optimizer not defined {optimizer}\")\n",
    "        return train_opt\n",
    "\n",
    "    def _build_graph(self):\n",
    "        \"\"\"Build NRMS model and scorer.\n",
    "\n",
    "        Returns:\n",
    "            object: a model used to train.\n",
    "            object: a model used to evaluate and inference.\n",
    "        \"\"\"\n",
    "        model, scorer = self._build_nrms()\n",
    "        return model, scorer\n",
    "\n",
    "    def _build_userencoder(self, titleencoder):\n",
    "        \"\"\"The main function to create user encoder of NRMS.\n",
    "\n",
    "        Args:\n",
    "            titleencoder (object): the news encoder of NRMS.\n",
    "\n",
    "        Return:\n",
    "            object: the user encoder of NRMS.\n",
    "        \"\"\"\n",
    "        his_input_title = tf.keras.Input(\n",
    "            shape=(self.hparams.history_size, self.hparams.title_size), dtype=\"int32\"\n",
    "        )\n",
    "\n",
    "        click_title_presents = tf.keras.layers.TimeDistributed(titleencoder)(\n",
    "            his_input_title\n",
    "        )\n",
    "        y = SelfAttention(self.hparams.head_num, self.hparams.head_dim, seed=self.seed)(\n",
    "            [click_title_presents] * 3\n",
    "        )\n",
    "        user_present = AttLayer2(self.hparams.attention_hidden_dim, seed=self.seed)(y)\n",
    "\n",
    "        model = tf.keras.Model(his_input_title, user_present, name=\"user_encoder\")\n",
    "        return model\n",
    "\n",
    "    def _build_newsencoder(self, units_per_layer: list[int] = None):\n",
    "        \"\"\"The main function to create news encoder of NRMS.\n",
    "\n",
    "        Args:\n",
    "            embedding_layer (object): a word embedding layer.\n",
    "\n",
    "        Return:\n",
    "            object: the news encoder of NRMS.\n",
    "        \"\"\"\n",
    "        embedding_layer = tf.keras.layers.Embedding(\n",
    "            self.word2vec_embedding.shape[0],\n",
    "            self.word2vec_embedding.shape[1],\n",
    "            weights=[self.word2vec_embedding],\n",
    "            trainable=True,\n",
    "        )\n",
    "        sequences_input_title = tf.keras.Input(\n",
    "            shape=(self.hparams.title_size,), dtype=\"int32\"\n",
    "        )\n",
    "        embedded_sequences_title = embedding_layer(sequences_input_title)\n",
    "\n",
    "        y = tf.keras.layers.Dropout(self.hparams.dropout)(embedded_sequences_title)\n",
    "        y = SelfAttention(self.hparams.head_num, self.hparams.head_dim, seed=self.seed)(\n",
    "            [y, y, y]\n",
    "        )\n",
    "\n",
    "        # Create configurable Dense layers (the if - else is something I've added):\n",
    "        if units_per_layer:\n",
    "            for layer in units_per_layer:\n",
    "                y = tf.keras.layers.Dense(\n",
    "                    units=layer,\n",
    "                    activation=\"relu\",\n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(\n",
    "                        self.hparams.newsencoder_l2_regularization\n",
    "                    ),\n",
    "                )(y)\n",
    "                y = tf.keras.layers.BatchNormalization()(y)\n",
    "                y = tf.keras.layers.Dropout(self.hparams.dropout)(y)\n",
    "        else:\n",
    "            y = tf.keras.layers.Dropout(self.hparams.dropout)(y)\n",
    "\n",
    "        pred_title = AttLayer2(self.hparams.attention_hidden_dim, seed=self.seed)(y)\n",
    "\n",
    "        model = tf.keras.Model(sequences_input_title, pred_title, name=\"news_encoder\")\n",
    "        return model\n",
    "\n",
    "    def _build_nrms(self):\n",
    "        \"\"\"The main function to create NRMS's logic. The core of NRMS\n",
    "        is a user encoder and a news encoder.\n",
    "\n",
    "        Returns:\n",
    "            object: a model used to train.\n",
    "            object: a model used to evaluate and inference.\n",
    "        \"\"\"\n",
    "\n",
    "        his_input_title = tf.keras.Input( \n",
    "            shape=(self.hparams.history_size, self.hparams.title_size),\n",
    "            dtype=\"int32\",\n",
    "        )\n",
    "        pred_input_title = tf.keras.Input(\n",
    "            # shape = (hparams.npratio + 1, hparams.title_size)\n",
    "            shape=(None, self.hparams.title_size),\n",
    "            dtype=\"int32\",\n",
    "        )\n",
    "        pred_input_title_one = tf.keras.Input(\n",
    "            shape=(\n",
    "                1,\n",
    "                self.hparams.title_size,\n",
    "            ),\n",
    "            dtype=\"int32\",\n",
    "        )\n",
    "        pred_title_one_reshape = tf.keras.layers.Reshape((self.hparams.title_size,))(\n",
    "            pred_input_title_one\n",
    "        )\n",
    "        titleencoder = self._build_newsencoder(\n",
    "            units_per_layer=self.hparams.newsencoder_units_per_layer\n",
    "        )\n",
    "        self.userencoder = self._build_userencoder(titleencoder)\n",
    "        self.newsencoder = titleencoder\n",
    "\n",
    "        user_present = self.userencoder(his_input_title)\n",
    "        news_present = tf.keras.layers.TimeDistributed(self.newsencoder)(\n",
    "            pred_input_title\n",
    "        )\n",
    "        news_present_one = self.newsencoder(pred_title_one_reshape)\n",
    "\n",
    "        preds = tf.keras.layers.Dot(axes=-1)([news_present, user_present])\n",
    "        preds = tf.keras.layers.Activation(activation=\"softmax\")(preds)\n",
    "\n",
    "        pred_one = tf.keras.layers.Dot(axes=-1)([news_present_one, user_present])\n",
    "        pred_one = tf.keras.layers.Activation(activation=\"sigmoid\")(pred_one)\n",
    "\n",
    "        model = tf.keras.Model([his_input_title, pred_input_title], preds)\n",
    "        scorer = tf.keras.Model([his_input_title, pred_input_title_one], pred_one)\n",
    "\n",
    "        return model, scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#src/ebrec/utils/_articles_behaviors.py\n",
    "\n",
    "try:\n",
    "    import polars as pl\n",
    "except ImportError:\n",
    "    print(\"polars not available\")\n",
    "\n",
    "\n",
    "def map_list_article_id_to_value(\n",
    "    behaviors: pl.DataFrame,\n",
    "    behaviors_column: str,\n",
    "    mapping: dict[int, pl.Series],\n",
    "    drop_nulls: bool = False,\n",
    "    fill_nulls: any = None,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "\n",
    "    Maps the values of a column in a DataFrame `behaviors` containing article IDs to their corresponding values\n",
    "    in a column in another DataFrame `articles`. The mapping is performed using a dictionary constructed from\n",
    "    the two DataFrames. The resulting DataFrame has the same columns as `behaviors`, but with the article IDs\n",
    "    replaced by their corresponding values.\n",
    "\n",
    "    Args:\n",
    "        behaviors (pl.DataFrame): The DataFrame containing the column to be mapped.\n",
    "        behaviors_column (str): The name of the column to be mapped in `behaviors`.\n",
    "        mapping (dict[int, pl.Series]): A dictionary with article IDs as keys and corresponding values as values.\n",
    "            Note, 'replace' works a lot faster when values are of type pl.Series!\n",
    "        drop_nulls (bool): If `True`, any rows in the resulting DataFrame with null values will be dropped.\n",
    "            If `False` and `fill_nulls` is specified, null values in `behaviors_column` will be replaced with `fill_null`.\n",
    "        fill_nulls (Optional[any]): If specified, any null values in `behaviors_column` will be replaced with this value.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: A new DataFrame with the same columns as `behaviors`, but with the article IDs in\n",
    "            `behaviors_column` replaced by their corresponding values in `mapping`.\n",
    "\n",
    "    Example:\n",
    "    >>> behaviors = pl.DataFrame(\n",
    "            {\"user_id\": [1, 2, 3, 4, 5], \"article_ids\": [[\"A1\", \"A2\"], [\"A2\", \"A3\"], [\"A1\", \"A4\"], [\"A4\", \"A4\"], None]}\n",
    "        )\n",
    "    >>> articles = pl.DataFrame(\n",
    "            {\n",
    "                \"article_id\": [\"A1\", \"A2\", \"A3\"],\n",
    "                \"article_type\": [\"News\", \"Sports\", \"Entertainment\"],\n",
    "            }\n",
    "        )\n",
    "    >>> articles_dict = dict(zip(articles[\"article_id\"], articles[\"article_type\"]))\n",
    "    >>> map_list_article_id_to_value(\n",
    "            behaviors=behaviors,\n",
    "            behaviors_column=\"article_ids\",\n",
    "            mapping=articles_dict,\n",
    "            fill_nulls=\"Unknown\",\n",
    "        )\n",
    "        shape: (4, 2)\n",
    "        \n",
    "         user_id  article_ids                 \n",
    "         ---      ---                         \n",
    "         i64      list[str]                   \n",
    "        \n",
    "         1        [\"News\", \"Sports\"]          \n",
    "         2        [\"Sports\", \"Entertainment\"] \n",
    "         3        [\"News\", \"Unknown\"]         \n",
    "         4        [\"Unknown\", \"Unknown\"]      \n",
    "         5        [\"Unknown\"]                 \n",
    "        \n",
    "    >>> map_list_article_id_to_value(\n",
    "            behaviors=behaviors,\n",
    "            behaviors_column=\"article_ids\",\n",
    "            mapping=articles_dict,\n",
    "            drop_nulls=True,\n",
    "        )\n",
    "        shape: (4, 2)\n",
    "        \n",
    "         user_id  article_ids                 \n",
    "         ---      ---                         \n",
    "         i64      list[str]                   \n",
    "        \n",
    "         1        [\"News\", \"Sports\"]          \n",
    "         2        [\"Sports\", \"Entertainment\"] \n",
    "         3        [\"News\"]                    \n",
    "         4        null                        \n",
    "         5        null                        \n",
    "        \n",
    "    >>> map_list_article_id_to_value(\n",
    "            behaviors=behaviors,\n",
    "            behaviors_column=\"article_ids\",\n",
    "            mapping=articles_dict,\n",
    "            drop_nulls=False,\n",
    "        )\n",
    "        shape: (4, 2)\n",
    "        \n",
    "         user_id  article_ids                 \n",
    "         ---      ---                         \n",
    "         i64      list[str]                   \n",
    "        \n",
    "         1        [\"News\", \"Sports\"]          \n",
    "         2        [\"Sports\", \"Entertainment\"] \n",
    "         3        [\"News\", null]              \n",
    "         4        [null, null]                \n",
    "         5        [null]                      \n",
    "        \n",
    "    \"\"\"\n",
    "    GROUPBY_ID = generate_unique_name(behaviors.columns, \"_groupby_id\")\n",
    "    behaviors = behaviors.lazy().with_row_index(GROUPBY_ID)\n",
    "    # =>\n",
    "    select_column = (\n",
    "        behaviors.select(pl.col(GROUPBY_ID), pl.col(behaviors_column))\n",
    "        .explode(behaviors_column)\n",
    "        .with_columns(pl.col(behaviors_column).replace(mapping, default=None))\n",
    "        .collect()\n",
    "    )\n",
    "    # =>\n",
    "    if drop_nulls:\n",
    "        select_column = select_column.drop_nulls()\n",
    "    elif fill_nulls is not None:\n",
    "        select_column = select_column.with_columns(\n",
    "            pl.col(behaviors_column).fill_null(fill_nulls)\n",
    "        )\n",
    "    # =>\n",
    "    select_column = (\n",
    "        select_column.lazy().group_by(GROUPBY_ID).agg(behaviors_column).collect()\n",
    "    )\n",
    "    return (\n",
    "        behaviors.drop(behaviors_column)\n",
    "        .collect()\n",
    "        .join(select_column, on=GROUPBY_ID, how=\"left\")\n",
    "        .drop(GROUPBY_ID)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT License.\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#ebsrec/models/newsrec/nrms_docsvec.py\n",
    "\n",
    "class NRMSDocVec:\n",
    "    \"\"\"\n",
    "    Modified NRMS model (Neural News Recommendation with Multi-Head Self-Attention)\n",
    "    - Initiated with article-embeddings.\n",
    "\n",
    "    Chuhan Wu, Fangzhao Wu, Suyu Ge, Tao Qi, Yongfeng Huang,and Xing Xie, \"Neural News\n",
    "    Recommendation with Multi-Head Self-Attention\" in Proceedings of the 2019 Conference\n",
    "    on Empirical Methods in Natural Language Processing and the 9th International Joint Conference\n",
    "    on Natural Language Processing (EMNLP-IJCNLP)\n",
    "\n",
    "    Attributes:\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hparams: dict,\n",
    "        seed: int = None,\n",
    "    ):\n",
    "        \"\"\"Initialization steps for NRMS.\"\"\"\n",
    "        self.hparams = hparams\n",
    "        self.seed = seed\n",
    "\n",
    "        # SET SEED:\n",
    "        tf.random.set_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        # BUILD AND COMPILE MODEL:\n",
    "        self.model, self.scorer = self._build_graph()\n",
    "        data_loss = self._get_loss(self.hparams.loss)\n",
    "        train_optimizer = self._get_opt(\n",
    "            optimizer=self.hparams.optimizer, lr=self.hparams.learning_rate\n",
    "        )\n",
    "        self.model.compile(loss=data_loss, optimizer=train_optimizer)\n",
    "\n",
    "    def _get_loss(self, loss: str):\n",
    "        \"\"\"Make loss function, consists of data loss and regularization loss\n",
    "        Returns:\n",
    "            object: Loss function or loss function name\n",
    "        \"\"\"\n",
    "        if loss == \"cross_entropy_loss\":\n",
    "            data_loss = \"categorical_crossentropy\"\n",
    "        elif loss == \"log_loss\":\n",
    "            data_loss = \"binary_crossentropy\"\n",
    "        else:\n",
    "            raise ValueError(f\"this loss not defined {loss}\")\n",
    "        return data_loss\n",
    "\n",
    "    def _get_opt(self, optimizer: str, lr: float):\n",
    "        \"\"\"Get the optimizer according to configuration. Usually we will use Adam.\n",
    "        Returns:\n",
    "            object: An optimizer.\n",
    "        \"\"\"\n",
    "        if optimizer == \"adam\":\n",
    "            train_opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        else:\n",
    "            raise ValueError(f\"this optimizer not defined {optimizer}\")\n",
    "        return train_opt\n",
    "\n",
    "    def _build_graph(self):\n",
    "        \"\"\"Build NRMS model and scorer.\n",
    "\n",
    "        Returns:\n",
    "            object: a model used to train.\n",
    "            object: a model used to evaluate and inference.\n",
    "        \"\"\"\n",
    "        model, scorer = self._build_nrms()\n",
    "        return model, scorer\n",
    "\n",
    "    def _build_userencoder(self, titleencoder):\n",
    "        \"\"\"The main function to create user encoder of NRMS.\n",
    "\n",
    "        Args:\n",
    "            titleencoder (object): the news encoder of NRMS.\n",
    "\n",
    "        Return:\n",
    "            object: the user encoder of NRMS.\n",
    "        \"\"\"\n",
    "        his_input_title = tf.keras.Input(\n",
    "            shape=(self.hparams.history_size, self.hparams.title_size), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "        click_title_presents = tf.keras.layers.TimeDistributed(titleencoder)(\n",
    "            his_input_title\n",
    "        )\n",
    "        y = SelfAttention(self.hparams.head_num, self.hparams.head_dim, seed=self.seed)(\n",
    "            [click_title_presents] * 3\n",
    "        )\n",
    "        user_present = AttLayer2(self.hparams.attention_hidden_dim, seed=self.seed)(y)\n",
    "\n",
    "        model = tf.keras.Model(his_input_title, user_present, name=\"user_encoder\")\n",
    "        return model\n",
    "\n",
    "    def _build_newsencoder(self, units_per_layer: list[int] = list[512, 512, 512]):\n",
    "        \"\"\"THIS IS OUR IMPLEMENTATION.\n",
    "        The main function to create a news encoder.\n",
    "\n",
    "        Parameters:\n",
    "            units_per_layer (int): The number of neurons in each Dense layer.\n",
    "\n",
    "        Return:\n",
    "            object: the news encoder.\n",
    "        \"\"\"\n",
    "        DOCUMENT_VECTOR_DIM = self.hparams.title_size\n",
    "        OUTPUT_DIM = self.hparams.head_num * self.hparams.head_dim\n",
    "\n",
    "        # DENSE LAYERS (FINE-TUNED):\n",
    "        sequences_input_title = tf.keras.Input(\n",
    "            shape=(DOCUMENT_VECTOR_DIM), dtype=\"float32\"\n",
    "        )\n",
    "        x = sequences_input_title\n",
    "        # Create configurable Dense layers:\n",
    "        for layer in units_per_layer:\n",
    "            x = tf.keras.layers.Dense(\n",
    "                units=layer,\n",
    "                activation=\"relu\",\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(\n",
    "                    self.hparams.newsencoder_l2_regularization\n",
    "                ),\n",
    "            )(x)\n",
    "            x = tf.keras.layers.BatchNormalization()(x)\n",
    "            x = tf.keras.layers.Dropout(self.hparams.dropout)(x)\n",
    "\n",
    "        # OUTPUT:\n",
    "        pred_title = tf.keras.layers.Dense(units=OUTPUT_DIM, activation=\"relu\")(x)\n",
    "\n",
    "        # Construct the final model\n",
    "        model = tf.keras.Model(\n",
    "            inputs=sequences_input_title, outputs=pred_title, name=\"news_encoder\"\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    def _build_nrms(self):\n",
    "        \"\"\"The main function to create NRMS's logic. The core of NRMS\n",
    "        is a user encoder and a news encoder.\n",
    "\n",
    "        Returns:\n",
    "            object: a model used to train.\n",
    "            object: a model used to evaluate and inference.\n",
    "        \"\"\"\n",
    "\n",
    "        his_input_title = tf.keras.Input(\n",
    "            shape=(self.hparams.history_size, self.hparams.title_size),\n",
    "            dtype=\"float32\",\n",
    "        )\n",
    "        pred_input_title = tf.keras.Input(\n",
    "            # shape = (hparams.npratio + 1, hparams.title_size)\n",
    "            shape=(None, self.hparams.title_size),\n",
    "            dtype=\"float32\",\n",
    "        )\n",
    "        pred_input_title_one = tf.keras.Input(\n",
    "            shape=(\n",
    "                1,\n",
    "                self.hparams.title_size,\n",
    "            ),\n",
    "            dtype=\"float32\",\n",
    "        )\n",
    "        pred_title_one_reshape = tf.keras.layers.Reshape((self.hparams.title_size,))(\n",
    "            pred_input_title_one\n",
    "        )\n",
    "        titleencoder = self._build_newsencoder(\n",
    "            units_per_layer=self.hparams.newsencoder_units_per_layer\n",
    "        )\n",
    "        self.userencoder = self._build_userencoder(titleencoder)\n",
    "        self.newsencoder = titleencoder\n",
    "\n",
    "        user_present = self.userencoder(his_input_title)\n",
    "        news_present = tf.keras.layers.TimeDistributed(self.newsencoder)(\n",
    "            pred_input_title\n",
    "        )\n",
    "        news_present_one = self.newsencoder(pred_title_one_reshape)\n",
    "\n",
    "        preds = tf.keras.layers.Dot(axes=-1)([news_present, user_present])\n",
    "        preds = tf.keras.layers.Activation(activation=\"softmax\")(preds)\n",
    "\n",
    "        pred_one = tf.keras.layers.Dot(axes=-1)([news_present_one, user_present])\n",
    "        pred_one = tf.keras.layers.Activation(activation=\"sigmoid\")(pred_one)\n",
    "\n",
    "        model = tf.keras.Model([his_input_title, pred_input_title], preds)\n",
    "        scorer = tf.keras.Model([his_input_title, pred_input_title_one], pred_one)\n",
    "\n",
    "        return model, scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "#ebrec/models/newsrec/layers.py\n",
    "\n",
    "\n",
    "class AttLayer2(layers.Layer):\n",
    "    \"\"\"Soft alignment attention implement.\n",
    "\n",
    "    Attributes:\n",
    "        dim (int): attention hidden dim\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim=200, seed=0, **kwargs):\n",
    "        \"\"\"Initialization steps for AttLayer2.\n",
    "\n",
    "        Args:\n",
    "            dim (int): attention hidden dim\n",
    "        \"\"\"\n",
    "\n",
    "        self.dim = dim\n",
    "        self.seed = seed\n",
    "        super(AttLayer2, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"Initialization for variables in AttLayer2\n",
    "        There are there variables in AttLayer2, i.e. W, b and q.\n",
    "\n",
    "        Args:\n",
    "            input_shape (object): shape of input tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        assert len(input_shape) == 3\n",
    "        dim = self.dim\n",
    "        self.W = self.add_weight(\n",
    "            name=\"W\",\n",
    "            shape=(int(input_shape[-1]), dim),\n",
    "            initializer=keras.initializers.glorot_uniform(seed=self.seed),\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            name=\"b\",\n",
    "            shape=(dim,),\n",
    "            initializer=keras.initializers.Zeros(),\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.q = self.add_weight(\n",
    "            name=\"q\",\n",
    "            shape=(dim, 1),\n",
    "            initializer=keras.initializers.glorot_uniform(seed=self.seed),\n",
    "            trainable=True,\n",
    "        )\n",
    "        super(AttLayer2, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "    def call(self, inputs, mask=None, **kwargs):\n",
    "        \"\"\"Core implemention of soft attention\n",
    "\n",
    "        Args:\n",
    "            inputs (object): input tensor.\n",
    "\n",
    "        Returns:\n",
    "            object: weighted sum of input tensors.\n",
    "        \"\"\"\n",
    "\n",
    "        attention = K.tanh(K.dot(inputs, self.W) + self.b)\n",
    "        attention = K.dot(attention, self.q)\n",
    "\n",
    "        attention = K.squeeze(attention, axis=2)\n",
    "\n",
    "        if mask == None:\n",
    "            attention = K.exp(attention)\n",
    "        else:\n",
    "            attention = K.exp(attention) * K.cast(mask, dtype=\"float32\")\n",
    "\n",
    "        attention_weight = attention / (\n",
    "            K.sum(attention, axis=-1, keepdims=True) + K.epsilon()\n",
    "        )\n",
    "\n",
    "        attention_weight = K.expand_dims(attention_weight)\n",
    "        weighted_input = inputs * attention_weight\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        \"\"\"Compte output mask value\n",
    "\n",
    "        Args:\n",
    "            input (object): input tensor.\n",
    "            input_mask: input mask\n",
    "\n",
    "        Returns:\n",
    "            object: output mask.\n",
    "        \"\"\"\n",
    "        return None\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\"Compute shape of output tensor\n",
    "\n",
    "        Args:\n",
    "            input_shape (tuple): shape of input tensor.\n",
    "\n",
    "        Returns:\n",
    "            tuple: shape of output tensor.\n",
    "        \"\"\"\n",
    "        return input_shape[0], input_shape[-1]\n",
    "\n",
    "\n",
    "class SelfAttention(layers.Layer):\n",
    "    \"\"\"Multi-head self attention implement.\n",
    "\n",
    "    Args:\n",
    "        multiheads (int): The number of heads.\n",
    "        head_dim (object): Dimention of each head.\n",
    "        mask_right (boolean): whether to mask right words.\n",
    "\n",
    "    Returns:\n",
    "        object: Weighted sum after attention.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, multiheads, head_dim, seed=0, mask_right=False, **kwargs):\n",
    "        \"\"\"Initialization steps for AttLayer2.\n",
    "\n",
    "        Args:\n",
    "            multiheads (int): The number of heads.\n",
    "            head_dim (object): Dimention of each head.\n",
    "            mask_right (boolean): whether to mask right words.\n",
    "        \"\"\"\n",
    "\n",
    "        self.multiheads = multiheads\n",
    "        self.head_dim = head_dim\n",
    "        self.output_dim = multiheads * head_dim\n",
    "        self.mask_right = mask_right\n",
    "        self.seed = seed\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\"Compute shape of output tensor.\n",
    "\n",
    "        Returns:\n",
    "            tuple: output shape tuple.\n",
    "        \"\"\"\n",
    "\n",
    "        return (input_shape[0][0], input_shape[0][1], self.output_dim)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"Initialization for variables in SelfAttention.\n",
    "        There are three variables in SelfAttention, i.e. WQ, WK ans WV.\n",
    "        WQ is used for linear transformation of query.\n",
    "        WK is used for linear transformation of key.\n",
    "        WV is used for linear transformation of value.\n",
    "\n",
    "        Args:\n",
    "            input_shape (object): shape of input tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        self.WQ = self.add_weight(\n",
    "            name=\"WQ\",\n",
    "            shape=(int(input_shape[0][-1]), self.output_dim),\n",
    "            initializer=keras.initializers.glorot_uniform(seed=self.seed),\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.WK = self.add_weight(\n",
    "            name=\"WK\",\n",
    "            shape=(int(input_shape[1][-1]), self.output_dim),\n",
    "            initializer=keras.initializers.glorot_uniform(seed=self.seed),\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.WV = self.add_weight(\n",
    "            name=\"WV\",\n",
    "            shape=(int(input_shape[2][-1]), self.output_dim),\n",
    "            initializer=keras.initializers.glorot_uniform(seed=self.seed),\n",
    "            trainable=True,\n",
    "        )\n",
    "        super(SelfAttention, self).build(input_shape)\n",
    "\n",
    "    def Mask(self, inputs, seq_len, mode=\"add\"):\n",
    "        \"\"\"Mask operation used in multi-head self attention\n",
    "\n",
    "        Args:\n",
    "            seq_len (object): sequence length of inputs.\n",
    "            mode (str): mode of mask.\n",
    "\n",
    "        Returns:\n",
    "            object: tensors after masking.\n",
    "        \"\"\"\n",
    "\n",
    "        if seq_len is None:\n",
    "            return inputs\n",
    "        else:\n",
    "            mask = K.one_hot(indices=seq_len[:, 0], num_classes=K.shape(inputs)[1])\n",
    "            mask = 1 - K.cumsum(mask, axis=1)\n",
    "\n",
    "            for _ in range(len(inputs.shape) - 2):\n",
    "                mask = K.expand_dims(mask, 2)\n",
    "\n",
    "            if mode == \"mul\":\n",
    "                return inputs * mask\n",
    "            elif mode == \"add\":\n",
    "                return inputs - (1 - mask) * 1e12\n",
    "\n",
    "    def call(self, QKVs):\n",
    "        \"\"\"Core logic of multi-head self attention.\n",
    "\n",
    "        Args:\n",
    "            QKVs (list): inputs of multi-head self attention i.e. qeury, key and value.\n",
    "\n",
    "        Returns:\n",
    "            object: ouput tensors.\n",
    "        \"\"\"\n",
    "        if len(QKVs) == 3:\n",
    "            Q_seq, K_seq, V_seq = QKVs\n",
    "            Q_len, V_len = None, None\n",
    "        elif len(QKVs) == 5:\n",
    "            Q_seq, K_seq, V_seq, Q_len, V_len = QKVs\n",
    "        Q_seq = K.dot(Q_seq, self.WQ)\n",
    "        Q_seq = K.reshape(\n",
    "            Q_seq, shape=(-1, K.shape(Q_seq)[1], self.multiheads, self.head_dim)\n",
    "        )\n",
    "        Q_seq = K.permute_dimensions(Q_seq, pattern=(0, 2, 1, 3))\n",
    "\n",
    "        K_seq = K.dot(K_seq, self.WK)\n",
    "        K_seq = K.reshape(\n",
    "            K_seq, shape=(-1, K.shape(K_seq)[1], self.multiheads, self.head_dim)\n",
    "        )\n",
    "        K_seq = K.permute_dimensions(K_seq, pattern=(0, 2, 1, 3))\n",
    "\n",
    "        V_seq = K.dot(V_seq, self.WV)\n",
    "        V_seq = K.reshape(\n",
    "            V_seq, shape=(-1, K.shape(V_seq)[1], self.multiheads, self.head_dim)\n",
    "        )\n",
    "        V_seq = K.permute_dimensions(V_seq, pattern=(0, 2, 1, 3))\n",
    "        A = tf.matmul(Q_seq, K_seq, adjoint_a=False, adjoint_b=True) / K.sqrt(\n",
    "            K.cast(self.head_dim, dtype=\"float32\")\n",
    "        )\n",
    "\n",
    "        A = K.permute_dimensions(\n",
    "            A, pattern=(0, 3, 2, 1)\n",
    "        )  # A.shape=[batch_size,K_sequence_length,Q_sequence_length,self.multiheads]\n",
    "\n",
    "        A = self.Mask(A, V_len, \"add\")\n",
    "        A = K.permute_dimensions(A, pattern=(0, 3, 2, 1))\n",
    "\n",
    "        if self.mask_right:\n",
    "            ones = K.ones_like(A[:1, :1])\n",
    "            lower_triangular = K.tf.matrix_band_part(ones, num_lower=-1, num_upper=0)\n",
    "            mask = (ones - lower_triangular) * 1e12\n",
    "            A = A - mask\n",
    "        A = K.softmax(A)\n",
    "\n",
    "        O_seq = tf.matmul(A, V_seq, adjoint_a=True, adjoint_b=False)\n",
    "        O_seq = K.permute_dimensions(O_seq, pattern=(0, 2, 1, 3))\n",
    "\n",
    "        O_seq = K.reshape(O_seq, shape=(-1, K.shape(O_seq)[1], self.output_dim))\n",
    "        O_seq = self.Mask(O_seq, Q_len, \"mul\")\n",
    "        return O_seq\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"add multiheads, multiheads and mask_right into layer config.\n",
    "\n",
    "        Returns:\n",
    "            dict: config of SelfAttention layer.\n",
    "        \"\"\"\n",
    "        config = super(SelfAttention, self).get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"multiheads\": self.multiheads,\n",
    "                \"head_dim\": self.head_dim,\n",
    "                \"mask_right\": self.mask_right,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "\n",
    "class ComputeMasking(layers.Layer):\n",
    "    \"\"\"Compute if inputs contains zero value.\n",
    "\n",
    "    Returns:\n",
    "        bool tensor: True for values not equal to zero.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ComputeMasking, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        mask = K.not_equal(inputs, 0)\n",
    "        return K.cast(mask, K.floatx())\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "\n",
    "class OverwriteMasking(layers.Layer):\n",
    "    \"\"\"Set values at spasific positions to zero.\n",
    "\n",
    "    Args:\n",
    "        inputs (list): value tensor and mask tensor.\n",
    "\n",
    "    Returns:\n",
    "        object: tensor after setting values to zero.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(OverwriteMasking, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(OverwriteMasking, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return inputs[0] * K.expand_dims(inputs[1])\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0]\n",
    "\n",
    "\n",
    "def PersonalizedAttentivePooling(dim1, dim2, dim3, seed=0):\n",
    "    \"\"\"Soft alignment attention implement.\n",
    "    Attributes:\n",
    "        dim1 (int): first dimention of value shape.\n",
    "        dim2 (int): second dimention of value shape.\n",
    "        dim3 (int): shape of query\n",
    "\n",
    "    Returns:\n",
    "        object: weighted summary of inputs value.\n",
    "    \"\"\"\n",
    "    vecs_input = keras.Input(shape=(dim1, dim2), dtype=\"float32\")\n",
    "    query_input = keras.Input(shape=(dim3,), dtype=\"float32\")\n",
    "\n",
    "    user_vecs = layers.Dropout(0.2)(vecs_input)\n",
    "    user_att = layers.Dense(\n",
    "        dim3,\n",
    "        activation=\"tanh\",\n",
    "        kernel_initializer=keras.initializers.glorot_uniform(seed=seed),\n",
    "        bias_initializer=keras.initializers.Zeros(),\n",
    "    )(user_vecs)\n",
    "    user_att2 = layers.Dot(axes=-1)([query_input, user_att])\n",
    "    user_att2 = layers.Activation(\"softmax\")(user_att2)\n",
    "    user_vec = layers.Dot((1, 1))([user_vecs, user_att2])\n",
    "\n",
    "    model = keras.Model([vecs_input, query_input], user_vec)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ebrec/models/newsrec/model_config.py)\n",
    "\n",
    "\n",
    "#\n",
    "DEFAULT_TITLE_SIZE = 30\n",
    "DEFAULT_BODY_SIZE = 40\n",
    "UNKNOWN_TITLE_VALUE = [0] * DEFAULT_TITLE_SIZE\n",
    "UNKNOWN_BODY_VALUE = [0] * DEFAULT_BODY_SIZE\n",
    "\n",
    "DEFAULT_DOCUMENT_SIZE = 768\n",
    "\n",
    "\n",
    "def print_hparams(hparams_class):\n",
    "    for attr, value in hparams_class.__annotations__.items():\n",
    "        # Print attribute names and values\n",
    "        print(f\"{attr}: {getattr(hparams_class, attr)}\")\n",
    "\n",
    "\n",
    "def hparams_to_dict(hparams_class) -> dict:\n",
    "    params = {}\n",
    "    for attr, value in hparams_class.__annotations__.items():\n",
    "        params[attr] = getattr(hparams_class, attr)\n",
    "    return params\n",
    "\n",
    "\n",
    "class hparams_naml:\n",
    "    # INPUT DIMENTIONS:\n",
    "    title_size: int = DEFAULT_TITLE_SIZE\n",
    "    history_size: int = 20\n",
    "    body_size: int = DEFAULT_BODY_SIZE\n",
    "    vert_num: int = 100\n",
    "    vert_emb_dim: int = 10\n",
    "    subvert_num: int = 100\n",
    "    subvert_emb_dim: int = 10\n",
    "    # MODEL ARCHITECTURE\n",
    "    dense_activation: str = \"relu\"\n",
    "    cnn_activation: str = \"relu\"\n",
    "    attention_hidden_dim: int = 200\n",
    "    filter_num: int = 400\n",
    "    window_size: int = 3\n",
    "    # MODEL OPTIMIZER:\n",
    "    optimizer: str = \"adam\"\n",
    "    loss: str = \"cross_entropy_loss\"\n",
    "    dropout: float = 0.2\n",
    "    learning_rate: float = 1e-4\n",
    "\n",
    "\n",
    "class hparams_lstur:\n",
    "    # INPUT DIMENTIONS:\n",
    "    title_size: int = DEFAULT_TITLE_SIZE\n",
    "    history_size: int = 20\n",
    "    n_users: int = 50000\n",
    "    # MODEL ARCHITECTURE\n",
    "    cnn_activation: str = \"relu\"\n",
    "    type: str = \"ini\"\n",
    "    attention_hidden_dim: int = 200\n",
    "    gru_unit: int = 400\n",
    "    filter_num: int = 400\n",
    "    window_size: int = 3\n",
    "    # MODEL OPTIMIZER:\n",
    "    optimizer: str = \"adam\"\n",
    "    loss: str = \"cross_entropy_loss\"\n",
    "    dropout: float = 0.2\n",
    "    learning_rate: float = 1e-4\n",
    "\n",
    "\n",
    "class hparams_npa:\n",
    "    # INPUT DIMENTIONS:\n",
    "    title_size: int = DEFAULT_TITLE_SIZE\n",
    "    history_size: int = 20\n",
    "    n_users: int = 50000\n",
    "    # MODEL ARCHITECTURE\n",
    "    cnn_activation: str = \"relu\"\n",
    "    attention_hidden_dim: int = 200\n",
    "    user_emb_dim: int = 400\n",
    "    filter_num: int = 400\n",
    "    window_size: int = 3\n",
    "    # MODEL OPTIMIZER:\n",
    "    optimizer: str = \"adam\"\n",
    "    loss: str = \"cross_entropy_loss\"\n",
    "    dropout: float = 0.2\n",
    "    learning_rate: float = 1e-4\n",
    "\n",
    "\n",
    "class hparams_nrms:\n",
    "    # INPUT DIMENTIONS:\n",
    "    title_size: int = DEFAULT_TITLE_SIZE\n",
    "    history_size: int = 20\n",
    "    # MODEL ARCHITECTURE\n",
    "    head_num: int = 20\n",
    "    head_dim: int = 20\n",
    "    attention_hidden_dim: int = 200\n",
    "    # MODEL OPTIMIZER:\n",
    "    optimizer: str = \"adam\"\n",
    "    loss: str = \"cross_entropy_loss\"\n",
    "    dropout: float = 0.2\n",
    "    learning_rate: float = 1e-4\n",
    "    # MY OWN LITTLE TWIST:\n",
    "    newsencoder_units_per_layer: list[int] = None\n",
    "    newsencoder_l2_regularization: float = 1e-4\n",
    "\n",
    "\n",
    "class hparams_nrms_docvec:\n",
    "    # INPUT DIMENTIONS:\n",
    "    title_size: int = DEFAULT_DOCUMENT_SIZE\n",
    "    history_size: int = 20\n",
    "    # MODEL ARCHITECTURE\n",
    "    head_num: int = 16\n",
    "    head_dim: int = 16\n",
    "    attention_hidden_dim: int = 200\n",
    "    # MODEL OPTIMIZER:\n",
    "    optimizer: str = \"adam\"\n",
    "    loss: str = \"cross_entropy_loss\"\n",
    "    dropout: float = 0.2\n",
    "    learning_rate: float = 1e-4\n",
    "    newsencoder_units_per_layer: list[int] = [512, 512, 512]\n",
    "    newsencoder_l2_regularization: float = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "import tensorflow as tf\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "#ebrec/models/newsrec/dataloader.py\n",
    "\n",
    "@dataclass\n",
    "class NewsrecDataLoader(tf.keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    A DataLoader for news recommendation.\n",
    "    \"\"\"\n",
    "\n",
    "    behaviors: pl.DataFrame\n",
    "    history_column: str\n",
    "    article_dict: dict[int, any]\n",
    "    unknown_representation: str\n",
    "    eval_mode: bool = False\n",
    "    batch_size: int = 32\n",
    "    inview_col: str = DEFAULT_INVIEW_ARTICLES_COL\n",
    "    labels_col: str = DEFAULT_LABELS_COL\n",
    "    user_col: str = DEFAULT_USER_COL\n",
    "    kwargs: field(default_factory=dict) = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"\n",
    "        Post-initialization method. Loads the data and sets additional attributes.\n",
    "        \"\"\"\n",
    "        self.lookup_article_index, self.lookup_article_matrix = create_lookup_objects(\n",
    "            self.article_dict, unknown_representation=self.unknown_representation\n",
    "        )\n",
    "        self.unknown_index = [0]\n",
    "        self.X, self.y = self.load_data()\n",
    "        if self.kwargs is not None:\n",
    "            self.set_kwargs(self.kwargs)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return int(np.ceil(len(self.X) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self):\n",
    "        raise ValueError(\"Function '__getitem__' needs to be implemented.\")\n",
    "\n",
    "    def load_data(self) -> tuple[pl.DataFrame, pl.DataFrame]:\n",
    "        X = self.behaviors.drop(self.labels_col).with_columns(\n",
    "            pl.col(self.inview_col).list.len().alias(\"n_samples\")\n",
    "        )\n",
    "        y = self.behaviors[self.labels_col]\n",
    "        return X, y\n",
    "\n",
    "    def set_kwargs(self, kwargs: dict):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class NRMSDataLoader(NewsrecDataLoader):\n",
    "    def transform(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        return df.pipe(\n",
    "            map_list_article_id_to_value,\n",
    "            behaviors_column=self.history_column,\n",
    "            mapping=self.lookup_article_index,\n",
    "            fill_nulls=self.unknown_index,\n",
    "            drop_nulls=False,\n",
    "        ).pipe(\n",
    "            map_list_article_id_to_value,\n",
    "            behaviors_column=self.inview_col,\n",
    "            mapping=self.lookup_article_index,\n",
    "            fill_nulls=self.unknown_index,\n",
    "            drop_nulls=False,\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx) -> tuple[tuple[np.ndarray], np.ndarray]:\n",
    "        \"\"\"\n",
    "        his_input_title:    (samples, history_size, document_dimension)\n",
    "        pred_input_title:   (samples, npratio, document_dimension)\n",
    "        batch_y:            (samples, npratio)\n",
    "        \"\"\"\n",
    "        batch_X = self.X[idx * self.batch_size : (idx + 1) * self.batch_size].pipe(\n",
    "            self.transform\n",
    "        )\n",
    "        batch_y = self.y[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        # =>\n",
    "        if self.eval_mode:\n",
    "            repeats = np.array(batch_X[\"n_samples\"])\n",
    "            # =>\n",
    "            batch_y = np.array(batch_y.explode().to_list()).reshape(-1, 1)\n",
    "            # =>\n",
    "            his_input_title = repeat_by_list_values_from_matrix(\n",
    "                batch_X[self.history_column].to_list(),\n",
    "                matrix=self.lookup_article_matrix,\n",
    "                repeats=repeats,\n",
    "            )\n",
    "            # =>\n",
    "            pred_input_title = self.lookup_article_matrix[\n",
    "                batch_X[self.inview_col].explode().to_list()\n",
    "            ]\n",
    "        else:\n",
    "            batch_y = np.array(batch_y.to_list())\n",
    "            his_input_title = self.lookup_article_matrix[\n",
    "                batch_X[self.history_column].to_list()\n",
    "            ]\n",
    "            pred_input_title = self.lookup_article_matrix[\n",
    "                batch_X[self.inview_col].to_list()\n",
    "            ]\n",
    "            pred_input_title = np.squeeze(pred_input_title, axis=2)\n",
    "\n",
    "        his_input_title = np.squeeze(his_input_title, axis=2)\n",
    "        return (his_input_title, pred_input_title), batch_y\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class NRMSDataLoaderPretransform(NewsrecDataLoader):\n",
    "    \"\"\"\n",
    "    In the __post_init__ pre-transform the entire DataFrame. This is useful for\n",
    "    when data can fit in memory, as it will be much faster ones training.\n",
    "    Note, it might not be as scaleable.\n",
    "    \"\"\"\n",
    "\n",
    "    def __post_init__(self):\n",
    "        super().__post_init__()\n",
    "        self.X = self.X.pipe(\n",
    "            map_list_article_id_to_value,\n",
    "            behaviors_column=self.history_column,\n",
    "            mapping=self.lookup_article_index,\n",
    "            fill_nulls=self.unknown_index,\n",
    "            drop_nulls=False,\n",
    "        ).pipe(\n",
    "            map_list_article_id_to_value,\n",
    "            behaviors_column=self.inview_col,\n",
    "            mapping=self.lookup_article_index,\n",
    "            fill_nulls=self.unknown_index,\n",
    "            drop_nulls=False,\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx) -> tuple[tuple[np.ndarray], np.ndarray]:\n",
    "        \"\"\"\n",
    "        his_input_title:    (samples, history_size, document_dimension)\n",
    "        pred_input_title:   (samples, npratio, document_dimension)\n",
    "        batch_y:            (samples, npratio)\n",
    "        \"\"\"\n",
    "        batch_X = self.X[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        # =>\n",
    "        if self.eval_mode:\n",
    "            repeats = np.array(batch_X[\"n_samples\"])\n",
    "            # =>\n",
    "            batch_y = np.array(batch_y.explode().to_list()).reshape(-1, 1)\n",
    "            # =>\n",
    "            his_input_title = repeat_by_list_values_from_matrix(\n",
    "                batch_X[self.history_column].to_list(),\n",
    "                matrix=self.lookup_article_matrix,\n",
    "                repeats=repeats,\n",
    "            )\n",
    "            # =>\n",
    "            pred_input_title = self.lookup_article_matrix[\n",
    "                batch_X[self.inview_col].explode().to_list()\n",
    "            ]\n",
    "        else:\n",
    "            batch_y = np.array(batch_y.to_list())\n",
    "            his_input_title = self.lookup_article_matrix[\n",
    "                batch_X[self.history_column].to_list()\n",
    "            ]\n",
    "            pred_input_title = self.lookup_article_matrix[\n",
    "                batch_X[self.inview_col].to_list()\n",
    "            ]\n",
    "            pred_input_title = np.squeeze(pred_input_title, axis=2)\n",
    "\n",
    "        his_input_title = np.squeeze(his_input_title, axis=2)\n",
    "        return (his_input_title, pred_input_title), batch_y\n",
    "\n",
    "\n",
    "@dataclass(kw_only=True)\n",
    "class LSTURDataLoader(NewsrecDataLoader):\n",
    "    \"\"\"\n",
    "    NPA and LSTUR shares the same DataLoader\n",
    "    \"\"\"\n",
    "\n",
    "    user_id_mapping: dict[int, int] = None\n",
    "    unknown_user_value: int = 0\n",
    "\n",
    "    def transform(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        return (\n",
    "            df.pipe(\n",
    "                map_list_article_id_to_value,\n",
    "                behaviors_column=self.history_column,\n",
    "                mapping=self.lookup_article_index,\n",
    "                fill_nulls=self.unknown_index,\n",
    "                drop_nulls=False,\n",
    "            )\n",
    "            .pipe(\n",
    "                map_list_article_id_to_value,\n",
    "                behaviors_column=self.inview_col,\n",
    "                mapping=self.lookup_article_index,\n",
    "                fill_nulls=self.unknown_index,\n",
    "                drop_nulls=False,\n",
    "            )\n",
    "            .with_columns(\n",
    "                pl.col(self.user_col).replace(\n",
    "                    self.user_id_mapping, default=self.unknown_user_value\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx) -> tuple[tuple[np.ndarray], np.ndarray]:\n",
    "        \"\"\"\n",
    "        user_indexes:       ()\n",
    "        his_input_title:    (samples, history_size, document_dimension)\n",
    "        pred_input_title:   (samples, npratio, document_dimension)\n",
    "        batch_y:            (samples, npratio)\n",
    "        \"\"\"\n",
    "        batch_X = self.X[idx * self.batch_size : (idx + 1) * self.batch_size].pipe(\n",
    "            self.transform\n",
    "        )\n",
    "        batch_y = self.y[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        # =>\n",
    "        if self.eval_mode:\n",
    "            repeats = np.array(batch_X[\"n_samples\"])\n",
    "            # =>\n",
    "            batch_y = np.array(batch_y.explode().to_list()).reshape(-1, 1)\n",
    "            # =>\n",
    "            user_indexes = np.array(\n",
    "                batch_X.select(\n",
    "                    pl.col(self.user_col).repeat_by(pl.col(\"n_samples\")).explode()\n",
    "                )[self.user_col].to_list()\n",
    "            ).reshape(-1, 1)\n",
    "            # =>\n",
    "            his_input_title = repeat_by_list_values_from_matrix(\n",
    "                batch_X[self.history_column].to_list(),\n",
    "                matrix=self.lookup_article_matrix,\n",
    "                repeats=repeats,\n",
    "            )\n",
    "            # =>\n",
    "            pred_input_title = self.lookup_article_matrix[\n",
    "                batch_X[self.inview_col].explode().to_list()\n",
    "            ]\n",
    "        else:\n",
    "            # =>\n",
    "            batch_y = np.array(batch_y.to_list())\n",
    "            # =>\n",
    "            user_indexes = np.array(batch_X[self.user_col].to_list()).reshape(-1, 1)\n",
    "            # =>\n",
    "            his_input_title = self.lookup_article_matrix[\n",
    "                batch_X[self.history_column].to_list()\n",
    "            ]\n",
    "            # =>\n",
    "            pred_input_title = self.lookup_article_matrix[\n",
    "                batch_X[self.inview_col].to_list()\n",
    "            ]\n",
    "            pred_input_title = np.squeeze(pred_input_title, axis=2)\n",
    "        # =>\n",
    "        his_input_title = np.squeeze(his_input_title, axis=2)\n",
    "        return (user_indexes, his_input_title, pred_input_title), batch_y\n",
    "\n",
    "\n",
    "@dataclass(kw_only=True)\n",
    "class NAMLDataLoader(NewsrecDataLoader):\n",
    "    \"\"\"\n",
    "    Eval mode not implemented\n",
    "    \"\"\"\n",
    "\n",
    "    unknown_category_value: int = 0\n",
    "    unknown_subcategory_value: int = 0\n",
    "    body_mapping: dict[int, list[int]] = None\n",
    "    category_mapping: dict[int, int] = None\n",
    "    subcategory_mapping: dict[int, int] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.title_prefix = \"title_\"\n",
    "        self.body_prefix = \"body_\"\n",
    "        self.category_prefix = \"category_\"\n",
    "        self.subcategory_prefix = \"subcategory_\"\n",
    "        (\n",
    "            self.lookup_article_index_body,\n",
    "            self.lookup_article_matrix_body,\n",
    "        ) = create_lookup_objects(\n",
    "            self.body_mapping, unknown_representation=self.unknown_representation\n",
    "        )\n",
    "        if self.eval_mode:\n",
    "            raise ValueError(\"'eval_mode = True' is not implemented for NAML\")\n",
    "\n",
    "        return super().__post_init__()\n",
    "\n",
    "    def transform(self, df: pl.DataFrame) -> tuple[pl.DataFrame]:\n",
    "        \"\"\"\n",
    "        Special case for NAML as it requires body-encoding, verticals, & subvertivals\n",
    "        \"\"\"\n",
    "        # =>\n",
    "        title = df.pipe(\n",
    "            map_list_article_id_to_value,\n",
    "            behaviors_column=self.history_column,\n",
    "            mapping=self.lookup_article_index,\n",
    "            fill_nulls=self.unknown_index,\n",
    "            drop_nulls=False,\n",
    "        ).pipe(\n",
    "            map_list_article_id_to_value,\n",
    "            behaviors_column=self.inview_col,\n",
    "            mapping=self.lookup_article_index,\n",
    "            fill_nulls=self.unknown_index,\n",
    "            drop_nulls=False,\n",
    "        )\n",
    "        # =>\n",
    "        body = df.pipe(\n",
    "            map_list_article_id_to_value,\n",
    "            behaviors_column=self.history_column,\n",
    "            mapping=self.lookup_article_index_body,\n",
    "            fill_nulls=self.unknown_index,\n",
    "            drop_nulls=False,\n",
    "        ).pipe(\n",
    "            map_list_article_id_to_value,\n",
    "            behaviors_column=self.inview_col,\n",
    "            mapping=self.lookup_article_index_body,\n",
    "            fill_nulls=self.unknown_index,\n",
    "            drop_nulls=False,\n",
    "        )\n",
    "        # =>\n",
    "        category = df.pipe(\n",
    "            map_list_article_id_to_value,\n",
    "            behaviors_column=self.history_column,\n",
    "            mapping=self.category_mapping,\n",
    "            fill_nulls=self.unknown_category_value,\n",
    "            drop_nulls=False,\n",
    "        ).pipe(\n",
    "            map_list_article_id_to_value,\n",
    "            behaviors_column=self.inview_col,\n",
    "            mapping=self.category_mapping,\n",
    "            fill_nulls=self.unknown_category_value,\n",
    "            drop_nulls=False,\n",
    "        )\n",
    "        # =>\n",
    "        subcategory = df.pipe(\n",
    "            map_list_article_id_to_value,\n",
    "            behaviors_column=self.history_column,\n",
    "            mapping=self.subcategory_mapping,\n",
    "            fill_nulls=self.unknown_subcategory_value,\n",
    "            drop_nulls=False,\n",
    "        ).pipe(\n",
    "            map_list_article_id_to_value,\n",
    "            behaviors_column=self.inview_col,\n",
    "            mapping=self.subcategory_mapping,\n",
    "            fill_nulls=self.unknown_subcategory_value,\n",
    "            drop_nulls=False,\n",
    "        )\n",
    "        return (\n",
    "            pl.DataFrame()\n",
    "            .with_columns(title.select(pl.all().name.prefix(self.title_prefix)))\n",
    "            .with_columns(body.select(pl.all().name.prefix(self.body_prefix)))\n",
    "            .with_columns(category.select(pl.all().name.prefix(self.category_prefix)))\n",
    "            .with_columns(\n",
    "                subcategory.select(pl.all().name.prefix(self.subcategory_prefix))\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx) -> tuple[tuple[np.ndarray], np.ndarray]:\n",
    "        batch_X = self.X[idx * self.batch_size : (idx + 1) * self.batch_size].pipe(\n",
    "            self.transform\n",
    "        )\n",
    "        batch_y = self.y[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        # =>\n",
    "        batch_y = np.array(batch_y.to_list())\n",
    "        his_input_title = np.array(\n",
    "            batch_X[self.title_prefix + self.history_column].to_list()\n",
    "        )\n",
    "        his_input_body = np.array(\n",
    "            batch_X[self.body_prefix + self.history_column].to_list()\n",
    "        )\n",
    "        his_input_vert = np.array(\n",
    "            batch_X[self.category_prefix + self.history_column].to_list()\n",
    "        )[:, :, np.newaxis]\n",
    "        his_input_subvert = np.array(\n",
    "            batch_X[self.subcategory_prefix + self.history_column].to_list()\n",
    "        )[:, :, np.newaxis]\n",
    "        # =>\n",
    "        pred_input_title = np.array(\n",
    "            batch_X[self.title_prefix + self.inview_col].to_list()\n",
    "        )\n",
    "        pred_input_body = np.array(\n",
    "            batch_X[self.body_prefix + self.inview_col].to_list()\n",
    "        )\n",
    "        pred_input_vert = np.array(\n",
    "            batch_X[self.category_prefix + self.inview_col].to_list()\n",
    "        )[:, :, np.newaxis]\n",
    "        pred_input_subvert = np.array(\n",
    "            batch_X[self.subcategory_prefix + self.inview_col].to_list()\n",
    "        )[:, :, np.newaxis]\n",
    "        # =>\n",
    "        his_input_title = np.squeeze(\n",
    "            self.lookup_article_matrix[his_input_title], axis=2\n",
    "        )\n",
    "        pred_input_title = np.squeeze(\n",
    "            self.lookup_article_matrix[pred_input_title], axis=2\n",
    "        )\n",
    "        his_input_body = np.squeeze(\n",
    "            self.lookup_article_matrix_body[his_input_body], axis=2\n",
    "        )\n",
    "        pred_input_body = np.squeeze(\n",
    "            self.lookup_article_matrix_body[pred_input_body], axis=2\n",
    "        )\n",
    "        # =>\n",
    "        return (\n",
    "            his_input_title,\n",
    "            his_input_body,\n",
    "            his_input_vert,\n",
    "            his_input_subvert,\n",
    "            pred_input_title,\n",
    "            pred_input_body,\n",
    "            pred_input_vert,\n",
    "            pred_input_subvert,\n",
    "        ), batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bilba\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "\n",
    "\n",
    "def get_transformers_word_embeddings(model):\n",
    "    \"\"\"\n",
    "    Extracts the word embeddings from a pre-trained transformer model.\n",
    "    For TensorFlow models, this uses the `model.get_input_embeddings()` method\n",
    "    to retrieve the embedding layer.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The word embeddings as a NumPy array.\n",
    "    \"\"\"\n",
    "    embedding_layer = model.get_input_embeddings()  # Get the embedding layer\n",
    "    # Convert to NumPy\n",
    "    return embedding_layer.weights[0].numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Protocol\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from typing import Iterable\n",
    "import numpy as np\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "#ebrec/evaluation/metrics/_sklearn.py\n",
    "\n",
    "try:\n",
    "    from sklearn.metrics import (\n",
    "        # _regression:\n",
    "        mean_squared_error,\n",
    "        # _ranking:\n",
    "        roc_auc_score,\n",
    "        # _classification:\n",
    "        accuracy_score,\n",
    "        f1_score,\n",
    "        log_loss,\n",
    "    )\n",
    "except ImportError:\n",
    "    print(\"sklearn not available\")\n",
    "\n",
    "\n",
    "#ebrec/evaluation/metrics/_ranking.py\n",
    "\n",
    "\n",
    "def reciprocal_rank_score(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"Computes the Mean Reciprocal Rank (MRR) score.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.ndarray): A 1D array of ground-truth labels. These should be binary (0 or 1),\n",
    "                                where 1 indicates the relevant item.\n",
    "        y_pred (np.ndarray): A 1D array of predicted scores. These scores indicate the likelihood\n",
    "                                of items being relevant.\n",
    "\n",
    "    Returns:\n",
    "        float: The mean reciprocal rank (MRR) score.\n",
    "\n",
    "    Note:\n",
    "        Both `y_true` and `y_pred` should be 1D arrays of the same length.\n",
    "        The function assumes higher scores in `y_pred` indicate higher relevance.\n",
    "\n",
    "    Examples:\n",
    "        >>> y_true_1 = np.array([0, 0, 1])\n",
    "        >>> y_pred_1 = np.array([0.5, 0.2, 0.1])\n",
    "        >>> reciprocal_rank_score(y_true_1, y_pred_1)\n",
    "            0.33\n",
    "\n",
    "        >>> y_true_2 = np.array([0, 1, 1])\n",
    "        >>> y_pred_2 = np.array([0.5, 0.2, 0.1])\n",
    "        >>> reciprocal_rank_score(y_true_2, y_pred_2)\n",
    "            0.5\n",
    "\n",
    "        >>> y_true_3 = np.array([1, 1, 0])\n",
    "        >>> y_pred_3 = np.array([0.5, 0.2, 0.1])\n",
    "        >>> reciprocal_rank_score(y_true_3, y_pred_3)\n",
    "            1.0\n",
    "\n",
    "        >>> np.mean(\n",
    "                [\n",
    "                    reciprocal_rank_score(y_true, y_pred)\n",
    "                    for y_true, y_pred in zip(\n",
    "                        [y_true_1, y_true_2, y_true_3], [y_pred_1, y_pred_2, y_pred_3]\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "            0.61\n",
    "            mrr_score([y_true_1, y_true_2, y_true_3], [y_pred_1, y_pred_2, y_pred_3])\n",
    "    \"\"\"\n",
    "    order = np.argsort(y_pred)[::-1]\n",
    "    y_true = np.take(y_true, order)\n",
    "    first_positive_rank = np.argmax(y_true) + 1\n",
    "    return 1.0 / first_positive_rank\n",
    "\n",
    "\n",
    "def dcg_score(y_true: np.ndarray, y_pred: np.ndarray, k: int = 10) -> float:\n",
    "    \"\"\"\n",
    "    Compute the Discounted Cumulative Gain (DCG) score at a particular rank `k`.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.ndarray): A 1D or 2D array of ground-truth relevance labels.\n",
    "                            Each element should be a non-negative integer.\n",
    "        y_pred (np.ndarray): A 1D or 2D array of predicted scores. Each element is\n",
    "                            a score corresponding to the predicted relevance.\n",
    "        k (int, optional): The rank at which the DCG score is calculated. Defaults\n",
    "                            to 10. If `k` is larger than the number of elements, it\n",
    "                            will be truncated to the number of elements.\n",
    "\n",
    "    Note:\n",
    "        In case of a 2D array, each row represents a different sample.\n",
    "\n",
    "    Returns:\n",
    "        float: The calculated DCG score for the top `k` elements.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If `y_true` and `y_pred` have different shapes.\n",
    "\n",
    "    Examples:\n",
    "        >>> from sklearn.metrics import dcg_score as dcg_score_sklearn\n",
    "        >>> y_true = np.array([1, 0, 0, 1, 0])\n",
    "        >>> y_pred = np.array([0.5, 0.2, 0.1, 0.8, 0.4])\n",
    "        >>> dcg_score(y_true, y_pred)\n",
    "            1.6309297535714575\n",
    "        >>> dcg_score_sklearn([y_true], [y_pred])\n",
    "            1.6309297535714573\n",
    "    \"\"\"\n",
    "    k = min(np.shape(y_true)[-1], k)\n",
    "    order = np.argsort(y_pred)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "    gains = 2**y_true - 1\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "\n",
    "def ndcg_score(y_true: np.ndarray, y_pred: np.ndarray, k: int = 10) -> float:\n",
    "    \"\"\"\n",
    "    Compute the Normalized Discounted Cumulative Gain (NDCG) score at a rank `k`.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.ndarray): A 1D or 2D array of ground-truth relevance labels.\n",
    "                            Each element should be a non-negative integer. In case\n",
    "                            of a 2D array, each row represents a different sample.\n",
    "        y_pred (np.ndarray): A 1D or 2D array of predicted scores. Each element is\n",
    "                            a score corresponding to the predicted relevance. The\n",
    "                            array should have the same shape as `y_true`.\n",
    "        k (int, optional): The rank at which the NDCG score is calculated. Defaults\n",
    "                            to 10. If `k` is larger than the number of elements, it\n",
    "                            will be truncated to the number of elements.\n",
    "\n",
    "    Returns:\n",
    "        float: The calculated NDCG score for the top `k` elements. The score ranges\n",
    "                from 0 to 1, with 1 representing the perfect ranking.\n",
    "\n",
    "    Examples:\n",
    "        >>> from sklearn.metrics import ndcg_score as ndcg_score_sklearn\n",
    "        >>> y_true = np.array([1, 0, 0, 1, 0])\n",
    "        >>> y_pred = np.array([0.1, 0.2, 0.1, 0.8, 0.4])\n",
    "        >>> ndcg_score([y_true], [y_pred])\n",
    "            0.863780110436402\n",
    "        >>> ndcg_score_sklearn([y_true], [y_pred])\n",
    "            0.863780110436402\n",
    "        >>>\n",
    "    \"\"\"\n",
    "    best = dcg_score(y_true, y_true, k)\n",
    "    actual = dcg_score(y_true, y_pred, k)\n",
    "    return actual / best\n",
    "\n",
    "\n",
    "def mrr_score(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"Computes the Mean Reciprocal Rank (MRR) score.\n",
    "\n",
    "    THIS MIGHT NOT ALL PROPER, TO BE DETERMIEND:\n",
    "        - https://github.com/recommenders-team/recommenders/issues/2141\n",
    "\n",
    "    Args:\n",
    "        y_true (np.ndarray): A 1D array of ground-truth labels. These should be binary (0 or 1),\n",
    "                                where 1 indicates the relevant item.\n",
    "        y_pred (np.ndarray): A 1D array of predicted scores. These scores indicate the likelihood\n",
    "                                of items being relevant.\n",
    "\n",
    "    Returns:\n",
    "        float: The mean reciprocal rank (MRR) score.\n",
    "\n",
    "    Note:\n",
    "        Both `y_true` and `y_pred` should be 1D arrays of the same length.\n",
    "        The function assumes higher scores in `y_pred` indicate higher relevance.\n",
    "\n",
    "    Examples:\n",
    "        >>> y_true = np.array([[1, 0, 0, 1, 0]])\n",
    "        >>> y_pred = np.array([[0.5, 0.2, 0.1, 0.8, 0.4]])\n",
    "        >>> mrr_score(y_true, y_pred)\n",
    "            0.75\n",
    "\n",
    "    \"\"\"\n",
    "    order = np.argsort(y_pred)[::-1]\n",
    "    y_true = np.take(y_true, order)\n",
    "    rr_score = y_true / (np.arange(len(y_true)) + 1)\n",
    "    return np.sum(rr_score) / np.sum(y_true)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#ebrec/evaluation/metric/_classification.py\n",
    "\n",
    "\n",
    "def auc_score_custom(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Computes the Area Under the Curve (AUC) score for the Receiver Operating Characteristic (ROC) curve using a\n",
    "    custom method. This implementation is particularly useful for understanding basic ROC curve properties and\n",
    "    for educational purposes to demonstrate how AUC scores can be manually calculated.\n",
    "\n",
    "    This function may produce slightly different results compared to standard library implementations (e.g., sklearn's roc_auc_score)\n",
    "    in cases where positive and negative predictions have the same score. The function treats the problem as a binary classification task,\n",
    "    comparing the prediction scores for positive instances against those for negative instances directly.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.ndarray): A binary array indicating the true classification (1 for positive class and 0 for negative class).\n",
    "        y_pred (np.ndarray): An array of scores as predicted by a model, indicating the likelihood of each instance being positive.\n",
    "\n",
    "    Returns:\n",
    "        float: The calculated AUC score, representing the probability that a randomly chosen positive instance is ranked\n",
    "                higher than a randomly chosen negative instance based on the prediction scores.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If `y_true` and `y_pred` do not have the same length or if they contain invalid data types.\n",
    "\n",
    "    Examples:\n",
    "        >>> y_true = np.array([1, 1, 0, 0, 1, 0, 0, 0])\n",
    "        >>> y_pred = np.array([0.9999, 0.9838, 0.5747, 0.8485, 0.8624, 0.4502, 0.3357, 0.8985])\n",
    "        >>> auc_score_custom(y_true, y_pred)\n",
    "            0.9333333333333333\n",
    "        >>> from sklearn.metrics import roc_auc_score\n",
    "        >>> roc_auc_score(y_true, y_pred)\n",
    "            0.9333333333333333\n",
    "\n",
    "        An error will occur when pos/neg prediction have same score:\n",
    "        >>> y_true = np.array([1, 1, 0, 0, 1, 0, 0, 0])\n",
    "        >>> y_pred = np.array([0.9999, 0.8, 0.8, 0.8485, 0.8624, 0.4502, 0.3357, 0.8985])\n",
    "        >>> auc_score_custom(y_true, y_pred)\n",
    "            0.7333\n",
    "        >>> roc_auc_score(y_true, y_pred)\n",
    "            0.7667\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    y_true_bool = y_true.astype(np.bool_)\n",
    "    # Index:\n",
    "    pos_scores = y_pred[y_true_bool]\n",
    "    neg_scores = y_pred[np.logical_not(y_true_bool)]\n",
    "    # Arrange:\n",
    "    pos_scores = np.repeat(pos_scores, len(neg_scores))\n",
    "    neg_scores = np.tile(neg_scores, sum(y_true_bool))\n",
    "    assert len(neg_scores) == len(pos_scores)\n",
    "    return (pos_scores > neg_scores).sum() / len(neg_scores)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#ebrec/evaluation/metrics/beyond_accuracy.py\n",
    "\n",
    "def intralist_diversity(\n",
    "    R: np.ndarray,\n",
    "    pairwise_distance_function: Callable = cosine_distances,\n",
    ") -> float:\n",
    "    \"\"\"Calculate the intra-list diversity of a recommendation list.\n",
    "\n",
    "    This function implements the method described by Smyth and McClave (2001) to\n",
    "    measure the diversity within a recommendation list. It calculates the average\n",
    "    pairwise distance between all items in the list.\n",
    "\n",
    "    Args:\n",
    "        R (np.ndarray): A 2D numpy array where each row represents a recommendation.\n",
    "            This array should be either array-like or a sparse matrix, with shape (n_samples_X, n_features).\n",
    "        pairwise_distance_function (Callable, optional): A function to compute pairwise distance\n",
    "            between samples. Defaults to `cosine_distances`.\n",
    "\n",
    "    Returns:\n",
    "        float: The calculated diversity score. If the recommendation list contains less than or\n",
    "            equal to one item, NaN is returned to signify an undefined diversity score.\n",
    "\n",
    "    Examples:\n",
    "        >>> R1 = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\n",
    "        >>> print(intralist_diversity(R1))\n",
    "            0.022588438516842262\n",
    "        >>> print(intralist_diversity(np.array([[0.1, 0.2], [0.1, 0.2]])))\n",
    "            1.1102230246251565e-16\n",
    "    \"\"\"\n",
    "    R_n = R.shape[0]  # number of recommendations\n",
    "    if R_n <= 1:\n",
    "        # Less than or equal to 1 recommendations in recommendation list\n",
    "        diversity = np.nan\n",
    "    else:\n",
    "        pairwise_distances = pairwise_distance_function(R, R)\n",
    "        diversity = np.sum(pairwise_distances) / (R_n * (R_n - 1))\n",
    "    return diversity\n",
    "\n",
    "\n",
    "def serendipity(\n",
    "    R: np.ndarray,\n",
    "    H: np.ndarray,\n",
    "    pairwise_distance_function: Callable = cosine_distances,\n",
    ") -> float:\n",
    "    \"\"\"Calculate the serendipity score between a set of recommendations and user's reading history.\n",
    "\n",
    "    This function implements the concept of serendipity as defined by Feng Lu, Anca Dumitrache, and David Graus (2020).\n",
    "    Serendipity in this context is measured as the mean distance between the items in the recommendation list and the\n",
    "    user's reading history.\n",
    "\n",
    "    Args:\n",
    "        R (np.ndarray): A 2D numpy array representing the recommendation list, where each row is a recommendation.\n",
    "            It should be either array-like or a sparse matrix, with shape (n_samples_X, n_features).\n",
    "        H (np.ndarray): A 2D numpy array representing the user's reading history, with the same format as R.\n",
    "        pairwise_distance_function (Callable, optional): A function to compute pairwise distance between samples.\n",
    "            Defaults to `cosine_distances`.\n",
    "\n",
    "    Returns:\n",
    "        float: The calculated serendipity score.\n",
    "\n",
    "    References:\n",
    "        Lu, F., Dumitrache, A., & Graus, D. (2020). Beyond Optimizing for Clicks: Incorporating Editorial Values in News Recommendation.\n",
    "        Retrieved from https://arxiv.org/abs/2004.09980\n",
    "\n",
    "    Examples:\n",
    "        >>> R1 = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\n",
    "        >>> H1 = np.array([[0.7, 0.8, 0.9], [0.1, 0.2, 0.3]])\n",
    "        >>> print(serendipity(R1, H1))\n",
    "            0.016941328887631724\n",
    "    \"\"\"\n",
    "    # Compute the pairwise distances between each vector:\n",
    "    dists = pairwise_distance_function(R, H)\n",
    "    # Compute serendipity:\n",
    "    return np.mean(dists)\n",
    "\n",
    "\n",
    "def coverage_count(R: np.ndarray) -> int:\n",
    "    \"\"\"Calculate the number of distinct items in a recommendation list.\n",
    "\n",
    "    Args:\n",
    "        R (np.ndarray): An array containing the items in the recommendation list.\n",
    "\n",
    "    Returns:\n",
    "        int: The count of distinct items in the recommendation list.\n",
    "\n",
    "    Examples:\n",
    "        >>> R1 = np.array([1, 2, 3, 4, 5, 5, 6])\n",
    "        >>> print(coverage_count(R1))\n",
    "            6\n",
    "    \"\"\"\n",
    "    # Distinct items:\n",
    "    return np.unique(R).size\n",
    "\n",
    "\n",
    "def coverage_fraction(R: np.ndarray, C: np.ndarray) -> float:\n",
    "    \"\"\"Calculate the fraction of distinct items in the recommendation list compared to a universal set.\n",
    "\n",
    "    Args:\n",
    "        R (np.ndarray): An array containing the items in the recommendation list.\n",
    "        C (np.ndarray): An array representing the universal set of items.\n",
    "            It should contain all possible items that can be recommended.\n",
    "\n",
    "    Returns:\n",
    "        float: The fraction representing the coverage of the recommendation system.\n",
    "            This is calculated as the size of unique elements in R divided by the size of unique elements in C.\n",
    "\n",
    "    Examples:\n",
    "        >>> R1 = np.array([1, 2, 3, 4, 5, 5, 6])\n",
    "        >>> C1 = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "        >>> print(coverage_fraction(R1, C1))  # Expected output: 0.6\n",
    "            0.6\n",
    "    \"\"\"\n",
    "    # Distinct items:\n",
    "    return np.unique(R).size / np.unique(C).size\n",
    "\n",
    "\n",
    "def novelty(R: np.ndarray) -> float:\n",
    "    \"\"\"Calculate the novelty score of recommendations based on their popularity.\n",
    "\n",
    "    This function computes the novelty score for a set of recommendations by applying the self-information popularity metric.\n",
    "    It uses the formula described by Zhou et al. (2010) and Vargas and Castells (2011). The novelty is calculated as the\n",
    "    average negative logarithm (base 2) of the popularity scores of the items in the recommendation list.\n",
    "\n",
    "    Formula:\n",
    "        Novelty(R) = ( sum_{iR} -log2( p_i ) / ( |R| )\n",
    "\n",
    "    where p_i represents the popularity score of each item in the recommendation list R, and |R| is the size of R.\n",
    "\n",
    "    Args:\n",
    "        R (np.ndarray): An array of popularity scores (p_i) for each item in the recommendation list.\n",
    "\n",
    "    Returns:\n",
    "        float: The calculated novelty score. Higher values indicate less popular (more novel) recommendations.\n",
    "\n",
    "    References:\n",
    "        Zhou et al. (2010).\n",
    "        Vargas & Castells (2011).\n",
    "\n",
    "    Examples:\n",
    "        >>> print(novelty(np.array([0.1, 0.2, 0.3, 0.4, 0.5])))  # Expected: High score (low popularity scores)\n",
    "            1.9405499757656586\n",
    "        >>> print(novelty(np.array([0.9, 0.9, 0.9, 1.0, 0.5])))  # Expected: Low score (high popularity scores)\n",
    "            0.29120185606703\n",
    "    \"\"\"\n",
    "    return np.mean(-np.log2(R))\n",
    "\n",
    "\n",
    "def index_of_dispersion(x: list[int]) -> float:\n",
    "    \"\"\"\n",
    "    Computes the Index of Dispersion (variance-to-mean ratio) for a given dataset of nominal variables.\n",
    "\n",
    "    The Index of Dispersion is a statistical measure used to quantify the dispersion or variability of a distribution\n",
    "    relative to its mean. It's particularly useful in identifying whether a dataset follows a Poisson distribution,\n",
    "    where the Index of Dispersion would be approximately 1.\n",
    "\n",
    "    Formula:\n",
    "        D = ( k * (N^2 - f^2) ) / ( N^2 * (k-1) )\n",
    "    Where:\n",
    "        k = number of categories in the data set (including categories with zero items),\n",
    "        N = number of items in the set,\n",
    "        f = number of frequencies or ratings,\n",
    "        f^2 = sum of squared frequencies/ratings.\n",
    "\n",
    "    Args:\n",
    "        x (list[int]): A list of integers representing frequencies or counts of occurrences in different categories.\n",
    "                        Each integer in the list corresponds to the count of occurrences in a given category.\n",
    "\n",
    "    Returns:\n",
    "        float: The Index of Dispersion for the dataset. Returns `np.nan` if the input list contains only one item,\n",
    "                indicating an undefined Index of Dispersion. Returns 0 if there's only one category present in the dataset.\n",
    "\n",
    "    References:\n",
    "        Walker, 1999, Statistics in criminal\n",
    "        Source: https://www.statisticshowto.com/index-of-dispersion/\n",
    "\n",
    "    Examples:\n",
    "        Given the following categories: Math(25), Economics(42), Chemistry(13), Physical Education (8), Religious Studies (13).\n",
    "        >>> N = np.sum(25+42+13+8+13)\n",
    "        >>> k = 5\n",
    "        >>> sq_f2 = np.sum(25**2 + 42**2 + 13**2 + 8**2 + 13**2)\n",
    "        >>> iod = ( k * (N**2 - sq_f2)) / ( N**2 * (k-1) )\n",
    "            0.9079992157631604\n",
    "\n",
    "        Validate method:\n",
    "        >>> cat = [[1]*25, [2]*42, [3]*13, [4]*8, [5]*13]\n",
    "        >>> flat_list = [item for sublist in cat for item in sublist]\n",
    "        >>> index_of_dispersion(flat_list)\n",
    "            0.9079992157631604\n",
    "    \"\"\"\n",
    "    # number of items\n",
    "    N = len(x)\n",
    "    # compute frequencies\n",
    "    count = Counter(x)\n",
    "    # number of categories\n",
    "    k = len(count)\n",
    "    if k == 1:\n",
    "        if N == 1:\n",
    "            return np.nan\n",
    "        else:\n",
    "            return 0\n",
    "    # squared frequencies\n",
    "    f_squared = [count.get(f) ** 2 for f in count]\n",
    "    # compute Index of Dispersion\n",
    "    D = k * (N**2 - sum(f_squared)) / (N**2 * (k - 1))\n",
    "    return D\n",
    "\n",
    "#from ebrec.evaluation.utils import convert_to_binary\n",
    "\n",
    "def convert_to_binary(y_pred: np.ndarray, threshold: float):\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    y_pred[y_pred >= threshold] = 1\n",
    "    y_pred[y_pred < threshold] = 0\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def is_iterable_nested_dtype(iterable: Iterable[any], dtypes) -> bool:\n",
    "    \"\"\"\n",
    "    Check whether iterable is a nested with dtype,\n",
    "    note, we assume all types in iterable are the the same.\n",
    "    Check all cases: any(isinstance(i, dtypes) for i in a)\n",
    "\n",
    "    Args:\n",
    "        iterable (Iterable[Any]): iterable (list, array, tuple) of any type of data\n",
    "        dtypes (Tuple): tuple of possible dtypes, e.g. dtypes = (list, np.ndarray)\n",
    "    Returns:\n",
    "        bool: boolean whether it is true or false\n",
    "\n",
    "    Examples:\n",
    "        >>> is_iterable_nested_dtype([1, 2, 3], list)\n",
    "            False\n",
    "        >>> is_iterable_nested_dtype([1, 2, 3], (list, int))\n",
    "            True\n",
    "        >>> is_iterable_nested_dtype([[1], [2], [3]], list)\n",
    "            True\n",
    "    \"\"\"\n",
    "    return isinstance(iterable[0], dtypes)\n",
    "\n",
    "\n",
    "def compute_combinations(n: int, r: int) -> int:\n",
    "    \"\"\"Compute Combinations where order does not matter (without replacement)\n",
    "\n",
    "    Source: https://www.statskingdom.com/combinations-calculator.html\n",
    "    Args:\n",
    "        n (int): number of items\n",
    "        r (int): number of items being chosen at a time\n",
    "    Returns:\n",
    "        int: number of possible combinations\n",
    "\n",
    "    Formula:\n",
    "    * nCr = n! / ( (n - r)! * r! )\n",
    "\n",
    "    Assume the following:\n",
    "    * we sample without replacement of items\n",
    "    * order of the outcomes does NOT matter\n",
    "    \"\"\"\n",
    "    return int(\n",
    "        (np.math.factorial(n)) / (np.math.factorial(n - r) * np.math.factorial(r))\n",
    "    )\n",
    "\n",
    "\n",
    "def scale_range(\n",
    "    m: np.ndarray,\n",
    "    r_min: float = None,\n",
    "    r_max: float = None,\n",
    "    t_min: float = 0,\n",
    "    t_max: float = 1.0,\n",
    ") -> None:\n",
    "    \"\"\"Scale an array between a range\n",
    "    Source: https://stats.stackexchange.com/questions/281162/scale-a-number-between-a-range\n",
    "\n",
    "    m -> ((m-r_min)/(r_max-r_min)) * (t_max-t_min) + t_min\n",
    "\n",
    "    Args:\n",
    "        m  [r_min,r_max] denote your measurements to be scaled\n",
    "        r_min denote the minimum of the range of your measurement\n",
    "        r_max denote the maximum of the range of your measurement\n",
    "        t_min denote the minimum of the range of your desired target scaling\n",
    "        t_max denote the maximum of the range of your desired target scaling\n",
    "    \"\"\"\n",
    "    if not r_min:\n",
    "        r_min = np.min(m)\n",
    "    if not r_max:\n",
    "        r_max = np.max(m)\n",
    "    return ((m - r_min) / (r_max - r_min)) * (t_max - t_min) + t_min\n",
    "\n",
    "\n",
    "# utils for\n",
    "def compute_item_popularity_scores(R: Iterable[np.ndarray]) -> dict[str, float]:\n",
    "    \"\"\"Compute popularity scores for items based on their occurrence in user interactions.\n",
    "\n",
    "    This function calculates the popularity score of each item as the fraction of users who have interacted with that item.\n",
    "    The popularity score, p_i, for an item is defined as the number of users who have interacted with the item divided by the\n",
    "    total number of users.\n",
    "\n",
    "    Formula:\n",
    "        p_i = | {u  U}, r_ui !=  | / |U|\n",
    "\n",
    "    where p_i is the popularity score of an item, U is the total number of users, and r_ui is the interaction of user u with item i (non-zero\n",
    "    interaction implies the user has seen the item).\n",
    "\n",
    "    Note:\n",
    "        Each entry can only have the same item ones. TODO - ADD THE TEXT DONE HERE.\n",
    "\n",
    "    Args:\n",
    "        R (Iterable[np.ndarray]): An iterable of numpy arrays, where each array represents the items interacted with by a single user.\n",
    "            Each element in the array should be a string identifier for an item.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, float]: A dictionary where keys are item identifiers and values are their corresponding popularity scores (as floats).\n",
    "\n",
    "    Examples:\n",
    "    >>> R = [\n",
    "            np.array([\"item1\", \"item2\", \"item3\"]),\n",
    "            np.array([\"item1\", \"item3\"]),\n",
    "            np.array([\"item1\", \"item4\"]),\n",
    "        ]\n",
    "    >>> print(popularity_scores(R))\n",
    "        {'item1': 1.0, 'item2': 0.3333333333333333, 'item3': 0.6666666666666666, 'item4': 0.3333333333333333}\n",
    "    \"\"\"\n",
    "    U = len(R)\n",
    "    R_flatten = np.concatenate(R)\n",
    "    item_counts = Counter(R_flatten)\n",
    "    return {item: (r_ui / U) for item, r_ui in item_counts.items()}\n",
    "\n",
    "\n",
    "def compute_normalized_distribution(\n",
    "    R: np.ndarray,\n",
    "    weights: np.ndarray = None,\n",
    "    distribution: dict = None,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Compute a normalized weighted distribution for a list of items that each can have a single representation assigned.\n",
    "\n",
    "    Args:\n",
    "        R (np.ndarray): An array of items representation.\n",
    "        weights (np.ndarray, optional): Weights to assign each element in R. Defaults to None.\n",
    "            * If None, equal weights are assigned to all elements.\n",
    "        distribution (dict, optional): Dictionary to accumulate distribution values. Defaults to None.\n",
    "            * If None, a new dictionary is created.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with normalized distribution values.\n",
    "\n",
    "    Examples:\n",
    "        >>> R = np.array([\"a\", \"b\", \"c\", \"c\"])\n",
    "        >>> compute_normalized_distribution(R)\n",
    "            {'a': 0.25, 'b': 0.25, 'c': 0.5}\n",
    "    \"\"\"\n",
    "    n_elements = len(R)\n",
    "\n",
    "    # Use existing distribution or create a new one\n",
    "    distr = distribution if distribution is not None else {}\n",
    "    \n",
    "    # Assign equal weights if weights are not provided\n",
    "    weights = weights if weights is not None else np.ones(n_elements) / n_elements\n",
    "    \n",
    "    for item, weight in zip(R, weights):\n",
    "        distr[item] = weight + distr.get(item, 0.0)\n",
    "    \n",
    "    return distr\n",
    "\n",
    "\n",
    "\n",
    "def get_keys_in_dict(id_list: any, dictionary: dict) -> list[any]:\n",
    "    \"\"\"\n",
    "    Returns a list of IDs from id_list that are keys in the dictionary.\n",
    "    Args:\n",
    "        id_list (List[Any]): List of IDs to check against the dictionary.\n",
    "        dictionary (Dict[Any, Any]): Dictionary where keys are checked against the IDs.\n",
    "\n",
    "    Returns:\n",
    "        List[Any]: List of IDs that are also keys in the dictionary.\n",
    "\n",
    "    Examples:\n",
    "        >>> get_keys_in_dict(['a', 'b', 'c'], {'a': 1, 'c': 3, 'd': 4})\n",
    "            ['a', 'c']\n",
    "    \"\"\"\n",
    "    return [id_ for id_ in id_list if id_ in dictionary]\n",
    "\n",
    "\n",
    "def check_key_in_all_nested_dicts(dictionary: dict, key: str) -> None:\n",
    "    \"\"\"\n",
    "    Checks if the given key is present in all nested dictionaries within the main dictionary.\n",
    "    Raises a ValueError if the key is not found in any of the nested dictionaries.\n",
    "\n",
    "    Args:\n",
    "        dictionary (dict): The dictionary containing nested dictionaries to check.\n",
    "        key (str): The key to look for in all nested dictionaries.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the key is not present in any of the nested dictionaries.\n",
    "\n",
    "    Example:\n",
    "        >>> nested_dict = {\n",
    "                \"101\": {\"name\": \"Alice\", \"age\": 30},\n",
    "                \"102\": {\"name\": \"Bob\", \"age\": 25},\n",
    "            }\n",
    "        >>> check_key_in_all_nested_dicts(nested_dict, \"age\")\n",
    "        # No error is raised\n",
    "        >>> check_key_in_all_nested_dicts(nested_dict, \"salary\")\n",
    "        # Raises ValueError: 'salary is not present in all nested dictionaries.'\n",
    "    \"\"\"\n",
    "    for dict_key, sub_dict in dictionary.items():\n",
    "        if not isinstance(sub_dict, dict) or key not in sub_dict:\n",
    "            raise ValueError(\n",
    "                f\"'{key}' is not present in '{dict_key}' nested dictionary.\"\n",
    "            )\n",
    "\n",
    "#ebrec/evaluation/protocols.py\n",
    "\n",
    "class Metric(Protocol):\n",
    "    name: str\n",
    "\n",
    "    def calculate(self, y_true: np.ndarray, y_score: np.ndarray) -> float: ...\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"<Callable Metric: {self.name}>: params: {self.__dict__}\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return str(self)\n",
    "\n",
    "    def __call__(self, y_true: np.ndarray, y_score: np.ndarray) -> float:\n",
    "        return self.calculate(y_true, y_score)\n",
    "\n",
    "class AccuracyScore(Metric):\n",
    "    def __init__(self, threshold: float = 0.5):\n",
    "        self.threshold = threshold\n",
    "        self.name = \"accuracy\"\n",
    "\n",
    "    def calculate(self, y_true: list[np.ndarray], y_pred: list[np.ndarray]) -> float:\n",
    "        res = np.mean(\n",
    "            [\n",
    "                accuracy_score(\n",
    "                    each_labels, convert_to_binary(each_preds, self.threshold)\n",
    "                )\n",
    "                for each_labels, each_preds in tqdm(\n",
    "                    zip(y_true, y_pred), ncols=80, total=len(y_true), desc=\"AUC\"\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        return float(res)\n",
    "\n",
    "\n",
    "class F1Score(Metric):\n",
    "    def __init__(self, threshold: float = 0.5):\n",
    "        self.threshold = threshold\n",
    "        self.name = \"f1\"\n",
    "\n",
    "    def calculate(self, y_true: list[np.ndarray], y_pred: list[np.ndarray]) -> float:\n",
    "        res = np.mean(\n",
    "            [\n",
    "                f1_score(each_labels, convert_to_binary(each_preds, self.threshold))\n",
    "                for each_labels, each_preds in tqdm(\n",
    "                    zip(y_true, y_pred), ncols=80, total=len(y_true), desc=\"AUC\"\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        return float(res)\n",
    "\n",
    "\n",
    "class RootMeanSquaredError(Metric):\n",
    "    def __init__(self):\n",
    "        self.name = \"rmse\"\n",
    "\n",
    "    def calculate(self, y_true: list[np.ndarray], y_pred: list[np.ndarray]) -> float:\n",
    "        res = np.mean(\n",
    "            [\n",
    "                np.sqrt(mean_squared_error(each_labels, each_preds))\n",
    "                for each_labels, each_preds in tqdm(\n",
    "                    zip(y_true, y_pred), ncols=80, total=len(y_true), desc=\"AUC\"\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        return float(res)\n",
    "\n",
    "\n",
    "class AucScore(Metric):\n",
    "    def __init__(self):\n",
    "        self.name = \"auc\"\n",
    "\n",
    "    def calculate(self, y_true: list[np.ndarray], y_pred: list[np.ndarray]) -> float:\n",
    "        res = np.mean(\n",
    "            [\n",
    "                roc_auc_score(each_labels, each_preds)\n",
    "                for each_labels, each_preds in tqdm(\n",
    "                    zip(y_true, y_pred), ncols=80, total=len(y_true), desc=\"AUC\"\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        return float(res)\n",
    "\n",
    "\n",
    "class LogLossScore(Metric):\n",
    "    def __init__(self):\n",
    "        self.name = \"logloss\"\n",
    "\n",
    "    def calculate(self, y_true: list[np.ndarray], y_pred: list[np.ndarray]) -> float:\n",
    "        res = np.mean(\n",
    "            [\n",
    "                log_loss(\n",
    "                    each_labels,\n",
    "                    [max(min(p, 1.0 - 10e-12), 10e-12) for p in each_preds],\n",
    "                )\n",
    "                for each_labels, each_preds in tqdm(\n",
    "                    zip(y_true, y_pred), ncols=80, total=len(y_true), desc=\"AUC\"\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        return float(res)\n",
    "\n",
    "\n",
    "class MrrScore(Metric):\n",
    "    def __init__(self) -> Metric:\n",
    "        self.name = \"mrr\"\n",
    "\n",
    "    def calculate(self, y_true: list[np.ndarray], y_pred: list[np.ndarray]) -> float:\n",
    "        mean_mrr = np.mean(\n",
    "            [\n",
    "                mrr_score(each_labels, each_preds)\n",
    "                for each_labels, each_preds in tqdm(\n",
    "                    zip(y_true, y_pred), ncols=80, total=len(y_true), desc=\"AUC\"\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        return float(mean_mrr)\n",
    "\n",
    "\n",
    "class NdcgScore(Metric):\n",
    "    def __init__(self, k: int):\n",
    "        self.k = k\n",
    "        self.name = f\"ndcg@{k}\"\n",
    "\n",
    "    def calculate(self, y_true: list[np.ndarray], y_pred: list[np.ndarray]) -> float:\n",
    "        res = np.mean(\n",
    "            [\n",
    "                ndcg_score(each_labels, each_preds, self.k)\n",
    "                for each_labels, each_preds in tqdm(\n",
    "                    zip(y_true, y_pred), ncols=80, total=len(y_true), desc=\"AUC\"\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        return float(res)\n",
    "\n",
    "\n",
    "class MetricEvaluator:\n",
    "    \"\"\"\n",
    "    >>> y_true = [[1, 0, 0], [1, 1, 0], [1, 0, 0, 0]]\n",
    "    >>> y_pred = [[0.2, 0.3, 0.5], [0.18, 0.7, 0.1], [0.18, 0.2, 0.1, 0.1]]\n",
    "\n",
    "    >>> met_eval = MetricEvaluator(\n",
    "            labels=y_true,\n",
    "            predictions=y_pred,\n",
    "            metric_functions=[\n",
    "                AucScore(),\n",
    "                MrrScore(),\n",
    "                NdcgScore(k=5),\n",
    "                NdcgScore(k=10),\n",
    "                LogLossScore(),\n",
    "                RootMeanSquaredError(),\n",
    "                AccuracyScore(threshold=0.5),\n",
    "                F1Score(threshold=0.5),\n",
    "            ],\n",
    "        )\n",
    "    >>> met_eval.evaluate()\n",
    "    {\n",
    "        \"auc\": 0.5555555555555556,\n",
    "        \"mrr\": 0.5277777777777778,\n",
    "        \"ndcg@5\": 0.7103099178571526,\n",
    "        \"ndcg@10\": 0.7103099178571526,\n",
    "        \"logloss\": 0.716399020295845,\n",
    "        \"rmse\": 0.5022870658128165\n",
    "        \"accuracy\": 0.5833333333333334,\n",
    "        \"f1\": 0.2222222222222222\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        labels: list[np.ndarray],\n",
    "        predictions: list[np.ndarray],\n",
    "        metric_functions: list[Metric],\n",
    "    ):\n",
    "        self.labels = labels\n",
    "        self.predictions = predictions\n",
    "        self.metric_functions = metric_functions\n",
    "        self.evaluations = dict()\n",
    "\n",
    "    def evaluate(self) -> dict:\n",
    "        self.evaluations = {\n",
    "            metric_function.name: metric_function(self.labels, self.predictions)\n",
    "            for metric_function in self.metric_functions\n",
    "        }\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def metric_functions(self):\n",
    "        return self.__metric_functions\n",
    "\n",
    "    @metric_functions.setter\n",
    "    def metric_functions(self, values):\n",
    "        invalid_callables = self.__invalid_callables(values)\n",
    "        if not any(invalid_callables) and invalid_callables:\n",
    "            self.__metric_functions = values\n",
    "        else:\n",
    "            invalid_objects = list(compress(values, invalid_callables))\n",
    "            invalid_types = [type(item) for item in invalid_objects]\n",
    "            raise TypeError(f\"Following object(s) are not callable: {invalid_types}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def __invalid_callables(iter: Iterable):\n",
    "        return [not callable(item) for item in iter]\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.evaluations:\n",
    "            evaluations_json = json.dumps(self.evaluations, indent=4)\n",
    "            return f\"<MetricEvaluator class>: \\n {evaluations_json}\"\n",
    "        else:\n",
    "            return f\"<MetricEvaluator class>: {self.evaluations}\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "\n",
    "try:\n",
    "    import polars as pl\n",
    "except ImportError:\n",
    "    print(\"polars not available\")\n",
    "\n",
    "\n",
    "\n",
    "def _check_columns_in_df(df: pl.DataFrame, columns: list[str]) -> None:\n",
    "    \"\"\"\n",
    "    Checks whether all specified columns are present in a Polars DataFrame.\n",
    "    Raises a ValueError if any of the specified columns are not present in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): The input DataFrame.\n",
    "        columns (list[str]): The names of the columns to check for.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "\n",
    "    Examples:\n",
    "    >>> df = pl.DataFrame({\"user_id\": [1], \"first_name\": [\"J\"]})\n",
    "    >>> check_columns_in_df(df, columns=[\"user_id\", \"not_in\"])\n",
    "        ValueError: Invalid input provided. The dataframe does not contain columns ['not_in'].\n",
    "    \"\"\"\n",
    "    columns_not_in_df = [col for col in columns if col not in df.columns]\n",
    "    if columns_not_in_df:\n",
    "        raise ValueError(\n",
    "            f\"Invalid input provided. The DataFrame does not contain columns {columns_not_in_df}.\"\n",
    "        )\n",
    "\n",
    "\n",
    "def _validate_equal_list_column_lengths(df: pl.DataFrame, col1: str, col2: str) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if the items in two list columns of a DataFrame have equal lengths.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): The DataFrame containing the list columns.\n",
    "        col1 (str): The name of the first list column.\n",
    "        col2 (str): The name of the second list column.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the items in the two list columns have equal lengths, False otherwise.\n",
    "\n",
    "    Raises:\n",
    "        None.\n",
    "\n",
    "    >>> df = pl.DataFrame({\n",
    "            'col1': [[1, 2, 3], [4, 5], [6]],\n",
    "            'col2': [[10, 20], [30, 40, 50], [60, 70, 80]],\n",
    "        })\n",
    "    >>> _validate_equal_list_column_lengths(df, 'col1', 'col2')\n",
    "        ValueError: Mismatch in the lengths of the number of items (row-based) between the columns: 'col1' and 'col2'. Please ensure equal lengths.\n",
    "    >>> df = df.with_columns(pl.Series('col1', [[1, 2], [3, 4, 5], [6, 7, 8]]))\n",
    "    >>> _validate_equal_list_column_lengths(df, 'col1', 'col2')\n",
    "    \"\"\"\n",
    "    if not df.select(pl.col(col1).list.len() == pl.col(col2).list.len())[col1].all():\n",
    "        raise ValueError(\n",
    "            f\"Mismatch in the lengths of the number of items (row-based) between the columns: '{col1}' and '{col2}'. Please ensure equal lengths.\"\n",
    "        )\n",
    "\n",
    "\n",
    "def slice_join_dataframes(\n",
    "    df1: pl.DataFrame,\n",
    "    df2: pl.DataFrame,\n",
    "    on: str,\n",
    "    how: str,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Join two dataframes optimized for memory efficiency.\n",
    "    \"\"\"\n",
    "    return pl.concat(\n",
    "        (\n",
    "            rows.join(\n",
    "                df2,\n",
    "                on=on,\n",
    "                how=how,\n",
    "            )\n",
    "            for rows in df1.iter_slices()\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def rename_columns(df: pl.DataFrame, map_dict: dict[str, str]) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Examples:\n",
    "        >>> import polars as pl\n",
    "        >>> df = pl.DataFrame({'A': [1, 2], 'B': [3, 4]})\n",
    "        >>> map_dict = {'A': 'X', 'B': 'Y'}\n",
    "        >>> rename_columns(df, map_dict)\n",
    "            shape: (2, 2)\n",
    "            \n",
    "             X    Y   \n",
    "             ---  --- \n",
    "             i64  i64 \n",
    "            \n",
    "             1    3   \n",
    "             2    4   \n",
    "            \n",
    "        >>> rename_columns(df, {\"Z\" : \"P\"})\n",
    "            shape: (2, 2)\n",
    "            \n",
    "             A    B   \n",
    "             ---  --- \n",
    "             i64  i64 \n",
    "            \n",
    "             1    3   \n",
    "             2    4   \n",
    "            \n",
    "    \"\"\"\n",
    "    map_dict = {key: val for key, val in map_dict.items() if key in df.columns}\n",
    "    if len(map_dict):\n",
    "        df = df.rename(map_dict)\n",
    "    return df\n",
    "\n",
    "\n",
    "def from_dict_to_polars(dictionary: dict) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    When dealing with dictionary with intergers as keys\n",
    "    Example:\n",
    "    >>> dictionary = {1: \"a\", 2: \"b\"}\n",
    "    >>> from_dict_to_polars(dictionary)\n",
    "        shape: (2, 2)\n",
    "        \n",
    "         keys  values \n",
    "         ---   ---    \n",
    "         i64   str    \n",
    "        \n",
    "         1     a      \n",
    "         2     b      \n",
    "        \n",
    "    >>> pl.from_dict(dictionary)\n",
    "        raise ValueError(\"Series name must be a string.\")\n",
    "            ValueError: Series name must be a string.\n",
    "    \"\"\"\n",
    "    return pl.DataFrame(\n",
    "        {\"keys\": list(dictionary.keys()), \"values\": list(dictionary.values())}\n",
    "    )\n",
    "\n",
    "\n",
    "def shuffle_rows(df: pl.DataFrame, seed: int = None) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Shuffle the rows of a DataFrame. This methods allows for LazyFrame,\n",
    "    whereas, 'df.sample(fraction=1)' is not compatible.\n",
    "\n",
    "    Examples:\n",
    "    >>> df = pl.DataFrame({\"a\": [1, 2, 3], \"b\": [1, 2, 3], \"c\": [1, 2, 3]})\n",
    "    >>> shuffle_rows(df.lazy(), seed=123).collect()\n",
    "        shape: (3, 3)\n",
    "        \n",
    "         a    b    c   \n",
    "         ---  ---  --- \n",
    "         i64  i64  i64 \n",
    "        \n",
    "         1    1    1   \n",
    "         3    3    3   \n",
    "         2    2    2   \n",
    "        \n",
    "    >>> shuffle_rows(df.lazy(), seed=None).collect().sort(\"a\")\n",
    "        shape: (3, 3)\n",
    "        \n",
    "         a    b    c   \n",
    "         ---  ---  --- \n",
    "         i64  i64  i64 \n",
    "        \n",
    "         1    1    1   \n",
    "         2    2    2   \n",
    "         3    3    3   \n",
    "        \n",
    "\n",
    "    Test_:\n",
    "    >>> all([sum(row) == row[0]*3 for row in shuffle_rows(df, seed=None).iter_rows()])\n",
    "        True\n",
    "\n",
    "    Note:\n",
    "        Be aware that 'pl.all().shuffle()' shuffles columns-wise, i.e., with if pl.all().shuffle(None)\n",
    "        each column's element are shuffled independently from each other (example might change with no seed):\n",
    "    >>> df_ = pl.DataFrame({\"a\": [1, 2, 3], \"b\": [1, 2, 3], \"c\": [1, 2, 3]}).select(pl.all().shuffle(None)).sort(\"a\")\n",
    "    >>> df_\n",
    "        shape: (3, 3)\n",
    "        \n",
    "         a    b    c   \n",
    "         ---  ---  --- \n",
    "         i64  i64  i64 \n",
    "        \n",
    "         1    3    1   \n",
    "         2    2    3   \n",
    "         3    1    2   \n",
    "        \n",
    "    >>> all([sum(row) == row[0]*3 for row in shuffle_rows(df_, seed=None).iter_rows()])\n",
    "        False\n",
    "    \"\"\"\n",
    "    seed = seed if seed is not None else random.randint(1, 1_000_000)\n",
    "    return df.select(pl.all().shuffle(seed))\n",
    "\n",
    "\n",
    "def keep_unique_values_in_list(df: pl.DataFrame, column: str) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Removes duplicate article IDs from the specified list column of a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): The input DataFrame containing the list column with article IDs.\n",
    "        column (str): The name of the list column containing article IDs.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: A new DataFrame with the same columns as the input DataFrame, but with duplicate\n",
    "        article IDs removed from the specified list column.\n",
    "\n",
    "    Example:\n",
    "        >>> df = pl.DataFrame({\n",
    "                \"article_ids\": [[1, 2, 3, 1, 2], [3, 4, 5, 3], [1, 2, 3, 1, 2, 3]],\n",
    "                \"hh\": [\"h\", \"e\", \"y\"]\n",
    "            })\n",
    "        >>> keep_unique_values_in_list(df.lazy(), \"article_ids\").collect()\n",
    "            shape: (3, 1)\n",
    "            \n",
    "             article_ids \n",
    "             ---         \n",
    "             list[i64]   \n",
    "            \n",
    "             [1, 2, 3]   \n",
    "             [3, 4, 5]   \n",
    "             [1, 2, 3]   \n",
    "            \n",
    "    \"\"\"\n",
    "    return df.with_columns(pl.col(column).list.unique())\n",
    "\n",
    "\n",
    "def filter_minimum_lengths_from_list(\n",
    "    df: pl.DataFrame,\n",
    "    n: int,\n",
    "    column: str,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"Filters a DataFrame based on the minimum number of elements in an array column.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): The input DataFrame to filter.\n",
    "        n (int): The minimum number of elements required in the array column.\n",
    "        column (str): The name of the array column to filter on.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: The filtered DataFrame.\n",
    "\n",
    "    Example:\n",
    "    >>> df = pl.DataFrame(\n",
    "            {\n",
    "                \"user_id\": [1, 2, 3, 4],\n",
    "                \"article_ids\": [[\"a\", \"b\", \"c\"], [\"a\", \"b\"], [\"a\"], [\"a\"]],\n",
    "            }\n",
    "        )\n",
    "    >>> filter_minimum_lengths_from_list(df, n=2, column=\"article_ids\")\n",
    "        shape: (2, 2)\n",
    "        \n",
    "         user_id  article_ids     \n",
    "         ---      ---             \n",
    "         i64      list[str]       \n",
    "        \n",
    "         1        [\"a\", \"b\", \"c\"] \n",
    "         2        [\"a\", \"b\"]      \n",
    "        \n",
    "    >>> filter_minimum_lengths_from_list(df, n=None, column=\"article_ids\")\n",
    "        shape: (4, 2)\n",
    "        \n",
    "         user_id  article_ids     \n",
    "         ---      ---             \n",
    "         i64      list[str]       \n",
    "        \n",
    "         1        [\"a\", \"b\", \"c\"] \n",
    "         2        [\"a\", \"b\"]      \n",
    "         3        [\"a\"]           \n",
    "         4        [\"a\"]           \n",
    "        \n",
    "    \"\"\"\n",
    "    return (\n",
    "        df.filter(pl.col(column).list.len() >= n)\n",
    "        if column in df and n is not None and n > 0\n",
    "        else df\n",
    "    )\n",
    "\n",
    "\n",
    "def filter_maximum_lengths_from_list(\n",
    "    df: pl.DataFrame,\n",
    "    n: int,\n",
    "    column: str,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"Filters a DataFrame based on the maximum number of elements in an array column.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): The input DataFrame to filter.\n",
    "        n (int): The maximum number of elements required in the array column.\n",
    "        column (str): The name of the array column to filter on.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: The filtered DataFrame.\n",
    "\n",
    "    Example:\n",
    "    >>> df = pl.DataFrame(\n",
    "            {\n",
    "                \"user_id\": [1, 2, 3, 4],\n",
    "                \"article_ids\": [[\"a\", \"b\", \"c\"], [\"a\", \"b\"], [\"a\"], [\"a\"]],\n",
    "            }\n",
    "        )\n",
    "    >>> filter_maximum_lengths_from_list(df, n=2, column=\"article_ids\")\n",
    "        shape: (3, 2)\n",
    "        \n",
    "         user_id  article_ids \n",
    "         ---      ---         \n",
    "         i64      list[str]   \n",
    "        \n",
    "         2        [\"a\", \"b\"]  \n",
    "         3        [\"a\"]       \n",
    "         4        [\"a\"]       \n",
    "        \n",
    "    >>> filter_maximum_lengths_from_list(df, n=None, column=\"article_ids\")\n",
    "        shape: (4, 2)\n",
    "        \n",
    "         user_id  article_ids     \n",
    "         ---      ---             \n",
    "         i64      list[str]       \n",
    "        \n",
    "         1        [\"a\", \"b\", \"c\"] \n",
    "         2        [\"a\", \"b\"]      \n",
    "         3        [\"a\"]           \n",
    "         4        [\"a\"]           \n",
    "        \n",
    "    \"\"\"\n",
    "    return (\n",
    "        df.filter(pl.col(column).list.len() <= n)\n",
    "        if column in df and n is not None and n > 0\n",
    "        else df\n",
    "    )\n",
    "\n",
    "\n",
    "def split_df_fraction(\n",
    "    df: pl.DataFrame,\n",
    "    fraction=0.8,\n",
    "    seed: int = None,\n",
    "    shuffle: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into two parts based on a specified fraction.\n",
    "    >>> df = pl.DataFrame({'A': range(10), 'B': range(10, 20)})\n",
    "    >>> df1, df2 = split_df(df, fraction=0.8, seed=42, shuffle=True)\n",
    "    >>> len(df1)\n",
    "        8\n",
    "    >>> len(df2)\n",
    "        2\n",
    "    \"\"\"\n",
    "    if not 0 < fraction < 1:\n",
    "        raise ValueError(\"fraction must be between 0 and 1\")\n",
    "    df = df.sample(fraction=1.0, shuffle=shuffle, seed=seed)\n",
    "    n_split_sample = int(len(df) * fraction)\n",
    "    return df[:n_split_sample], df[n_split_sample:]\n",
    "\n",
    "\n",
    "def split_df_chunks(df: pl.DataFrame, n_chunks: int):\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into a specified number of chunks.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): The DataFrame to be split into chunks.\n",
    "        n_chunks (int): The number of chunks to divide the DataFrame into.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of DataFrame chunks. Each element in the list is a DataFrame\n",
    "        representing a chunk of the original data.\n",
    "\n",
    "    Examples\n",
    "    >>> import polars as pl\n",
    "    >>> df = pl.DataFrame({'A': range(3)})\n",
    "    >>> chunks = split_df_chunks(df, 2)\n",
    "    >>> chunks\n",
    "        [shape: (1, 1)\n",
    "        \n",
    "         A   \n",
    "         --- \n",
    "         i64 \n",
    "        \n",
    "         0   \n",
    "        , shape: (2, 1)\n",
    "        \n",
    "         A   \n",
    "         --- \n",
    "         i64 \n",
    "        \n",
    "         1   \n",
    "         2   \n",
    "        ]\n",
    "    \"\"\"\n",
    "    # Calculate the number of rows per chunk\n",
    "    chunk_size = df.height // n_chunks\n",
    "\n",
    "    # Split the DataFrame into chunks\n",
    "    chunks = [df[i * chunk_size : (i + 1) * chunk_size] for i in range(n_chunks)]\n",
    "\n",
    "    # Append the remainder rows to the last chunk\n",
    "    if df.height % n_chunks != 0:\n",
    "        remainder_start_idx = n_chunks * chunk_size\n",
    "        chunks[-1] = pl.concat([chunks[-1], df[remainder_start_idx:]])\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def drop_nulls_from_list(df: pl.DataFrame, column: str) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Drops null values from a specified column in a Polars DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): The input DataFrame.\n",
    "        column (str): The name of the column to drop null values from.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: A new DataFrame with null values dropped from the specified column.\n",
    "\n",
    "    Examples:\n",
    "    >>> df = pl.DataFrame(\n",
    "            {\"user_id\": [101, 102, 103], \"dynamic_article_id\": [[1, None, 3], None, [4, 5]]}\n",
    "        )\n",
    "    >>> print(df)\n",
    "        shape: (3, 2)\n",
    "        \n",
    "         user_id  dynamic_article_id \n",
    "         ---      ---                \n",
    "         i64      list[i64]          \n",
    "        \n",
    "         101      [1, null, 3]       \n",
    "         102      null               \n",
    "         103      [4, 5]             \n",
    "        \n",
    "    >>> drop_nulls_from_list(df, \"dynamic_article_id\")\n",
    "        shape: (3, 2)\n",
    "        \n",
    "         user_id  dynamic_article_id \n",
    "         ---      ---                \n",
    "         i64      list[i64]          \n",
    "        \n",
    "         101      [1, 3]             \n",
    "         102      null               \n",
    "         103      [4, 5]             \n",
    "        \n",
    "    \"\"\"\n",
    "    return df.with_columns(pl.col(column).list.eval(pl.element().drop_nulls()))\n",
    "\n",
    "\n",
    "def filter_list_elements(df: pl.DataFrame, column: str, ids: list[any]) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Removes list elements from a specified column in a Polars DataFrame that are not found in a given list of identifiers.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): The Polars DataFrame to process.\n",
    "        column (str): The name of the column from which to remove unknown elements.\n",
    "        ids (list[any]): A list of identifiers to retain in the specified column. Elements not in this list will be removed.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: A new Polars DataFrame with the same structure as the input DataFrame, but with elements not found in\n",
    "                    the 'ids' list removed from the specified 'column'.\n",
    "\n",
    "    Examples:\n",
    "    >>> df = pl.DataFrame({\"A\": [1, 2, 3, 4, 5], \"B\": [[1, 3], [3, 4], None, [7, 8], [9, 10]]})\n",
    "    >>> ids = [1, 3, 5, 7]\n",
    "    >>> filter_list_elements(df.lazy(), \"B\", ids).collect()\n",
    "        shape: (5, 2)\n",
    "        \n",
    "         A    B         \n",
    "         ---  ---       \n",
    "         i64  list[i64] \n",
    "        \n",
    "         1    [1, 3]    \n",
    "         2    [3]       \n",
    "         3    null      \n",
    "         4    [7]       \n",
    "         5    null      \n",
    "        \n",
    "    \"\"\"\n",
    "    GROUPBY_COL = \"_groupby\"\n",
    "    COLUMNS = df.columns\n",
    "    df = df.with_row_index(GROUPBY_COL)\n",
    "    df_ = (\n",
    "        df.select(pl.col(GROUPBY_COL, column))\n",
    "        .drop_nulls()\n",
    "        .explode(column)\n",
    "        .filter(pl.col(column).is_in(ids))\n",
    "        .group_by(GROUPBY_COL)\n",
    "        .agg(column)\n",
    "    )\n",
    "    return df.drop(column).join(df_, on=GROUPBY_COL, how=\"left\").select(COLUMNS)\n",
    "\n",
    "\n",
    "def filter_elements(df: pl.DataFrame, column: str, ids: list[any]) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Removes elements from a specified column in a Polars DataFrame that are not found in a given list of identifiers.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): The Polars DataFrame to process.\n",
    "        column (str): The name of the column from which to remove unknown elements.\n",
    "        ids (list[any]): A list of identifiers to retain in the specified column. Elements not in this list will be removed.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: A new Polars DataFrame with the same structure as the input DataFrame, but with elements not found in\n",
    "                    the 'ids' list removed from the specified 'column'.\n",
    "\n",
    "    Examples:\n",
    "    >>> df = pl.DataFrame({\"A\": [1, 2, 3, 4, 5], \"B\": [[1, 3], [3, 4], None, [7, 8], [9, 10]]})\n",
    "        shape: (5, 2)\n",
    "        \n",
    "         A    B         \n",
    "         ---  ---       \n",
    "         i64  list[i64] \n",
    "        \n",
    "         1    [1, 3]    \n",
    "         2    [3, 4]    \n",
    "         3    null      \n",
    "         4    [7, 8]    \n",
    "         5    [9, 10]   \n",
    "        \n",
    "    >>> ids = [1, 3, 5, 7]\n",
    "    >>> filter_elements(df.lazy(), \"A\", ids).collect()\n",
    "        shape: (5, 2)\n",
    "        \n",
    "         A     B         \n",
    "         ---   ---       \n",
    "         i64   list[i64] \n",
    "        \n",
    "         1     [1, 3]    \n",
    "         null  [3, 4]    \n",
    "         3     null      \n",
    "         null  [7, 8]    \n",
    "         5     [9, 10]   \n",
    "        \n",
    "    \"\"\"\n",
    "    GROUPBY_COL = \"_groupby\"\n",
    "    COLUMNS = df.columns\n",
    "    df = df.with_row_index(GROUPBY_COL)\n",
    "    df_ = (\n",
    "        df.select(pl.col(GROUPBY_COL, column))\n",
    "        .drop_nulls()\n",
    "        .filter(pl.col(column).is_in(ids))\n",
    "    )\n",
    "    return df.drop(column).join(df_, on=GROUPBY_COL, how=\"left\").select(COLUMNS)\n",
    "\n",
    "\n",
    "def concat_str_columns(df: pl.DataFrame, columns: list[str]) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    >>> df = pl.DataFrame(\n",
    "            {\n",
    "                \"id\": [1, 2, 3],\n",
    "                \"first_name\": [\"John\", \"Jane\", \"Alice\"],\n",
    "                \"last_name\": [\"Doe\", \"Doe\", \"Smith\"],\n",
    "            }\n",
    "        )\n",
    "    >>> concatenated_df, concatenated_column_name = concat_str_columns(df, columns=['first_name', 'last_name'])\n",
    "    >>> concatenated_df\n",
    "        shape: (3, 4)\n",
    "        \n",
    "         id   first_name  last_name  first_name-last_name \n",
    "         ---  ---         ---        ---                  \n",
    "         i64  str         str        str                  \n",
    "        \n",
    "         1    John        Doe        John Doe             \n",
    "         2    Jane        Doe        Jane Doe             \n",
    "         3    Alice       Smith      Alice Smith          \n",
    "        \n",
    "    \"\"\"\n",
    "    concat_name = \"-\".join(columns)\n",
    "    concat_columns = df.select(pl.concat_str(columns, separator=\" \").alias(concat_name))\n",
    "    return df.with_columns(concat_columns), concat_name\n",
    "\n",
    "\n",
    "def filter_empty_text_column(df: pl.DataFrame, column: str) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Example:\n",
    "    >>> df = pl.DataFrame({\"Name\": [\"John\", \"Alice\", \"Bob\", \"\"], \"Age\": [25, 28, 30, 22]})\n",
    "    >>> filter_empty_text_column(df, \"Name\")\n",
    "        shape: (3, 2)\n",
    "        \n",
    "         Name   Age \n",
    "         ---    --- \n",
    "         str    i64 \n",
    "        \n",
    "         John   25  \n",
    "         Alice  28  \n",
    "         Bob    30  \n",
    "        \n",
    "    \"\"\"\n",
    "    return df.filter(pl.col(column).str.lengths() > 0)\n",
    "\n",
    "\n",
    "def shuffle_list_column(\n",
    "    df: pl.DataFrame, column: str, seed: int = None\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"Shuffles the values in a list column of a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): The input DataFrame.\n",
    "        column (str): The name of the column to shuffle.\n",
    "        seed (int, optional): An optional seed value.\n",
    "            Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: A new DataFrame with the specified column shuffled.\n",
    "\n",
    "    Example:\n",
    "    >>> df = pl.DataFrame(\n",
    "            {\n",
    "                \"id\": [1, 2, 3],\n",
    "                \"list_col\": [[\"a-\", \"b-\", \"c-\"], [\"a#\", \"b#\"], [\"a@\", \"b@\", \"c@\"]],\n",
    "                \"rdn\": [\"h\", \"e\", \"y\"],\n",
    "            }\n",
    "        )\n",
    "    >>> shuffle_list_column(df, 'list_col', seed=1)\n",
    "        shape: (3, 3)\n",
    "        \n",
    "         id   list_col            rdn \n",
    "         ---  ---                 --- \n",
    "         i64  list[str]           str \n",
    "        \n",
    "         1    [\"c-\", \"b-\", \"a-\"]  h   \n",
    "         2    [\"a#\", \"b#\"]        e   \n",
    "         3    [\"b@\", \"c@\", \"a@\"]  y   \n",
    "        \n",
    "\n",
    "    No seed:\n",
    "    >>> shuffle_list_column(df, 'list_col', seed=None)\n",
    "        shape: (3, 3)\n",
    "        \n",
    "         id   list_col            rdn \n",
    "         ---  ---                 --- \n",
    "         i64  list[str]           str \n",
    "        \n",
    "         1    [\"b-\", \"a-\", \"c-\"]  h   \n",
    "         2    [\"a#\", \"b#\"]        e   \n",
    "         3    [\"a@\", \"c@\", \"b@\"]  y   \n",
    "        \n",
    "\n",
    "    Test_:\n",
    "    >>> assert (\n",
    "            sorted(shuffle_list_column(df, \"list_col\", seed=None)[\"list_col\"].to_list()[0])\n",
    "            == df[\"list_col\"].to_list()[0]\n",
    "        )\n",
    "\n",
    "    >>> df = pl.DataFrame({\n",
    "            'id': [1, 2, 3],\n",
    "            'list_col': [[6, 7, 8], [-6, -7, -8], [60, 70, 80]],\n",
    "            'rdn': ['h', 'e', 'y']\n",
    "        })\n",
    "    >>> shuffle_list_column(df.lazy(), 'list_col', seed=2).collect()\n",
    "        shape: (3, 3)\n",
    "        \n",
    "         id   list_col      rdn \n",
    "         ---  ---           --- \n",
    "         i64  list[i64]     str \n",
    "        \n",
    "         1    [7, 6, 8]     h   \n",
    "         2    [-8, -7, -6]  e   \n",
    "         3    [60, 80, 70]  y   \n",
    "        \n",
    "\n",
    "    Test_:\n",
    "    >>> assert (\n",
    "            sorted(shuffle_list_column(df, \"list_col\", seed=None)[\"list_col\"].to_list()[0])\n",
    "            == df[\"list_col\"].to_list()[0]\n",
    "        )\n",
    "    \"\"\"\n",
    "    _COLUMN_ORDER = df.columns\n",
    "    GROUPBY_ID = generate_unique_name(_COLUMN_ORDER, \"_groupby_id\")\n",
    "\n",
    "    df = df.with_row_count(GROUPBY_ID)\n",
    "    df_shuffle = (\n",
    "        df.explode(column)\n",
    "        .pipe(shuffle_rows, seed=seed)\n",
    "        .group_by(GROUPBY_ID)\n",
    "        .agg(column)\n",
    "    )\n",
    "    return (\n",
    "        df.drop(column)\n",
    "        .join(df_shuffle, on=GROUPBY_ID, how=\"left\")\n",
    "        .drop(GROUPBY_ID)\n",
    "        .select(_COLUMN_ORDER)\n",
    "    )\n",
    "\n",
    "\n",
    "def split_df_in_n(df: pl.DataFrame, num_splits: int) -> list[pl.DataFrame]:\n",
    "    \"\"\"\n",
    "    Split a DataFrame into n equal-sized splits.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame to be split.\n",
    "        num_splits (int): The number of splits to create.\n",
    "\n",
    "    Returns:\n",
    "        List[pandas.DataFrame]: A list of DataFrames, each representing a split.\n",
    "\n",
    "    Examples:\n",
    "        >>> df = pl.DataFrame({'A': [1, 2, 3, 4, 5, 6, 7], \"B\" : [1, 2, 3, 4, 5, 6, 7]})\n",
    "        >>> splits = split_df_in_n(df, 3)\n",
    "        >>> for d in splits:\n",
    "                print(d)\n",
    "                shape: (3, 2)\n",
    "                \n",
    "                 A    B   \n",
    "                 ---  --- \n",
    "                 i64  i64 \n",
    "                \n",
    "                 1    1   \n",
    "                 2    2   \n",
    "                 3    3   \n",
    "                \n",
    "                shape: (3, 2)\n",
    "                \n",
    "                 A    B   \n",
    "                 ---  --- \n",
    "                 i64  i64 \n",
    "                \n",
    "                 4    4   \n",
    "                 5    5   \n",
    "                 6    6   \n",
    "                \n",
    "                shape: (1, 2)\n",
    "                \n",
    "                 A    B   \n",
    "                 ---  --- \n",
    "                 i64  i64 \n",
    "                \n",
    "                 7    7   \n",
    "                \n",
    "\n",
    "    \"\"\"\n",
    "    rows_per_split = int(np.ceil(df.shape[0] / num_splits))\n",
    "    return [\n",
    "        df[i * rows_per_split : (1 + i) * rows_per_split] for i in range(num_splits)\n",
    "    ]\n",
    "\n",
    "\n",
    "def concat_list_str(df: pl.DataFrame, column: str) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Concatenate strings within lists for a specified column in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (polars.DataFrame): The input DataFrame.\n",
    "        column (str): The name of the column in `df` that contains lists of strings\n",
    "                        to be concatenated.\n",
    "\n",
    "    Returns:\n",
    "        polars.DataFrame: A DataFrame with the same structure as `df` but with the\n",
    "                            specified column's lists of strings concatenated and\n",
    "                            converted to a string instead of list.\n",
    "\n",
    "    Examples:\n",
    "        >>> df = pl.DataFrame({\n",
    "                \"strings\": [[\"ab\", \"cd\"], [\"ef\", \"gh\"], [\"ij\", \"kl\"]]\n",
    "            })\n",
    "        >>> concat_list_str(df, \"strings\")\n",
    "            shape: (3, 1)\n",
    "            \n",
    "             strings \n",
    "             ---     \n",
    "             str     \n",
    "            \n",
    "             ab cd   \n",
    "             ef gh   \n",
    "             ij kl   \n",
    "            \n",
    "    \"\"\"\n",
    "    return df.with_columns(\n",
    "        pl.col(column).list.eval(pl.element().str.concat(\" \"))\n",
    "    ).explode(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Iterable\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import datetime\n",
    "import inspect\n",
    "\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "\n",
    "def create_binary_labels_column(\n",
    "    df: pl.DataFrame,\n",
    "    shuffle: bool = False,\n",
    "    seed: int = None,\n",
    "    clicked_col: str = DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    inview_col: str = DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    label_col: str = DEFAULT_LABELS_COL,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"Creates a new column in a DataFrame containing binary labels indicating\n",
    "    whether each article ID in the \"article_ids\" column is present in the corresponding\n",
    "    \"list_destination\" column.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: A new DataFrame with an additional \"labels\" column.\n",
    "\n",
    "    Examples:\n",
    "    >>> from ebrec.utils._constants import (\n",
    "            DEFAULT_CLICKED_ARTICLES_COL,\n",
    "            DEFAULT_INVIEW_ARTICLES_COL,\n",
    "            DEFAULT_LABELS_COL,\n",
    "        )\n",
    "    >>> df = pl.DataFrame(\n",
    "            {\n",
    "                DEFAULT_INVIEW_ARTICLES_COL: [[1, 2, 3], [4, 5, 6], [7, 8]],\n",
    "                DEFAULT_CLICKED_ARTICLES_COL: [[2, 3, 4], [3, 5], None],\n",
    "            }\n",
    "        )\n",
    "    >>> create_binary_labels_column(df)\n",
    "        shape: (3, 3)\n",
    "        \n",
    "         article_ids_inview  article_ids_clicked  labels    \n",
    "         ---                 ---                  ---       \n",
    "         list[i64]           list[i64]            list[i8]  \n",
    "        \n",
    "         [1, 2, 3]           [2, 3, 4]            [0, 1, 1] \n",
    "         [4, 5, 6]           [3, 5]               [0, 1, 0] \n",
    "         [7, 8]              null                 [0, 0]    \n",
    "        \n",
    "    >>> create_binary_labels_column(df.lazy(), shuffle=True, seed=123).collect()\n",
    "        shape: (3, 3)\n",
    "        \n",
    "         article_ids_inview  article_ids_clicked  labels    \n",
    "         ---                 ---                  ---       \n",
    "         list[i64]           list[i64]            list[i8]  \n",
    "        \n",
    "         [3, 1, 2]           [2, 3, 4]            [1, 0, 1] \n",
    "         [5, 6, 4]           [3, 5]               [1, 0, 0] \n",
    "         [7, 8]              null                 [0, 0]    \n",
    "        \n",
    "    Test_:\n",
    "    >>> assert create_binary_labels_column(df, shuffle=False)[DEFAULT_LABELS_COL].to_list() == [\n",
    "            [0, 1, 1],\n",
    "            [0, 1, 0],\n",
    "            [0, 0],\n",
    "        ]\n",
    "    >>> assert create_binary_labels_column(df, shuffle=True)[DEFAULT_LABELS_COL].list.sum().to_list() == [\n",
    "            2,\n",
    "            1,\n",
    "            0,\n",
    "        ]\n",
    "    \"\"\"\n",
    "    _check_columns_in_df(df, [inview_col, clicked_col])\n",
    "    _COLUMNS = df.columns\n",
    "    GROUPBY_ID = generate_unique_name(_COLUMNS, \"_groupby_id\")\n",
    "\n",
    "    df = df.with_row_index(GROUPBY_ID)\n",
    "\n",
    "    if shuffle:\n",
    "        df = shuffle_list_column(df, column=inview_col, seed=seed)\n",
    "\n",
    "    df_labels = (\n",
    "        df.explode(inview_col)\n",
    "        .with_columns(\n",
    "            pl.col(inview_col).is_in(pl.col(clicked_col)).cast(pl.Int8).alias(label_col)\n",
    "        )\n",
    "        .group_by(GROUPBY_ID)\n",
    "        .agg(label_col)\n",
    "    )\n",
    "    return (\n",
    "        df.join(df_labels, on=GROUPBY_ID, how=\"left\")\n",
    "        .drop(GROUPBY_ID)\n",
    "        .select(_COLUMNS + [label_col])\n",
    "    )\n",
    "\n",
    "\n",
    "def create_user_id_to_int_mapping(\n",
    "    df: pl.DataFrame, user_col: str = DEFAULT_USER_COL, value_str: str = \"id\"\n",
    "):\n",
    "    return create_lookup_dict(\n",
    "        df.select(pl.col(user_col).unique()).with_row_index(value_str),\n",
    "        key=user_col,\n",
    "        value=value_str,\n",
    "    )\n",
    "\n",
    "\n",
    "def filter_minimum_negative_samples(\n",
    "    df,\n",
    "    n: int,\n",
    "    inview_col: str = DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    clicked_col: str = DEFAULT_CLICKED_ARTICLES_COL,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    >>> from ebrec.utils._constants import DEFAULT_CLICKED_ARTICLES_COL, DEFAULT_INVIEW_ARTICLES_COL\n",
    "    >>> df = pl.DataFrame(\n",
    "            {\n",
    "                DEFAULT_INVIEW_ARTICLES_COL: [[1, 2, 3], [1], [1, 2, 3]],\n",
    "                DEFAULT_CLICKED_ARTICLES_COL: [[1], [1], [1, 2]],\n",
    "            }\n",
    "        )\n",
    "    >>> filter_minimum_negative_samples(df, n=1)\n",
    "        shape: (2, 2)\n",
    "        \n",
    "         article_ids_inview  article_ids_clicked \n",
    "         ---                 ---                 \n",
    "         list[i64]           list[i64]           \n",
    "        \n",
    "         [1, 2, 3]           [1]                 \n",
    "         [1, 2, 3]           [1, 2]              \n",
    "        \n",
    "    >>> filter_minimum_negative_samples(df, n=2)\n",
    "        shape: (3, 2)\n",
    "        \n",
    "         article_ids  list_destination \n",
    "         ---          ---              \n",
    "         list[i64]    list[i64]        \n",
    "        \n",
    "         [1, 2, 3]    [1]              \n",
    "        \n",
    "    \"\"\"\n",
    "    return (\n",
    "        df.filter((pl.col(inview_col).list.len() - pl.col(clicked_col).list.len()) >= n)\n",
    "        if n is not None and n > 0\n",
    "        else df\n",
    "    )\n",
    "\n",
    "\n",
    "def ebnerd_from_path(\n",
    "    path: Path,\n",
    "    history_size: int = 30,\n",
    "    padding: int = 0,\n",
    "    user_col: str = DEFAULT_USER_COL,\n",
    "    history_aids_col: str = DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Load ebnerd - function\n",
    "    \"\"\"\n",
    "    df_history = (\n",
    "        pl.scan_parquet(r\"C:\\Users\\bilba\\Downloads\\DL_Small\\history.parquet\")\n",
    "        .select(user_col, history_aids_col)\n",
    "        .pipe(\n",
    "            truncate_history,\n",
    "            column=history_aids_col,\n",
    "            history_size=history_size,\n",
    "            padding_value=padding,\n",
    "            enable_warning=False,\n",
    "        )\n",
    "    )\n",
    "    df_behaviors = (\n",
    "        pl.scan_parquet(r\"C:\\Users\\bilba\\Downloads\\DL_Small\\behaviors.parquet\")\n",
    "        .collect()\n",
    "        .pipe(\n",
    "            slice_join_dataframes,\n",
    "            df2=df_history.collect(),\n",
    "            on=user_col,\n",
    "            how=\"left\",\n",
    "        )\n",
    "    )\n",
    "    return df_behaviors\n",
    "\n",
    "\n",
    "def filter_read_times(df, n: int, column: str) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Use this to set the cutoff for 'read_time' and 'next_read_time'\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df.filter(pl.col(column) >= n)\n",
    "        if column in df and n is not None and n > 0\n",
    "        else df\n",
    "    )\n",
    "\n",
    "\n",
    "def unique_article_ids_in_behaviors(\n",
    "    df: pl.DataFrame,\n",
    "    col: str = \"ids\",\n",
    "    item_col: str = DEFAULT_ARTICLE_ID_COL,\n",
    "    inview_col: str = DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    clicked_col: str = DEFAULT_CLICKED_ARTICLES_COL,\n",
    ") -> pl.Series:\n",
    "    \"\"\"\n",
    "    Examples:\n",
    "        >>> df = pl.DataFrame({\n",
    "                DEFAULT_ARTICLE_ID_COL: [1, 2, 3, 4],\n",
    "                DEFAULT_INVIEW_ARTICLES_COL: [[2, 3], [1, 4], [4], [1, 2, 3]],\n",
    "                DEFAULT_CLICKED_ARTICLES_COL: [[], [2], [3, 4], [1]],\n",
    "            })\n",
    "        >>> unique_article_ids_in_behaviors(df).sort()\n",
    "            [\n",
    "                1\n",
    "                2\n",
    "                3\n",
    "                4\n",
    "            ]\n",
    "    \"\"\"\n",
    "    df = df.lazy()\n",
    "    return (\n",
    "        pl.concat(\n",
    "            (\n",
    "                df.select(pl.col(item_col).unique().alias(col)),\n",
    "                df.select(pl.col(inview_col).explode().unique().alias(col)),\n",
    "                df.select(pl.col(clicked_col).explode().unique().alias(col)),\n",
    "            )\n",
    "        )\n",
    "        .drop_nulls()\n",
    "        .unique()\n",
    "        .collect()\n",
    "    ).to_series()\n",
    "\n",
    "\n",
    "def add_known_user_column(\n",
    "    df: pl.DataFrame,\n",
    "    known_users: Iterable[int],\n",
    "    user_col: str = DEFAULT_USER_COL,\n",
    "    known_user_col: str = DEFAULT_KNOWN_USER_COL,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds a new column to the DataFrame indicating whether the user ID is in the list of known users.\n",
    "    Args:\n",
    "        df: A Polars DataFrame object.\n",
    "        known_users: An iterable of integers representing the known user IDs.\n",
    "    Returns:\n",
    "        A new Polars DataFrame with an additional column 'is_known_user' containing a boolean value\n",
    "        indicating whether the user ID is in the list of known users.\n",
    "    Examples:\n",
    "        >>> df = pl.DataFrame({'user_id': [1, 2, 3, 4]})\n",
    "        >>> add_known_user_column(df, [2, 4])\n",
    "            shape: (4, 2)\n",
    "            \n",
    "             user_id  is_known_user \n",
    "             ---      ---           \n",
    "             i64      bool          \n",
    "            \n",
    "             1        false         \n",
    "             2        true          \n",
    "             3        false         \n",
    "             4        true          \n",
    "            \n",
    "    \"\"\"\n",
    "    return df.with_columns(pl.col(user_col).is_in(known_users).alias(known_user_col))\n",
    "\n",
    "\n",
    "def sample_article_ids(\n",
    "    df: pl.DataFrame,\n",
    "    n: int,\n",
    "    with_replacement: bool = False,\n",
    "    seed: int = None,\n",
    "    inview_col: str = DEFAULT_INVIEW_ARTICLES_COL,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Randomly sample article IDs from each row of a DataFrame with or without replacement\n",
    "\n",
    "    Args:\n",
    "        df: A polars DataFrame containing the column of article IDs to be sampled.\n",
    "        n: The number of article IDs to sample from each list.\n",
    "        with_replacement: A boolean indicating whether to sample with replacement.\n",
    "            Default is False.\n",
    "        seed: An optional seed to use for the random number generator.\n",
    "\n",
    "    Returns:\n",
    "        A new polars DataFrame with the same columns as `df`, but with the article\n",
    "        IDs in the specified column replaced by a list of `n` sampled article IDs.\n",
    "\n",
    "    Examples:\n",
    "    >>> from ebrec.utils._constants import DEFAULT_INVIEW_ARTICLES_COL\n",
    "    >>> df = pl.DataFrame(\n",
    "            {\n",
    "                \"clicked\": [\n",
    "                    [1],\n",
    "                    [4, 5],\n",
    "                    [7, 8, 9],\n",
    "                ],\n",
    "                DEFAULT_INVIEW_ARTICLES_COL: [\n",
    "                    [\"A\", \"B\", \"C\"],\n",
    "                    [\"D\", \"E\", \"F\"],\n",
    "                    [\"G\", \"H\", \"I\"],\n",
    "                ],\n",
    "                \"col\" : [\n",
    "                    [\"h\"],\n",
    "                    [\"e\"],\n",
    "                    [\"y\"]\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "    >>> print(df)\n",
    "        shape: (3, 3)\n",
    "        \n",
    "         list_destination  article_ids      col       \n",
    "         ---               ---              ---       \n",
    "         list[i64]         list[str]        list[str] \n",
    "        \n",
    "         [1]               [\"A\", \"B\", \"C\"]  [\"h\"]     \n",
    "         [4, 5]            [\"D\", \"E\", \"F\"]  [\"e\"]     \n",
    "         [7, 8, 9]         [\"G\", \"H\", \"I\"]  [\"y\"]     \n",
    "        \n",
    "    >>> sample_article_ids(df, n=2, seed=42)\n",
    "        shape: (3, 3)\n",
    "        \n",
    "         list_destination  article_ids  col       \n",
    "         ---               ---          ---       \n",
    "         list[i64]         list[str]    list[str] \n",
    "        \n",
    "         [1]               [\"A\", \"C\"]   [\"h\"]     \n",
    "         [4, 5]            [\"D\", \"F\"]   [\"e\"]     \n",
    "         [7, 8, 9]         [\"G\", \"I\"]   [\"y\"]     \n",
    "        \n",
    "    >>> sample_article_ids(df.lazy(), n=4, with_replacement=True, seed=42).collect()\n",
    "        shape: (3, 3)\n",
    "        \n",
    "         list_destination  article_ids        col       \n",
    "         ---               ---                ---       \n",
    "         list[i64]         list[str]          list[str] \n",
    "        \n",
    "         [1]               [\"A\", \"A\",  \"C\"]  [\"h\"]     \n",
    "         [4, 5]            [\"D\", \"D\",  \"F\"]  [\"e\"]     \n",
    "         [7, 8, 9]         [\"G\", \"G\",  \"I\"]  [\"y\"]     \n",
    "        \n",
    "    \"\"\"\n",
    "    _check_columns_in_df(df, [inview_col])\n",
    "    _COLUMNS = df.columns\n",
    "    GROUPBY_ID = generate_unique_name(_COLUMNS, \"_groupby_id\")\n",
    "    df = df.with_row_count(name=GROUPBY_ID)\n",
    "\n",
    "    df_ = (\n",
    "        df.explode(inview_col)\n",
    "        .group_by(GROUPBY_ID)\n",
    "        .agg(\n",
    "            pl.col(inview_col).sample(n=n, with_replacement=with_replacement, seed=seed)\n",
    "        )\n",
    "    )\n",
    "    return (\n",
    "        df.drop(inview_col)\n",
    "        .join(df_, on=GROUPBY_ID, how=\"left\")\n",
    "        .drop(GROUPBY_ID)\n",
    "        .select(_COLUMNS)\n",
    "    )\n",
    "\n",
    "\n",
    "def remove_positives_from_inview(\n",
    "    df: pl.DataFrame,\n",
    "    inview_col: str = DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    clicked_col: str = DEFAULT_CLICKED_ARTICLES_COL,\n",
    "):\n",
    "    \"\"\"Removes all positive article IDs from a DataFrame column containing inview articles and another column containing\n",
    "    clicked articles. Only negative article IDs (i.e., those that appear in the inview articles column but not in the\n",
    "    clicked articles column) are retained.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): A DataFrame with columns containing inview articles and clicked articles.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: A new DataFrame with only negative article IDs retained.\n",
    "\n",
    "    Examples:\n",
    "    >>> from ebrec.utils._constants import DEFAULT_INVIEW_ARTICLES_COL, DEFAULT_CLICKED_ARTICLES_COL\n",
    "    >>> df = pl.DataFrame(\n",
    "            {\n",
    "                \"user_id\": [1, 1, 2],\n",
    "                DEFAULT_CLICKED_ARTICLES_COL: [\n",
    "                    [1, 2],\n",
    "                    [1],\n",
    "                    [3],\n",
    "                ],\n",
    "                DEFAULT_INVIEW_ARTICLES_COL: [\n",
    "                    [1, 2, 3],\n",
    "                    [1, 2, 3],\n",
    "                    [1, 2, 3],\n",
    "                ],\n",
    "            }\n",
    "        )\n",
    "    >>> remove_positives_from_inview(df)\n",
    "        shape: (3, 3)\n",
    "        \n",
    "         user_id  article_ids_clicked  article_ids_inview \n",
    "         ---      ---                  ---                \n",
    "         i64      list[i64]            list[i64]          \n",
    "        \n",
    "         1        [1, 2]               [3]                \n",
    "         1        [1]                  [2, 3]             \n",
    "         2        [3]                  [1, 2]             \n",
    "        \n",
    "    \"\"\"\n",
    "    _check_columns_in_df(df, [inview_col, clicked_col])\n",
    "    negative_article_ids = (\n",
    "        list(filter(lambda x: x not in clicked, inview))\n",
    "        for inview, clicked in zip(df[inview_col].to_list(), df[clicked_col].to_list())\n",
    "    )\n",
    "    return df.with_columns(pl.Series(inview_col, list(negative_article_ids)))\n",
    "\n",
    "\n",
    "def sampling_strategy_wu2019(\n",
    "    df: pl.DataFrame,\n",
    "    npratio: int,\n",
    "    shuffle: bool = False,\n",
    "    with_replacement: bool = True,\n",
    "    seed: int = None,\n",
    "    inview_col: str = DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    clicked_col: str = DEFAULT_CLICKED_ARTICLES_COL,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Samples negative articles from the inview article pool for a given negative-position-ratio (npratio).\n",
    "    The npratio (negative article per positive article) is defined as the number of negative article samples\n",
    "    to draw for each positive article sample.\n",
    "\n",
    "    This function follows the sampling strategy introduced in the paper \"NPA: Neural News Recommendation with\n",
    "    Personalized Attention\" by Wu et al. (KDD '19).\n",
    "\n",
    "    This is done according to the following steps:\n",
    "    1. Remove the positive click-article id pairs from the DataFrame.\n",
    "    2. Explode the DataFrame based on the clicked articles column.\n",
    "    3. Downsample the inview negative article ids for each exploded row using the specified npratio, either\n",
    "        with or without replacement.\n",
    "    4. Concatenate the clicked articles back to the inview articles as lists.\n",
    "    5. Convert clicked articles column to type List(Int)\n",
    "\n",
    "    References:\n",
    "        Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang, and Xing Xie. 2019.\n",
    "        Npa: Neural news recommendation with personalized attention. In KDD, pages 2576-2584. ACM.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): The input DataFrame containing click-article id pairs.\n",
    "        npratio (int): The ratio of negative in-view article ids to positive click-article ids.\n",
    "        shuffle (bool, optional): Whether to shuffle the order of the in-view article ids in each list. Default is True.\n",
    "        with_replacement (bool, optional): Whether to sample the inview article ids with or without replacement.\n",
    "            Default is True.\n",
    "        seed (int, optional): Random seed for reproducibility. Default is None.\n",
    "        inview_col (int, optional): inview column name. Default is DEFAULT_INVIEW_ARTICLES_COL,\n",
    "        clicked_col (int, optional): clicked column name. Default is DEFAULT_CLICKED_ARTICLES_COL,\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: A new DataFrame with downsampled in-view article ids for each click according to the specified npratio.\n",
    "        The DataFrame has the same columns as the input DataFrame.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If npratio is less than 0.\n",
    "        ValueError: If the input DataFrame does not contain the necessary columns.\n",
    "\n",
    "    Examples:\n",
    "    >>> from ebrec.utils._constants import DEFAULT_CLICKED_ARTICLES_COL, DEFAULT_INVIEW_ARTICLES_COL\n",
    "    >>> import polars as pl\n",
    "    >>> df = pl.DataFrame(\n",
    "            {\n",
    "                \"impression_id\": [0, 1, 2, 3],\n",
    "                \"user_id\": [1, 1, 2, 3],\n",
    "                DEFAULT_INVIEW_ARTICLES_COL: [[1, 2, 3], [1, 2, 3, 4], [1, 2, 3], [1]],\n",
    "                DEFAULT_CLICKED_ARTICLES_COL: [[1, 2], [1, 3], [1], [1]],\n",
    "            }\n",
    "        )\n",
    "    >>> df\n",
    "        shape: (4, 4)\n",
    "        \n",
    "         impression_id  user_id  article_ids_inview  article_ids_clicked \n",
    "         ---            ---      ---                 ---                 \n",
    "         i64            i64      list[i64]           list[i64]           \n",
    "        \n",
    "         0              1        [1, 2, 3]           [1, 2]              \n",
    "         1              1        [1, 2,  4]         [1, 3]              \n",
    "         2              2        [1, 2, 3]           [1]                 \n",
    "         3              3        [1]                 [1]                 \n",
    "        \n",
    "    >>> sampling_strategy_wu2019(df, npratio=1, shuffle=False, with_replacement=True, seed=123)\n",
    "        shape: (6, 4)\n",
    "        \n",
    "         impression_id  user_id  article_ids_inview  article_ids_clicked \n",
    "         ---            ---      ---                 ---                 \n",
    "         i64            i64      list[i64]           list[i64]           \n",
    "        \n",
    "         0              1        [3, 1]              [1]                 \n",
    "         0              1        [3, 2]              [2]                 \n",
    "         1              1        [4, 1]              [1]                 \n",
    "         1              1        [4, 3]              [3]                 \n",
    "         2              2        [3, 1]              [1]                 \n",
    "         3              3        [null, 1]           [1]                 \n",
    "        \n",
    "    >>> sampling_strategy_wu2019(df, npratio=1, shuffle=True, with_replacement=True, seed=123)\n",
    "        shape: (6, 4)\n",
    "        \n",
    "         impression_id  user_id  article_ids_inview  article_ids_clicked \n",
    "         ---            ---      ---                 ---                 \n",
    "         i64            i64      list[i64]           list[i64]           \n",
    "        \n",
    "         0              1        [3, 1]              [1]                 \n",
    "         0              1        [2, 3]              [2]                 \n",
    "         1              1        [4, 1]              [1]                 \n",
    "         1              1        [4, 3]              [3]                 \n",
    "         2              2        [3, 1]              [1]                 \n",
    "         3              3        [null, 1]           [1]                 \n",
    "        \n",
    "    >>> sampling_strategy_wu2019(df, npratio=2, shuffle=False, with_replacement=True, seed=123)\n",
    "        shape: (6, 4)\n",
    "        \n",
    "         impression_id  user_id  article_ids_inview  article_ids_clicked \n",
    "         ---            ---      ---                 ---                 \n",
    "         i64            i64      list[i64]           list[i64]           \n",
    "        \n",
    "         0              1        [3, 3, 1]           [1]                 \n",
    "         0              1        [3, 3, 2]           [2]                 \n",
    "         1              1        [4, 2, 1]           [1]                 \n",
    "         1              1        [4, 2, 3]           [3]                 \n",
    "         2              2        [3, 2, 1]           [1]                 \n",
    "         3              3        [null, null, 1]     [1]                 \n",
    "        \n",
    "    # If we use without replacement, we need to ensure there are enough negative samples:\n",
    "    >>> sampling_strategy_wu2019(df, npratio=2, shuffle=False, with_replacement=False, seed=123)\n",
    "        polars.exceptions.ShapeError: cannot take a larger sample than the total population when `with_replacement=false`\n",
    "    ## Either you'll have to remove the samples or split the dataframe yourself and only upsample the samples that doesn't have enough\n",
    "    >>> min_neg = 2\n",
    "    >>> sampling_strategy_wu2019(\n",
    "            df.filter(pl.col(DEFAULT_INVIEW_ARTICLES_COL).list.len() > (min_neg + 1)),\n",
    "            npratio=min_neg,\n",
    "            shuffle=False,\n",
    "            with_replacement=False,\n",
    "            seed=123,\n",
    "        )\n",
    "        shape: (2, 4)\n",
    "        \n",
    "         impression_id  user_id  article_ids_inview  article_ids_clicked \n",
    "         ---            ---      ---                 ---                 \n",
    "         i64            i64      list[i64]           i64                 \n",
    "        \n",
    "         1              1        [2, 4, 1]           1                   \n",
    "         1              1        [2, 4, 3]           3                   \n",
    "        \n",
    "    \"\"\"\n",
    "    df = (\n",
    "        # Step 1: Remove the positive 'article_id' from inview articles\n",
    "        df.pipe(\n",
    "            remove_positives_from_inview, inview_col=inview_col, clicked_col=clicked_col\n",
    "        )\n",
    "        # Step 2: Explode the DataFrame based on the clicked articles column\n",
    "        .explode(clicked_col)\n",
    "        # Step 3: Downsample the inview negative 'article_id' according to npratio (negative 'article_id' per positive 'article_id')\n",
    "        .pipe(\n",
    "            sample_article_ids,\n",
    "            n=npratio,\n",
    "            with_replacement=with_replacement,\n",
    "            seed=seed,\n",
    "            inview_col=inview_col,\n",
    "        )\n",
    "        # Step 4: Concatenate the clicked articles back to the inview articles as lists\n",
    "        .with_columns(pl.concat_list([inview_col, clicked_col]))\n",
    "        # Step 5: Convert clicked articles column to type List(Int):\n",
    "        .with_columns(pl.col(inview_col).list.tail(1).alias(clicked_col))\n",
    "    )\n",
    "    if shuffle:\n",
    "        df = shuffle_list_column(df, inview_col, seed)\n",
    "    return df\n",
    "\n",
    "\n",
    "def truncate_history(\n",
    "    df: pl.DataFrame,\n",
    "    column: str,\n",
    "    history_size: int,\n",
    "    padding_value: Any = None,\n",
    "    enable_warning: bool = True,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"Truncates the history of a column containing a list of items.\n",
    "\n",
    "    It is the tail of the values, i.e. the history ids should ascending order\n",
    "    because each subsequent element (original timestamp) is greater than the previous element\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): The input DataFrame.\n",
    "        column (str): The name of the column to truncate.\n",
    "        history_size (int): The maximum size of the history to retain.\n",
    "        padding_value (Any): Pad each list with specified value, ensuring\n",
    "            equal length to each element. Default is None (no padding).\n",
    "        enable_warning (bool): warn the user that history is expected in ascedings order.\n",
    "            Default is True\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: A new DataFrame with the specified column truncated.\n",
    "\n",
    "    Examples:\n",
    "    >>> df = pl.DataFrame(\n",
    "            {\"id\": [1, 2, 3], \"history\": [[\"a\", \"b\", \"c\"], [\"d\", \"e\", \"f\", \"g\"], [\"h\", \"i\"]]}\n",
    "        )\n",
    "    >>> df\n",
    "        shape: (3, 2)\n",
    "        \n",
    "         id   history           \n",
    "         ---  ---               \n",
    "         i64  list[str]         \n",
    "        \n",
    "         1    [\"a\", \"b\", \"c\"]   \n",
    "         2    [\"d\", \"e\",  \"g\"] \n",
    "         3    [\"h\", \"i\"]        \n",
    "        \n",
    "    >>> truncate_history(df, 'history', 3)\n",
    "        shape: (3, 2)\n",
    "        \n",
    "         id   history         \n",
    "         ---  ---             \n",
    "         i64  list[str]       \n",
    "        \n",
    "         1    [\"a\", \"b\", \"c\"] \n",
    "         2    [\"e\", \"f\", \"g\"] \n",
    "         3    [\"h\", \"i\"]      \n",
    "        \n",
    "    >>> truncate_history(df.lazy(), 'history', 3, '-').collect()\n",
    "        shape: (3, 2)\n",
    "        \n",
    "         id   history         \n",
    "         ---  ---             \n",
    "         i64  list[str]       \n",
    "        \n",
    "         1    [\"a\", \"b\", \"c\"] \n",
    "         2    [\"e\", \"f\", \"g\"] \n",
    "         3    [\"-\", \"h\", \"i\"] \n",
    "        \n",
    "    \"\"\"\n",
    "    if enable_warning:\n",
    "        function_name = inspect.currentframe().f_code.co_name\n",
    "        warnings.warn(f\"{function_name}: The history IDs expeced in ascending order\")\n",
    "    if padding_value is not None:\n",
    "        df = df.with_columns(\n",
    "            pl.col(column)\n",
    "            .list.reverse()\n",
    "            .list.eval(pl.element().extend_constant(padding_value, n=history_size))\n",
    "            .list.reverse()\n",
    "        )\n",
    "    return df.with_columns(pl.col(column).list.tail(history_size))\n",
    "\n",
    "\n",
    "def create_dynamic_history(\n",
    "    df: pl.DataFrame,\n",
    "    history_size: int,\n",
    "    history_col: str = \"history_dynamic\",\n",
    "    user_col: str = DEFAULT_USER_COL,\n",
    "    item_col: str = DEFAULT_ARTICLE_ID_COL,\n",
    "    timestamp_col: str = DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"Generates a dynamic history of user interactions with articles based on a given DataFrame.\n",
    "\n",
    "    Beaware, the groupby_rolling will add all the Null values, which can only be removed afterwards.\n",
    "    Unlike the 'create_fixed_history' where we first remove all the Nulls, we can only do this afterwards.\n",
    "    As a results, the 'history_size' might be set to N but after removal of Nulls it is (N-n_nulls) long.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): A Polars DataFrame with columns 'user_id', 'article_id', and 'first_page_time'.\n",
    "        history_size (int): The maximum number of previous interactions to include in the dynamic history for each user.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: A new Polars DataFrame with the same columns as the input DataFrame, plus two new columns per user:\n",
    "        - 'dynamic_article_id': a list of up to 'history_size' article IDs representing the user's previous interactions,\n",
    "            ordered from most to least recent. If there are fewer than 'history_size' previous interactions, the list\n",
    "            is padded with 'None' values.\n",
    "    Raises:\n",
    "        ValueError: If the input DataFrame does not contain columns 'user_id', 'article_id', and 'first_page_time'.\n",
    "\n",
    "    Examples:\n",
    "    >>> from ebrec.utils._constants import (\n",
    "            DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    "            DEFAULT_ARTICLE_ID_COL,\n",
    "            DEFAULT_USER_COL,\n",
    "        )\n",
    "    >>> df = pl.DataFrame(\n",
    "            {\n",
    "                DEFAULT_USER_COL: [0, 0, 0, 1, 1, 1, 0, 2],\n",
    "                DEFAULT_ARTICLE_ID_COL: [\n",
    "                    9604210,\n",
    "                    9634540,\n",
    "                    9640420,\n",
    "                    9647983,\n",
    "                    9647984,\n",
    "                    9647981,\n",
    "                    None,\n",
    "                    None,\n",
    "                ],\n",
    "                DEFAULT_IMPRESSION_TIMESTAMP_COL: [\n",
    "                    datetime.datetime(2023, 2, 18),\n",
    "                    datetime.datetime(2023, 2, 18),\n",
    "                    datetime.datetime(2023, 2, 25),\n",
    "                    datetime.datetime(2023, 2, 22),\n",
    "                    datetime.datetime(2023, 2, 21),\n",
    "                    datetime.datetime(2023, 2, 23),\n",
    "                    datetime.datetime(2023, 2, 19),\n",
    "                    datetime.datetime(2023, 2, 26),\n",
    "                ],\n",
    "            }\n",
    "        )\n",
    "    >>> create_dynamic_history(df, 3)\n",
    "        shape: (8, 4)\n",
    "        \n",
    "         user_id  article_id  impression_time      history_dynamic    \n",
    "         ---      ---         ---                  ---                \n",
    "         i64      i64         datetime[s]         list[i64]          \n",
    "        \n",
    "         0        9604210     2023-02-18 00:00:00  []                 \n",
    "         0        9634540     2023-02-18 00:00:00  [9604210]          \n",
    "         0        null        2023-02-19 00:00:00  [9604210, 9634540] \n",
    "         0        9640420     2023-02-25 00:00:00  [9604210, 9634540] \n",
    "         1        9647984     2023-02-21 00:00:00  []                 \n",
    "         1        9647983     2023-02-22 00:00:00  [9647984]          \n",
    "         1        9647981     2023-02-23 00:00:00  [9647984, 9647983] \n",
    "         2        null        2023-02-26 00:00:00  []                 \n",
    "        \n",
    "    \"\"\"\n",
    "    _check_columns_in_df(df, [user_col, timestamp_col, item_col])\n",
    "    GROUPBY_ID = generate_unique_name(df.columns, \"_groupby_id\")\n",
    "    df = df.sort([user_col, timestamp_col])\n",
    "    return (\n",
    "        df.with_columns(\n",
    "            # DYNAMIC HISTORY START\n",
    "            df.with_row_index(name=GROUPBY_ID)\n",
    "            .with_columns(pl.col([GROUPBY_ID]).cast(pl.Int64))\n",
    "            .rolling(\n",
    "                index_column=GROUPBY_ID,\n",
    "                period=f\"{history_size}i\",\n",
    "                closed=\"left\",\n",
    "                by=[user_col],\n",
    "            )\n",
    "            .agg(pl.col(item_col).alias(history_col))\n",
    "            # DYNAMIC HISTORY END\n",
    "        )\n",
    "        .pipe(drop_nulls_from_list, column=history_col)\n",
    "        .drop(GROUPBY_ID)\n",
    "    )\n",
    "\n",
    "\n",
    "def create_fixed_history(\n",
    "    df: pl.DataFrame,\n",
    "    dt_cutoff: datetime,\n",
    "    history_size: int = None,\n",
    "    history_col: str = \"history_fixed\",\n",
    "    user_col: str = DEFAULT_USER_COL,\n",
    "    item_col: str = DEFAULT_ARTICLE_ID_COL,\n",
    "    timestamp_col: str = DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Create fixed histories for each user in a dataframe of user browsing behavior.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): A dataframe with columns \"user_id\", \"first_page_time\", and \"article_id\", representing user browsing behavior.\n",
    "        dt_cutoff (datetime): A datetime object representing the cutoff time. Only browsing behavior before this time will be considered.\n",
    "        history_size (int, optional): The maximum number of previous interactions to include in the fixed history for each user (using tail). Default is None.\n",
    "            If None, all interactions are included.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: A modified dataframe with columns \"user_id\" and \"fixed_article_id\". Each row represents a user and their fixed browsing history,\n",
    "        which is a list of article IDs. The \"fixed_\" prefix is added to distinguish the fixed history from the original \"article_id\" column.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the input dataframe does not contain the required columns.\n",
    "\n",
    "    Examples:\n",
    "        >>> from ebrec.utils._constants import (\n",
    "                DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    "                DEFAULT_ARTICLE_ID_COL,\n",
    "                DEFAULT_USER_COL,\n",
    "            )\n",
    "        >>> df = pl.DataFrame(\n",
    "                {\n",
    "                    DEFAULT_USER_COL: [0, 0, 0, 1, 1, 1, 0, 2],\n",
    "                    DEFAULT_ARTICLE_ID_COL: [\n",
    "                        9604210,\n",
    "                        9634540,\n",
    "                        9640420,\n",
    "                        9647983,\n",
    "                        9647984,\n",
    "                        9647981,\n",
    "                        None,\n",
    "                        None,\n",
    "                    ],\n",
    "                    DEFAULT_IMPRESSION_TIMESTAMP_COL: [\n",
    "                        datetime.datetime(2023, 2, 18),\n",
    "                        datetime.datetime(2023, 2, 18),\n",
    "                        datetime.datetime(2023, 2, 25),\n",
    "                        datetime.datetime(2023, 2, 22),\n",
    "                        datetime.datetime(2023, 2, 21),\n",
    "                        datetime.datetime(2023, 2, 23),\n",
    "                        datetime.datetime(2023, 2, 19),\n",
    "                        datetime.datetime(2023, 2, 26),\n",
    "                    ],\n",
    "                }\n",
    "            )\n",
    "        >>> dt_cutoff = datetime.datetime(2023, 2, 24)\n",
    "        >>> create_fixed_history(df.lazy(), dt_cutoff).collect()\n",
    "            shape: (8, 4)\n",
    "            \n",
    "             user_id  article_id  impression_time      history_fixed               \n",
    "             ---      ---         ---                  ---                         \n",
    "             i64      i64         datetime[s]         list[i64]                   \n",
    "            \n",
    "             0        9604210     2023-02-18 00:00:00  [9604210, 9634540]          \n",
    "             0        9634540     2023-02-18 00:00:00  [9604210, 9634540]          \n",
    "             0        null        2023-02-19 00:00:00  [9604210, 9634540]          \n",
    "             0        9640420     2023-02-25 00:00:00  [9604210, 9634540]          \n",
    "             1        9647984     2023-02-21 00:00:00  [9647984, 9647983, 9647981] \n",
    "             1        9647983     2023-02-22 00:00:00  [9647984, 9647983, 9647981] \n",
    "             1        9647981     2023-02-23 00:00:00  [9647984, 9647983, 9647981] \n",
    "             2        null        2023-02-26 00:00:00  null                        \n",
    "            \n",
    "        >>> create_fixed_history(df.lazy(), dt_cutoff, 1).collect()\n",
    "            shape: (8, 4)\n",
    "            \n",
    "             user_id  article_id  impression_time      history_fixed \n",
    "             ---      ---         ---                  ---           \n",
    "             i64      i64         datetime[s]         list[i64]     \n",
    "            \n",
    "             0        9604210     2023-02-18 00:00:00  [9634540]     \n",
    "             0        9634540     2023-02-18 00:00:00  [9634540]     \n",
    "             0        null        2023-02-19 00:00:00  [9634540]     \n",
    "             0        9640420     2023-02-25 00:00:00  [9634540]     \n",
    "             1        9647984     2023-02-21 00:00:00  [9647981]     \n",
    "             1        9647983     2023-02-22 00:00:00  [9647981]     \n",
    "             1        9647981     2023-02-23 00:00:00  [9647981]     \n",
    "             2        null        2023-02-26 00:00:00  null          \n",
    "            \n",
    "    \"\"\"\n",
    "    _check_columns_in_df(df, [user_col, timestamp_col, item_col])\n",
    "\n",
    "    df = df.sort(user_col, timestamp_col)\n",
    "    df_history = (\n",
    "        df.select(user_col, timestamp_col, item_col)\n",
    "        .filter(pl.col(item_col).is_not_null())\n",
    "        .filter(pl.col(timestamp_col) < dt_cutoff)\n",
    "        .group_by(user_col)\n",
    "        .agg(\n",
    "            pl.col(item_col).alias(history_col),\n",
    "        )\n",
    "    )\n",
    "    if history_size is not None:\n",
    "        df_history = df_history.with_columns(\n",
    "            pl.col(history_col).list.tail(history_size)\n",
    "        )\n",
    "    return df.join(df_history, on=user_col, how=\"left\")\n",
    "\n",
    "\n",
    "def create_fixed_history_aggr_columns(\n",
    "    df: pl.DataFrame,\n",
    "    dt_cutoff: datetime,\n",
    "    history_size: int = None,\n",
    "    columns: list[str] = [],\n",
    "    suffix: str = \"_fixed\",\n",
    "    user_col: str = DEFAULT_USER_COL,\n",
    "    item_col: str = DEFAULT_ARTICLE_ID_COL,\n",
    "    timestamp_col: str = DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    This function aggregates historical data in a Polars DataFrame based on a specified cutoff datetime and user-defined columns.\n",
    "    The historical data is fixed to a given number of most recent records per user.\n",
    "\n",
    "    Parameters:\n",
    "        df (pl.DataFrame): The input Polars DataFrame OR LazyFrame.\n",
    "        dt_cutoff (datetime): The cutoff datetime for filtering the history.\n",
    "        history_size (int, optional): The number of most recent records to keep for each user.\n",
    "            If None, all history before the cutoff is kept.\n",
    "        columns (list[str], optional): List of column names to be included in the aggregation.\n",
    "            These columns are in addition to the mandatory 'user_id', 'article_id', and 'impression_timestamp'.\n",
    "        lazy_output (bool, optional): whether to output df as LazyFrame.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: A new DataFrame with the original columns and added columns for each specified column in the history.\n",
    "        Each new column contains a list of historical values.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the input dataframe does not contain the required columns.\n",
    "\n",
    "    Examples:\n",
    "        >>> from ebrec.utils._constants import (\n",
    "                DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    "                DEFAULT_ARTICLE_ID_COL,\n",
    "                DEFAULT_READ_TIME_COL,\n",
    "                DEFAULT_USER_COL,\n",
    "            )\n",
    "        >>> df = pl.DataFrame(\n",
    "                {\n",
    "                    DEFAULT_USER_COL: [0, 0, 0, 1, 1, 1, 0, 2],\n",
    "                    DEFAULT_ARTICLE_ID_COL: [\n",
    "                        9604210,\n",
    "                        9634540,\n",
    "                        9640420,\n",
    "                        9647983,\n",
    "                        9647984,\n",
    "                        9647981,\n",
    "                        None,\n",
    "                        None,\n",
    "                    ],\n",
    "                    DEFAULT_IMPRESSION_TIMESTAMP_COL: [\n",
    "                        datetime.datetime(2023, 2, 18),\n",
    "                        datetime.datetime(2023, 2, 18),\n",
    "                        datetime.datetime(2023, 2, 25),\n",
    "                        datetime.datetime(2023, 2, 22),\n",
    "                        datetime.datetime(2023, 2, 21),\n",
    "                        datetime.datetime(2023, 2, 23),\n",
    "                        datetime.datetime(2023, 2, 19),\n",
    "                        datetime.datetime(2023, 2, 26),\n",
    "                    ],\n",
    "                    DEFAULT_READ_TIME_COL: [\n",
    "                        0,\n",
    "                        2,\n",
    "                        8,\n",
    "                        13,\n",
    "                        1,\n",
    "                        1,\n",
    "                        6,\n",
    "                        1\n",
    "                    ],\n",
    "                    \"nothing\": [\n",
    "                        None,\n",
    "                        None,\n",
    "                        None,\n",
    "                        None,\n",
    "                        None,\n",
    "                        None,\n",
    "                        None,\n",
    "                        None,\n",
    "                    ],\n",
    "                }\n",
    "            )\n",
    "        >>> dt_cutoff = datetime.datetime(2023, 2, 24)\n",
    "        >>> columns = [DEFAULT_IMPRESSION_TIMESTAMP_COL, DEFAULT_READ_TIME_COL]\n",
    "        >>> create_fixed_history_aggr_columns(df.lazy(), dt_cutoff, columns=columns).collect()\n",
    "            shape: (8, 8)\n",
    "            \n",
    "             user_id  article_id  impression_time      read_time  nothing  read_time_fixed  article_id_fixed             impression_time_fixed             \n",
    "             ---      ---         ---                  ---        ---      ---              ---                          ---                               \n",
    "             i64      i64         datetime[s]         i64        null     list[i64]        list[i64]                    list[datetime[s]]                \n",
    "            \n",
    "             0        9604210     2023-02-18 00:00:00  0          null     [0, 2]           [9604210, 9634540]           [2023-02-18 00:00:00, 2023-02-18 \n",
    "             0        9634540     2023-02-18 00:00:00  2          null     [0, 2]           [9604210, 9634540]           [2023-02-18 00:00:00, 2023-02-18 \n",
    "             0        null        2023-02-19 00:00:00  6          null     [0, 2]           [9604210, 9634540]           [2023-02-18 00:00:00, 2023-02-18 \n",
    "             0        9640420     2023-02-25 00:00:00  8          null     [0, 2]           [9604210, 9634540]           [2023-02-18 00:00:00, 2023-02-18 \n",
    "             1        9647984     2023-02-21 00:00:00  1          null     [1, 13, 1]       [9647984, 9647983, 9647981]  [2023-02-21 00:00:00, 2023-02-22 \n",
    "             1        9647983     2023-02-22 00:00:00  13         null     [1, 13, 1]       [9647984, 9647983, 9647981]  [2023-02-21 00:00:00, 2023-02-22 \n",
    "             1        9647981     2023-02-23 00:00:00  1          null     [1, 13, 1]       [9647984, 9647983, 9647981]  [2023-02-21 00:00:00, 2023-02-22 \n",
    "             2        null        2023-02-26 00:00:00  1          null     null             null                         null                              \n",
    "            \n",
    "        >>> create_fixed_history_aggr_columns(df.lazy(), dt_cutoff, 1, columns=columns).collect()\n",
    "            shape: (8, 8)\n",
    "            \n",
    "             user_id  article_id  impression_time      read_time  nothing  read_time_fixed  article_id_fixed  impression_time_fixed \n",
    "             ---      ---         ---                  ---        ---      ---              ---               ---                   \n",
    "             i64      i64         datetime[s]         i64        null     list[i64]        list[i64]         list[datetime[s]]    \n",
    "            \n",
    "             0        9604210     2023-02-18 00:00:00  0          null     [2]              [9634540]         [2023-02-18 00:00:00] \n",
    "             0        9634540     2023-02-18 00:00:00  2          null     [2]              [9634540]         [2023-02-18 00:00:00] \n",
    "             0        null        2023-02-19 00:00:00  6          null     [2]              [9634540]         [2023-02-18 00:00:00] \n",
    "             0        9640420     2023-02-25 00:00:00  8          null     [2]              [9634540]         [2023-02-18 00:00:00] \n",
    "             1        9647984     2023-02-21 00:00:00  1          null     [1]              [9647981]         [2023-02-23 00:00:00] \n",
    "             1        9647983     2023-02-22 00:00:00  13         null     [1]              [9647981]         [2023-02-23 00:00:00] \n",
    "             1        9647981     2023-02-23 00:00:00  1          null     [1]              [9647981]         [2023-02-23 00:00:00] \n",
    "             2        null        2023-02-26 00:00:00  1          null     null             null              null                  \n",
    "            \n",
    "        >>> create_fixed_history_aggr_columns(df.lazy(), dt_cutoff, 1).collect()\n",
    "            shape: (8, 6)\n",
    "            \n",
    "             user_id  article_id  impression_time      read_time  nothing  article_id_fixed \n",
    "             ---      ---         ---                  ---        ---      ---              \n",
    "             i64      i64         datetime[s]         i64        null     list[i64]        \n",
    "            \n",
    "             0        9604210     2023-02-18 00:00:00  0          null     [9634540]        \n",
    "             0        9634540     2023-02-18 00:00:00  2          null     [9634540]        \n",
    "             0        null        2023-02-19 00:00:00  6          null     [9634540]        \n",
    "             0        9640420     2023-02-25 00:00:00  8          null     [9634540]        \n",
    "             1        9647984     2023-02-21 00:00:00  1          null     [9647981]        \n",
    "             1        9647983     2023-02-22 00:00:00  13         null     [9647981]        \n",
    "             1        9647981     2023-02-23 00:00:00  1          null     [9647981]        \n",
    "             2        null        2023-02-26 00:00:00  1          null     null             \n",
    "            \n",
    "        >>> create_fixed_history_aggr_columns(df.lazy(), dt_cutoff, 1).head(1).collect()\n",
    "            shape: (1, 6)\n",
    "            \n",
    "             user_id  article_id  impression_time      read_time  nothing  article_id_fixed \n",
    "             ---      ---         ---                  ---        ---      ---              \n",
    "             i64      i64         datetime[s]         i64        null     list[i64]        \n",
    "            \n",
    "             0        9604210     2023-02-18 00:00:00  0          null     [9634540]        \n",
    "            \n",
    "    \"\"\"\n",
    "    _check_columns_in_df(df, [user_col, item_col, timestamp_col] + columns)\n",
    "    aggr_columns = list(set([item_col] + columns))\n",
    "    df = df.sort(user_col, timestamp_col)\n",
    "    df_history = (\n",
    "        df.select(pl.all())\n",
    "        .filter(pl.col(item_col).is_not_null())\n",
    "        .filter(pl.col(timestamp_col) < dt_cutoff)\n",
    "        .group_by(user_col)\n",
    "        .agg(\n",
    "            pl.col(aggr_columns).suffix(suffix),\n",
    "        )\n",
    "    )\n",
    "    if history_size is not None:\n",
    "        for col in aggr_columns:\n",
    "            df_history = df_history.with_columns(\n",
    "                pl.col(col + suffix).list.tail(history_size)\n",
    "            )\n",
    "    return df.join(df_history, on=\"user_id\", how=\"left\")\n",
    "\n",
    "\n",
    "def add_prediction_scores(\n",
    "    df: pl.DataFrame,\n",
    "    scores: Iterable[float],\n",
    "    prediction_scores_col: str = \"scores\",\n",
    "    inview_col: str = DEFAULT_INVIEW_ARTICLES_COL,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds prediction scores to a DataFrame for the corresponding test predictions.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): The DataFrame to which the prediction scores will be added.\n",
    "        test_prediction (Iterable[float]): A list, array or simialr of prediction scores for the test data.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: The DataFrame with the prediction scores added.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If there is a mismatch in the lengths of the list columns.\n",
    "\n",
    "    >>> from ebrec.utils._constants import DEFAULT_INVIEW_ARTICLES_COL\n",
    "    >>> df = pl.DataFrame(\n",
    "            {\n",
    "                \"id\": [1,2],\n",
    "                DEFAULT_INVIEW_ARTICLES_COL: [\n",
    "                    [1, 2, 3],\n",
    "                    [4, 5],\n",
    "                ],\n",
    "            }\n",
    "        )\n",
    "    >>> test_prediction = [[0.3], [0.4], [0.5], [0.6], [0.7]]\n",
    "    >>> add_prediction_scores(df.lazy(), test_prediction).collect()\n",
    "        shape: (2, 3)\n",
    "        \n",
    "         id   article_ids  prediction_scores_test \n",
    "         ---  ---          ---                    \n",
    "         i64  list[i64]    list[f32]              \n",
    "        \n",
    "         1    [1, 2, 3]    [0.3, 0.4, 0.5]        \n",
    "         2    [4, 5]       [0.6, 0.7]             \n",
    "        \n",
    "    ## The input can can also be an np.array\n",
    "    >>> add_prediction_scores(df.lazy(), np.array(test_prediction)).collect()\n",
    "        shape: (2, 3)\n",
    "        \n",
    "         id   article_ids  prediction_scores_test \n",
    "         ---  ---          ---                    \n",
    "         i64  list[i64]    list[f32]              \n",
    "        \n",
    "         1    [1, 2, 3]    [0.3, 0.4, 0.5]        \n",
    "         2    [4, 5]       [0.6, 0.7]             \n",
    "        \n",
    "    \"\"\"\n",
    "    GROUPBY_ID = generate_unique_name(df.columns, \"_groupby_id\")\n",
    "    # df_preds = pl.DataFrame()\n",
    "    scores = (\n",
    "        df.lazy()\n",
    "        .select(pl.col(inview_col))\n",
    "        .with_row_index(GROUPBY_ID)\n",
    "        .explode(inview_col)\n",
    "        .with_columns(pl.Series(prediction_scores_col, scores).explode())\n",
    "        .group_by(GROUPBY_ID)\n",
    "        .agg(inview_col, prediction_scores_col)\n",
    "        .sort(GROUPBY_ID)\n",
    "        .collect()\n",
    "    )\n",
    "    return df.with_columns(scores.select(prediction_scores_col)).drop(GROUPBY_ID)\n",
    "\n",
    "\n",
    "def down_sample_on_users(\n",
    "    df: pl.DataFrame,\n",
    "    n: int,\n",
    "    user_col: str = DEFAULT_USER_COL,\n",
    "    seed: int = None,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Down-samples a DataFrame by randomly selecting up to 'n' rows per unique user.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): The input DataFrame to be down-sampled.\n",
    "        n (int): The maximum number of rows to retain per user.\n",
    "        user_col (str): The column representing user identifiers. Defaults to DEFAULT_USER_COL.\n",
    "        seed (int, optional): The random seed for reproducibility. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: A down-sampled DataFrame with at most 'n' rows per user.\n",
    "    >>> import polars as pl\n",
    "    >>> df = pl.DataFrame(\n",
    "            {\n",
    "                \"user_id\": [1, 1, 1, 2, 2, 3],\n",
    "                \"value\": [10, 20, 30, 40, 50, 60],\n",
    "            }\n",
    "        )\n",
    "    >>> down_sample_on_users(df, n=2, user_col=\"user_id\", seed=42)\n",
    "        shape: (5, 2)\n",
    "        \n",
    "         user_id  value \n",
    "         ---      ---   \n",
    "         i64      i64   \n",
    "        \n",
    "         1        10    \n",
    "         1        20    \n",
    "         2        40    \n",
    "         2        50    \n",
    "         3        60    \n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    GROUPBY_ID = generate_unique_name(df.columns, \"_groupby_id\")\n",
    "    df = df.with_row_index(GROUPBY_ID)\n",
    "\n",
    "    filter_index = (\n",
    "        df.sample(fraction=1.0, shuffle=True, seed=seed)\n",
    "        .group_by(pl.col(user_col))\n",
    "        .agg(GROUPBY_ID)\n",
    "        .with_columns(pl.col(GROUPBY_ID).list.tail(n))\n",
    "    ).select(pl.col(GROUPBY_ID).explode())\n",
    "\n",
    "    return df.filter(pl.col(GROUPBY_ID).is_in(filter_index)).drop(GROUPBY_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 84\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_article_id_to_value_mapping\u001b[39m(\n\u001b[0;32m     73\u001b[0m     df: pl\u001b[38;5;241m.\u001b[39mDataFrame,\n\u001b[0;32m     74\u001b[0m     value_col: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m     75\u001b[0m     article_col: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m DEFAULT_ARTICLE_ID_COL,\n\u001b[0;32m     76\u001b[0m ):\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m create_lookup_dict(\n\u001b[0;32m     78\u001b[0m         df\u001b[38;5;241m.\u001b[39mselect(article_col, value_col), key\u001b[38;5;241m=\u001b[39marticle_col, value\u001b[38;5;241m=\u001b[39mvalue_col\n\u001b[0;32m     79\u001b[0m     )\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_text2encoding_with_transformers\u001b[39m(\n\u001b[0;32m     83\u001b[0m     df: pl\u001b[38;5;241m.\u001b[39mDataFrame,\n\u001b[1;32m---> 84\u001b[0m     tokenizer: \u001b[43mAutoTokenizer\u001b[49m,\n\u001b[0;32m     85\u001b[0m     column: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m     86\u001b[0m     max_length: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     87\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pl\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[0;32m     88\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Converts text in a specified DataFrame column to tokens using a provided tokenizer.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m        df (pl.DataFrame): The input DataFrame containing the text column.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m        text_encode_bert-base-uncased\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     text \u001b[38;5;241m=\u001b[39m df[column]\u001b[38;5;241m.\u001b[39mto_list()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# BEHAVIORS\n",
    "DEFAULT_IMPRESSION_TIMESTAMP_COL = \"impression_time\"\n",
    "DEFAULT_IS_BEYOND_ACCURACY_COL = \"is_beyond_accuracy\"\n",
    "DEFAULT_CLICKED_ARTICLES_COL = \"article_ids_clicked\"\n",
    "DEFAULT_SCROLL_PERCENTAGE_COL = \"scroll_percentage\"\n",
    "DEFAULT_INVIEW_ARTICLES_COL = \"article_ids_inview\"\n",
    "DEFAULT_IMPRESSION_ID_COL = \"impression_id\"\n",
    "DEFAULT_IS_SUBSCRIBER_COL = \"is_subscriber\"\n",
    "DEFAULT_IS_SSO_USER_COL = \"is_sso_user\"\n",
    "DEFAULT_ARTICLE_ID_COL = \"article_id\"\n",
    "DEFAULT_SESSION_ID_COL = \"session_id\"\n",
    "DEFAULT_READ_TIME_COL = \"read_time\"\n",
    "DEFAULT_DEVICE_COL = \"device_type\"\n",
    "DEFAULT_POSTCODE_COL = \"postcode\"\n",
    "DEFAULT_GENDER_COL = \"gender\"\n",
    "DEFAULT_USER_COL = \"user_id\"\n",
    "DEFAULT_AGE_COL = \"age\"\n",
    "\n",
    "DEFAULT_NEXT_SCROLL_PERCENTAGE_COL = f\"next_{DEFAULT_SCROLL_PERCENTAGE_COL}\"\n",
    "DEFAULT_NEXT_READ_TIME_COL = f\"next_{DEFAULT_READ_TIME_COL}\"\n",
    "\n",
    "# ARTICLES\n",
    "DEFAULT_ARTICLE_MODIFIED_TIMESTAMP_COL = \"last_modified_time\"\n",
    "DEFAULT_ARTICLE_PUBLISHED_TIMESTAMP_COL = \"published_time\"\n",
    "DEFAULT_SENTIMENT_LABEL_COL = \"sentiment_label\"\n",
    "DEFAULT_SENTIMENT_SCORE_COL = \"sentiment_score\"\n",
    "DEFAULT_TOTAL_READ_TIME_COL = \"total_read_time\"\n",
    "DEFAULT_TOTAL_PAGEVIEWS_COL = \"total_pageviews\"\n",
    "DEFAULT_TOTAL_INVIEWS_COL = \"total_inviews\"\n",
    "DEFAULT_ARTICLE_TYPE_COL = \"article_type\"\n",
    "DEFAULT_CATEGORY_STR_COL = \"category_str\"\n",
    "DEFAULT_SUBCATEGORY_COL = \"subcategory\"\n",
    "DEFAULT_ENTITIES_COL = \"entity_groups\"\n",
    "DEFAULT_IMAGE_IDS_COL = \"image_ids\"\n",
    "DEFAULT_SUBTITLE_COL = \"subtitle\"\n",
    "DEFAULT_CATEGORY_COL = \"category\"\n",
    "DEFAULT_NER_COL = \"ner_clusters\"\n",
    "DEFAULT_PREMIUM_COL = \"premium\"\n",
    "DEFAULT_TOPICS_COL = \"topics\"\n",
    "DEFAULT_TITLE_COL = \"title\"\n",
    "DEFAULT_BODY_COL = \"body\"\n",
    "DEFAULT_URL_COL = \"url\"\n",
    "\n",
    "# HISTORY\n",
    "DEFAULT_HISTORY_IMPRESSION_TIMESTAMP_COL = f\"{DEFAULT_IMPRESSION_TIMESTAMP_COL}_fixed\"\n",
    "DEFAULT_HISTORY_SCROLL_PERCENTAGE_COL = f\"{DEFAULT_SCROLL_PERCENTAGE_COL}_fixed\"\n",
    "DEFAULT_HISTORY_ARTICLE_ID_COL = f\"{DEFAULT_ARTICLE_ID_COL}_fixed\"\n",
    "DEFAULT_HISTORY_READ_TIME_COL = f\"{DEFAULT_READ_TIME_COL}_fixed\"\n",
    "\n",
    "# CREATE\n",
    "DEFAULT_KNOWN_USER_COL = \"is_known_user\"\n",
    "DEFAULT_LABELS_COL = \"labels\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#from ebrec.utils._python import create_lookup_dict\n",
    "import polars as pl\n",
    "#from ebrec.utils._constants import DEFAULT_ARTICLE_ID_COL\n",
    "\n",
    "\n",
    "def load_article_id_embeddings(\n",
    "    df: pl.DataFrame, path: str, item_col: str = DEFAULT_ARTICLE_ID_COL\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"Load embeddings artifacts and join to articles on 'article_id'\n",
    "    Args:\n",
    "        path (str): Path to document embeddings\n",
    "    \"\"\"\n",
    "    return df.join(pl.read_parquet(path), on=item_col, how=\"left\")\n",
    "\n",
    "\n",
    "def create_article_id_to_value_mapping(\n",
    "    df: pl.DataFrame,\n",
    "    value_col: str,\n",
    "    article_col: str = DEFAULT_ARTICLE_ID_COL,\n",
    "):\n",
    "    return create_lookup_dict(\n",
    "        df.select(article_col, value_col), key=article_col, value=value_col\n",
    "    )\n",
    "\n",
    "\n",
    "def convert_text2encoding_with_transformers(\n",
    "    df: pl.DataFrame,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    column: str,\n",
    "    max_length: int = None,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"Converts text in a specified DataFrame column to tokens using a provided tokenizer.\n",
    "    Args:\n",
    "        df (pl.DataFrame): The input DataFrame containing the text column.\n",
    "        tokenizer (AutoTokenizer): The tokenizer to use for encoding the text. (from transformers import AutoTokenizer)\n",
    "        column (str): The name of the column containing the text.\n",
    "        max_length (int, optional): The maximum length of the encoded tokens. Defaults to None.\n",
    "    Returns:\n",
    "        pl.DataFrame: A new DataFrame with an additional column containing the encoded tokens.\n",
    "    Example:\n",
    "    >>> from transformers import AutoTokenizer\n",
    "    >>> import polars as pl\n",
    "    >>> df = pl.DataFrame({\n",
    "            'text': ['This is a test.', 'Another test string.', 'Yet another one.']\n",
    "        })\n",
    "    >>> tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    >>> encoded_df, new_column = convert_text2encoding_with_transformers(df, tokenizer, 'text', max_length=20)\n",
    "    >>> print(encoded_df)\n",
    "        shape: (3, 2)\n",
    "        \n",
    "         text                  text_encode_bert-base-uncased \n",
    "         ---                   ---                           \n",
    "         str                   list[i64]                     \n",
    "        \n",
    "         This is a test.       [2023, 2003,  0]             \n",
    "         Another test string.  [2178, 3231,  0]             \n",
    "         Yet another one.      [2664, 2178,  0]             \n",
    "        \n",
    "    >>> print(new_column)\n",
    "        text_encode_bert-base-uncased\n",
    "    \"\"\"\n",
    "    text = df[column].to_list()\n",
    "    # set columns\n",
    "    new_column = f\"{column}_encode_{tokenizer.name_or_path}\"\n",
    "    # If 'max_length' is provided then set it, else encode each string its original length\n",
    "    padding = \"max_length\" if max_length else False\n",
    "    encoded_tokens = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=False,\n",
    "        padding=padding,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )[\"input_ids\"]\n",
    "    return df.with_columns(pl.Series(new_column, encoded_tokens)), new_column\n",
    "\n",
    "\n",
    "def create_sort_based_prediction_score(\n",
    "    df: pl.DataFrame,\n",
    "    column: str,\n",
    "    desc: bool,\n",
    "    article_id_col: str = DEFAULT_ARTICLE_ID_COL,\n",
    "    prediction_score_col: str = \"prediction_score\",\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates a prediction score for each row in a Polars DataFrame based on the sorting of a specified column.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): The input DataFrame to process.\n",
    "        column (str): The name of the column to sort by and to base the prediction scores on.\n",
    "        desc (bool): Determines the sorting order. If True, sort in descending order; otherwise, in ascending order.\n",
    "        article_id_col (str, optional): The name article ID column. Defaults to \"article_id\".\n",
    "        prediction_score_col (str, optional): The name to assign to the prediction score column. Defaults to \"prediction_score\".\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: A Polars DataFrame including the original data along with the new prediction score column.\n",
    "\n",
    "    Examples:\n",
    "    >>> import polars as pl\n",
    "    >>> df = pl.DataFrame({\n",
    "            \"article_id\": [1, 2, 3, 4, 5],\n",
    "            \"views\": [100, 150, 200, 50, 300],\n",
    "        })\n",
    "    >>> create_sort_based_prediction_score(df, \"views\", True)\n",
    "        shape: (5, 3)\n",
    "        \n",
    "         article_id  views  prediction_score \n",
    "         ---         ---    ---              \n",
    "         i64         i64    f64              \n",
    "        \n",
    "         5           300    1.0              \n",
    "         3           200    0.5              \n",
    "         2           150    0.333333         \n",
    "         1           100    0.25             \n",
    "         4           50     0.2              \n",
    "        \n",
    "    \"\"\"\n",
    "    _TEMP_NAME = \"index\"\n",
    "    return (\n",
    "        (\n",
    "            df.select(article_id_col, column)\n",
    "            .sort(by=column, descending=desc)\n",
    "            .with_row_index(name=_TEMP_NAME, offset=1)\n",
    "        )\n",
    "        .with_columns((1 / pl.col(_TEMP_NAME)).alias(prediction_score_col))\n",
    "        .drop(_TEMP_NAME)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TFAutoModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_embeddings_with_transformers\u001b[39m(\n\u001b[1;32m----> 2\u001b[0m     model: \u001b[43mTFAutoModel\u001b[49m,\n\u001b[0;32m      3\u001b[0m     tokenizer: AutoTokenizer,\n\u001b[0;32m      4\u001b[0m     text_list: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m      5\u001b[0m     batch_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m      6\u001b[0m     disable_tqdm: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m      7\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m tf\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m      8\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m    Generates embeddings for a list of texts using a pre-trained transformer model.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m        tf.Tensor: A tensor containing the embeddings for the input texts.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Tokenize input texts\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TFAutoModel' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def generate_embeddings_with_transformers(\n",
    "    model: TFAutoModel,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    text_list: list[str],\n",
    "    batch_size: int = 8,\n",
    "    disable_tqdm: bool = False,\n",
    ") -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Generates embeddings for a list of texts using a pre-trained transformer model.\n",
    "\n",
    "    Args:\n",
    "        model (TFAutoModel): The pre-trained transformer model to use.\n",
    "        tokenizer (AutoTokenizer): Tokenizer for the transformer model.\n",
    "        text_list (list of str): A list of texts to generate embeddings for.\n",
    "        batch_size (int): The batch size to use for generating embeddings.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: A tensor containing the embeddings for the input texts.\n",
    "    \"\"\"\n",
    "    # Tokenize input texts\n",
    "    tokenized_text = tokenizer(\n",
    "        text_list, padding=True, truncation=True, return_tensors=\"tf\"\n",
    "    )\n",
    "\n",
    "    # Prepare TensorFlow dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (tokenized_text[\"input_ids\"], tokenized_text[\"attention_mask\"])\n",
    "    )\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    # Collect embeddings\n",
    "    embeddings = []\n",
    "    for input_ids, attention_mask in tqdm(dataset, desc=\"Encoding\", disable=disable_tqdm):\n",
    "        inputs = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        }\n",
    "        outputs = model(**inputs, training=False)\n",
    "        embeddings.append(outputs.last_hidden_state[:, 0, :])  # [CLS] token embedding\n",
    "\n",
    "    return tf.concat(embeddings, axis=0)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_name = \"xlm-roberta-base\"\n",
    "    batch_size = 8\n",
    "    text_list = [\n",
    "        \"hej med dig. Jeg er en tekst.\",\n",
    "        \"Jeg er en anden tekst, skal du spille smart?\",\n",
    "        \"oh nej..\",\n",
    "    ]\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "    model = TFAutoModel.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Generate embeddings\n",
    "    embeddings = generate_embeddings_with_transformers(\n",
    "        model, tokenizer, text_list, batch_size\n",
    "    )\n",
    "    print(embeddings.numpy())  # Convert TensorFlow tensor to NumPy array for inspection\n",
    "\n",
    "\n",
    "from typing import Iterable\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import datetime\n",
    "import zipfile\n",
    "import torch\n",
    "import time\n",
    "import json\n",
    "import yaml\n",
    "import time\n",
    "\n",
    "\n",
    "def read_json_file(path: str, verbose: bool = False) -> dict:\n",
    "    if verbose:\n",
    "        print(f\"Writing JSON: '{path}'\")\n",
    "    with open(path) as file:\n",
    "        return json.load(file)\n",
    "\n",
    "\n",
    "def write_json_file(dictionary: dict, path: str, verbose: bool = False) -> None:\n",
    "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\") as file:\n",
    "        json.dump(dictionary, file)\n",
    "    if verbose:\n",
    "        print(f\"Writing JSON: '{path}'\")\n",
    "\n",
    "\n",
    "def read_yaml_file(path: str) -> dict:\n",
    "    with open(path, \"r\") as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "\n",
    "def write_yaml_file(dictionary: dict, path: str) -> None:\n",
    "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\") as file:\n",
    "        yaml.dump(dictionary, file, default_flow_style=False)\n",
    "\n",
    "\n",
    "def rank_predictions_by_score(\n",
    "    arr: Iterable[float],\n",
    ") -> list[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Converts the prediction scores based on their ranking (1 for highest score,\n",
    "    2 for second highest, etc.), effectively ranking prediction scores for each row.\n",
    "\n",
    "    Reference:\n",
    "        https://github.com/recommenders-team/recommenders/blob/main/examples/00_quick_start/nrms_MIND.ipynb\n",
    "\n",
    "    >>> prediction_scores = [[0.2, 0.1, 0.3], [0.1, 0.2], [0.4, 0.2, 0.1, 0.3]]\n",
    "    >>> [rank_predictions_by_score(row) for row in prediction_scores]\n",
    "        [\n",
    "            array([2, 3, 1]),\n",
    "            array([2, 1]),\n",
    "            array([1, 3, 4, 2])\n",
    "        ]\n",
    "    \"\"\"\n",
    "    return np.argsort(np.argsort(arr)[::-1]) + 1\n",
    "\n",
    "\n",
    "def write_submission_file(\n",
    "    impression_ids: Iterable[int],\n",
    "    prediction_scores: Iterable[any],\n",
    "    path: Path = Path(\"predictions.txt\"),\n",
    "    rm_file: bool = True,\n",
    "    filename_zip: str = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    We align the submission file similar to MIND-format for users who are familar.\n",
    "\n",
    "    Reference:\n",
    "        https://github.com/recommenders-team/recommenders/blob/main/examples/00_quick_start/nrms_MIND.ipynb\n",
    "\n",
    "    Example:\n",
    "    >>> impression_ids = [237, 291, 320]\n",
    "    >>> prediction_scores = [[0.2, 0.1, 0.3], [0.1, 0.2], [0.4, 0.2, 0.1, 0.3]]\n",
    "    >>> write_submission_file(impression_ids, prediction_scores, path=\"predictions.txt\", rm_file=False)\n",
    "    ## Output file:\n",
    "        237 [0.2,0.1,0.3]\n",
    "        291 [0.1,0.2]\n",
    "        320 [0.4,0.2,0.1,0.3]\n",
    "    \"\"\"\n",
    "    path = Path(path)\n",
    "    with open(path, \"w\") as f:\n",
    "        for impr_index, preds in tqdm(zip(impression_ids, prediction_scores)):\n",
    "            preds = \"[\" + \",\".join([str(i) for i in preds]) + \"]\"\n",
    "            f.write(\" \".join([str(impr_index), preds]) + \"\\n\")\n",
    "    # =>\n",
    "    zip_submission_file(path=path, rm_file=rm_file, filename_zip=filename_zip)\n",
    "\n",
    "\n",
    "def read_submission_file(path: Path) -> tuple[int, any]:\n",
    "    \"\"\"\n",
    "    >>> impression_ids = [237, 291, 320]\n",
    "    >>> prediction_scores = [[0.2, 0.1, 0.3], [0.1, 0.2], [0.4, 0.2, 0.1, 0.3]]\n",
    "    >>> write_submission_file(impression_ids, prediction_scores, path=\"predictions.txt\", rm_file=False)\n",
    "    >>> read_submission_file(\"predictions.txt\")\n",
    "        (\n",
    "            [237, 291, 320],\n",
    "            [[0.2, 0.1, 0.3], [0.1, 0.2], [0.4, 0.2, 0.1, 0.3]]\n",
    "        )\n",
    "    \"\"\"\n",
    "    # Read and parse the file\n",
    "    impression_ids = []\n",
    "    prediction_scores = []\n",
    "    with open(path, \"r\") as file:\n",
    "        for line in file:\n",
    "            impression_id_str, scores_str = parse_line(line)\n",
    "            impression_ids.append(int(impression_id_str))\n",
    "            prediction_scores.append(scores_str)\n",
    "    return impression_ids, prediction_scores\n",
    "\n",
    "\n",
    "def zip_submission_file(\n",
    "    path: Path,\n",
    "    filename_zip: str = None,\n",
    "    verbose: bool = True,\n",
    "    rm_file: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Compresses a specified file into a ZIP archive within the same directory.\n",
    "\n",
    "    Args:\n",
    "        path (Path): The directory path where the file to be zipped and the resulting zip file will be located.\n",
    "        filename_input (str, optional): The name of the file to be compressed. Defaults to the path.name.\n",
    "        filename_zip (str, optional): The name of the output ZIP file. Defaults to \"prediction.zip\".\n",
    "        verbose (bool, optional): If set to True, the function will print the process details. Defaults to True.\n",
    "        rm_file (bool, optional): If set to True, the original file will be removed after compression. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        None: This function does not return any value.\n",
    "    \"\"\"\n",
    "    path = Path(path)\n",
    "    if filename_zip:\n",
    "        path_zip = path.parent.joinpath(filename_zip)\n",
    "    else:\n",
    "        path_zip = path.with_suffix(\".zip\")\n",
    "\n",
    "    if path_zip.suffix != \".zip\":\n",
    "        raise ValueError(f\"suffix for {path_zip.name} has to be '.zip'\")\n",
    "    if verbose:\n",
    "        print(f\"Zipping {path} to {path_zip}\")\n",
    "    f = zipfile.ZipFile(path_zip, \"w\", zipfile.ZIP_DEFLATED)\n",
    "    f.write(path, arcname=path.name)\n",
    "    f.close()\n",
    "    if rm_file:\n",
    "        path.unlink()\n",
    "\n",
    "\n",
    "def parse_line(l) -> tuple[str, list[float]]:\n",
    "    \"\"\"\n",
    "    Parses a single line of text into an identifier and a list of ranks.\n",
    "    \"\"\"\n",
    "    impid, ranks = l.strip(\"\\n\").split()\n",
    "    ranks = json.loads(ranks)\n",
    "    return impid, ranks\n",
    "\n",
    "\n",
    "def time_it(enable=True):\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            if enable:\n",
    "                start_time = time.time()\n",
    "            result = func(*args, **kwargs)\n",
    "            if enable:\n",
    "                end_time = time.time()\n",
    "                elapsed_time = end_time - start_time\n",
    "                print(f\"... {func.__name__} completed in {elapsed_time:.2f} seconds\")\n",
    "            return result\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return decorator\n",
    "\n",
    "\n",
    "def df_shape_time_it(enable=True):\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            #\n",
    "            if enable:\n",
    "                try:\n",
    "                    # Incase of LazyFrame, this is not possible:\n",
    "                    start_shape = args[0].shape\n",
    "                except:\n",
    "                    pass\n",
    "                start_time = time.time()\n",
    "\n",
    "            # Run function:\n",
    "            result = func(*args, **kwargs)\n",
    "\n",
    "            #\n",
    "            if enable:\n",
    "                end_time = time.time()\n",
    "                time_taken = round(end_time - start_time, 6)\n",
    "                try:\n",
    "                    # Incase of LazyFrame, this is not possible:\n",
    "                    end_shape = result.shape\n",
    "                    row_dropped_frac = round(\n",
    "                        (start_shape[0] - end_shape[0]) / start_shape[0] * 100, 2\n",
    "                    )\n",
    "                    shape_ba = f\"=> Before/After: {start_shape}/{end_shape} ({row_dropped_frac}% rows dropped)\"\n",
    "                except:\n",
    "                    shape_ba = f\"=> Before/After: NA/NA (NA% rows dropped)\"\n",
    "                print(\n",
    "                    f\"\"\"Time taken by '{func.__name__}': {time_taken} seconds\\n{shape_ba}\"\"\"\n",
    "                )\n",
    "            return result\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return decorator\n",
    "\n",
    "\n",
    "def generate_unique_name(existing_names: list[str], base_name: str = \"new_name\"):\n",
    "    \"\"\"\n",
    "    Generate a unique name based on a list of existing names.\n",
    "\n",
    "    Args:\n",
    "        existing_names (list of str): The list of existing names.\n",
    "        base_name (str): The base name to start with. Default is 'newName'.\n",
    "\n",
    "    Returns:\n",
    "        str: A unique name.\n",
    "    Example\n",
    "    >>> existing_names = ['name1', 'name2', 'newName', 'newName_1']\n",
    "    >>> generate_unique_name(existing_names, 'newName')\n",
    "        'newName_2'\n",
    "    \"\"\"\n",
    "    if base_name not in existing_names:\n",
    "        return base_name\n",
    "\n",
    "    suffix = 1\n",
    "    new_name = f\"{base_name}_{suffix}\"\n",
    "\n",
    "    while new_name in existing_names:\n",
    "        suffix += 1\n",
    "        new_name = f\"{base_name}_{suffix}\"\n",
    "\n",
    "    return new_name\n",
    "\n",
    "\n",
    "def compute_npratio(n_pos: int, n_neg: int) -> float:\n",
    "    \"\"\"\n",
    "    Similar approach as:\n",
    "        \"Neural News Recommendation with Long- and Short-term User Representations (An et al., ACL 2019)\"\n",
    "\n",
    "    Example:\n",
    "    >>> pos = 492_185\n",
    "    >>> neg = 9_224_537\n",
    "    >>> round(compute_npratio(pos, neg), 2)\n",
    "        18.74\n",
    "    \"\"\"\n",
    "    return 1 / (n_pos / n_neg)\n",
    "\n",
    "\n",
    "def strfdelta(tdelta: datetime.timedelta):\n",
    "    \"\"\"\n",
    "    Example:\n",
    "    >>> tdelta = datetime.timedelta(days=1, hours=3, minutes=42, seconds=54)\n",
    "    >>> strfdelta(tdelta)\n",
    "        '1 days 3:42:54'\n",
    "    \"\"\"\n",
    "    days = tdelta.days\n",
    "    hours, rem = divmod(tdelta.seconds, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    return f\"{days} days {hours}:{minutes}:{seconds}\"\n",
    "\n",
    "\n",
    "def str_datetime_now():\n",
    "    return datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "\n",
    "def get_object_variables(object_: object) -> dict:\n",
    "    \"\"\"\n",
    "    Example:\n",
    "    >>> class example:\n",
    "            a = 2\n",
    "            b = 3\n",
    "    >>> get_object_variables(example)\n",
    "        {'a': 2, 'b': 3}\n",
    "    \"\"\"\n",
    "    return {\n",
    "        name: value\n",
    "        for name, value in vars(object_).items()\n",
    "        if not name.startswith(\"__\") and not callable(value)\n",
    "    }\n",
    "\n",
    "\n",
    "def batch_items_generator(items: Iterable[any], batch_size: int):\n",
    "    \"\"\"\n",
    "    Generator function that chunks a list of items into batches of a specified size.\n",
    "\n",
    "    Args:\n",
    "        items (list): The list of items to be chunked.\n",
    "        batch_size (int): The number of items to include in each batch.\n",
    "\n",
    "    Yields:\n",
    "        list: A batch of items from the input list.\n",
    "\n",
    "    Examples:\n",
    "        >>> items = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "        >>> batch_size = 3\n",
    "        >>> for batch in chunk_list(items, batch_size):\n",
    "        ...     print(batch)\n",
    "        [1, 2, 3]\n",
    "        [4, 5, 6]\n",
    "        [7, 8, 9]\n",
    "        [10]\n",
    "    \"\"\"\n",
    "    for i in range(0, len(items), batch_size):\n",
    "        yield items[i : i + batch_size]\n",
    "\n",
    "\n",
    "def unnest_dictionary(dictionary, parent_key=\"\") -> dict:\n",
    "    \"\"\"\n",
    "    Unnests a dictionary by adding the key to the nested names.\n",
    "\n",
    "    Args:\n",
    "        dictionary (dict): The nested dictionary to be unnested.\n",
    "        parent_key (str, optional): The parent key to be prepended to the nested keys. Defaults to \"\".\n",
    "\n",
    "    Returns:\n",
    "        dict: The unnested dictionary where each nested key is prefixed with the parent keys, separated by dots.\n",
    "\n",
    "    Example:\n",
    "    >>> nested_dict = {\n",
    "            \"key1\": \"value1\",\n",
    "            \"key2\": {\"nested_key1\": \"nested_value1\", \"nested_key2\": \"nested_value2\"},\n",
    "            \"key3\": {\"nested_key3\": {\"deeply_nested_key\": \"deeply_nested_value\"}},\n",
    "        }\n",
    "    >>> unnest_dictionary(nested_dict)\n",
    "        {\n",
    "            \"key1\": \"value1\",\n",
    "            \"nested_key1-key2\": \"nested_value1\",\n",
    "            \"nested_key2-key2\": \"nested_value2\",\n",
    "            \"deeply_nested_key-nested_key3-key3\": \"deeply_nested_value\",\n",
    "        }\n",
    "    \"\"\"\n",
    "    unnested_dict = {}\n",
    "    for key, value in dictionary.items():\n",
    "        new_key = f\"{key}-{parent_key}\" if parent_key else key\n",
    "        if isinstance(value, dict):\n",
    "            unnested_dict.update(unnest_dictionary(value, parent_key=new_key))\n",
    "        else:\n",
    "            unnested_dict[new_key] = value\n",
    "    return unnested_dict\n",
    "\n",
    "\n",
    "def get_torch_device(use_gpu: bool = True):\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        return \"cuda:0\"\n",
    "    elif use_gpu and torch.backends.mps.is_available():\n",
    "        return \"cpu\"  # \"mps\" is not working for me..\n",
    "    else:\n",
    "        return \"cpu\"\n",
    "\n",
    "\n",
    "def convert_to_nested_list(lst, sublist_size: int):\n",
    "    \"\"\"\n",
    "    Example:\n",
    "    >>> list_ = [0, 0, 1, 1, 0, 0]\n",
    "    >>> convert_to_nested_list(list_,3)\n",
    "        [[0, 0, 1], [1, 0, 0]]\n",
    "    \"\"\"\n",
    "    nested_list = [lst[i : i + sublist_size] for i in range(0, len(lst), sublist_size)]\n",
    "    return nested_list\n",
    "\n",
    "\n",
    "def repeat_by_list_values_from_matrix(\n",
    "    input_array: np.array,\n",
    "    matrix: np.array,\n",
    "    repeats: np.array,\n",
    ") -> np.array:\n",
    "    \"\"\"\n",
    "    Example:\n",
    "        >>> input = np.array([[1, 0], [0, 0]])\n",
    "        >>> matrix = np.array([[7,8,9], [10,11,12]])\n",
    "        >>> repeats = np.array([1, 2])\n",
    "        >>> repeat_by_list_values_from_matrix(input, matrix, repeats)\n",
    "            array([[[10, 11, 12],\n",
    "                    [ 7,  8,  9]],\n",
    "                    [[ 7,  8,  9],\n",
    "                    [ 7,  8,  9]],\n",
    "                    [[ 7,  8,  9],\n",
    "                    [ 7,  8,  9]]])\n",
    "    \"\"\"\n",
    "    return np.repeat(matrix[input_array], repeats=repeats, axis=0)\n",
    "\n",
    "\n",
    "def create_lookup_dict(df: pl.DataFrame, key: str, value: str) -> dict:\n",
    "    \"\"\"\n",
    "    Creates a dictionary lookup table from a Pandas-like DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): The DataFrame from which to create the lookup table.\n",
    "        key (str): The name of the column containing the keys for the lookup table.\n",
    "        value (str): The name of the column containing the values for the lookup table.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are the values from the `key` column of the DataFrame\n",
    "            and the values are the values from the `value` column of the DataFrame.\n",
    "\n",
    "    Example:\n",
    "        >>> df = pl.DataFrame({'id': [1, 2, 3], 'name': ['Alice', 'Bob', 'Charlie']})\n",
    "        >>> create_lookup_dict(df, 'id', 'name')\n",
    "            {1: 'Alice', 2: 'Bob', 3: 'Charlie'}\n",
    "    \"\"\"\n",
    "    return dict(zip(df[key], df[value]))\n",
    "\n",
    "\n",
    "def create_lookup_objects(\n",
    "    lookup_dictionary: dict[int, np.array], unknown_representation: str\n",
    ") -> tuple[dict[int, pl.Series], np.array]:\n",
    "    \"\"\"Creates lookup objects for efficient data retrieval.\n",
    "\n",
    "    This function generates a dictionary of indexes and a matrix from the given lookup dictionary.\n",
    "    The generated lookup matrix has an additional row based on the specified unknown representation\n",
    "    which could be either zeros or the mean of the values in the lookup dictionary.\n",
    "\n",
    "    Args:\n",
    "        lookup_dictionary (dict[int, np.array]): A dictionary where keys are unique identifiers (int)\n",
    "            and values are some representations which can be any data type, commonly used for lookup operations.\n",
    "        unknown_representation (str): Specifies the method to represent unknown entries.\n",
    "            It can be either 'zeros' to represent unknowns with a row of zeros, or 'mean' to represent\n",
    "            unknowns with a row of mean values computed from the lookup dictionary.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the unknown_representation is not either 'zeros' or 'mean',\n",
    "            a ValueError will be raised.\n",
    "\n",
    "    Returns:\n",
    "        tuple[dict[int, pl.Series], np.array]: A tuple containing two items:\n",
    "            - A dictionary with the same keys as the lookup_dictionary where values are polars Series\n",
    "                objects containing a single value, which is the index of the key in the lookup dictionary.\n",
    "            - A numpy array where the rows correspond to the values in the lookup_dictionary and an\n",
    "                additional row representing unknown entries as specified by the unknown_representation argument.\n",
    "\n",
    "    Example:\n",
    "    >>> data = {\n",
    "            10: np.array([0.1, 0.2, 0.3]),\n",
    "            20: np.array([0.4, 0.5, 0.6]),\n",
    "            30: np.array([0.7, 0.8, 0.9]),\n",
    "        }\n",
    "    >>> lookup_dict, lookup_matrix = create_lookup_objects(data, \"zeros\")\n",
    "\n",
    "    >>> lookup_dict\n",
    "        {10: shape: (1,)\n",
    "            Series: '' [i64]\n",
    "            [\n",
    "                    1\n",
    "            ], 20: shape: (1,)\n",
    "            Series: '' [i64]\n",
    "            [\n",
    "                    2\n",
    "            ], 30: shape: (1,)\n",
    "            Series: '' [i64]\n",
    "            [\n",
    "                    3\n",
    "        ]}\n",
    "    >>> lookup_matrix\n",
    "        array([[0. , 0. , 0. ],\n",
    "            [0.1, 0.2, 0.3],\n",
    "            [0.4, 0.5, 0.6],\n",
    "            [0.7, 0.8, 0.9]])\n",
    "    \"\"\"\n",
    "    # MAKE LOOKUP DICTIONARY\n",
    "    lookup_indexes = {\n",
    "        id: pl.Series(\"\", [i]) for i, id in enumerate(lookup_dictionary, start=1)\n",
    "    }\n",
    "    # MAKE LOOKUP MATRIX\n",
    "    lookup_matrix = np.array(list(lookup_dictionary.values()))\n",
    "\n",
    "    if unknown_representation == \"zeros\":\n",
    "        UNKNOWN_ARRAY = np.zeros(lookup_matrix.shape[1], dtype=lookup_matrix.dtype)\n",
    "    elif unknown_representation == \"mean\":\n",
    "        UNKNOWN_ARRAY = np.mean(lookup_matrix, axis=0, dtype=lookup_matrix.dtype)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"'{unknown_representation}' is not a specified method. Can be either 'zeros' or 'mean'.\"\n",
    "        )\n",
    "\n",
    "    lookup_matrix = np.vstack([UNKNOWN_ARRAY, lookup_matrix])\n",
    "    return lookup_indexes, lookup_matrix\n",
    "\n",
    "\n",
    "def batch_items_generator(items: Iterable[any], batch_size: int):\n",
    "    \"\"\"\n",
    "    Generator function that chunks a list of items into batches of a specified size.\n",
    "\n",
    "    Args:\n",
    "        items (list): The list of items to be chunked.\n",
    "        batch_size (int): The number of items to include in each batch.\n",
    "\n",
    "    Yields:\n",
    "        list: A batch of items from the input list.\n",
    "\n",
    "    Examples:\n",
    "        >>> items = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "        >>> batch_size = 3\n",
    "        >>> for batch in chunk_list(items, batch_size):\n",
    "        ...     print(batch)\n",
    "        [1, 2, 3]\n",
    "        [4, 5, 6]\n",
    "        [7, 8, 9]\n",
    "        [10]\n",
    "    \"\"\"\n",
    "    for i in range(0, len(items), batch_size):\n",
    "        yield items[i : i + batch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
