{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 2070 SUPER, compute capability 7.5\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, Input, Dropout, Dense, BatchNormalization, TimeDistributed, Dot, Activation, Reshape\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "\n",
    "import random\n",
    "import gc\n",
    "\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history = pd.read_parquet(r\"C:\\Users\\bilba\\Downloads\\DL_Small\\history.parquet\", engine='pyarrow')\n",
    "train_behaviors = pd.read_parquet(r\"C:\\Users\\bilba\\Downloads\\DL_Small\\behaviors.parquet\", engine='pyarrow').dropna(subset=[\"article_id\"])\n",
    "\n",
    "val_history = pd.read_parquet(r\"C:\\Users\\bilba\\Downloads\\DL_Small\\validation\\history.parquet\", engine='pyarrow')\n",
    "val_behaviors = pd.read_parquet(r\"C:\\Users\\bilba\\Downloads\\DL_Small\\validation\\behaviors.parquet\", engine='pyarrow').dropna(subset=[\"article_id\"])\n",
    "\n",
    "articles_ = pd.read_parquet(r\"C:\\Users\\bilba\\Downloads\\DL_Small\\articles.parquet\", engine='pyarrow')\n",
    "\n",
    "word2vec_file = pd.read_parquet(r\"C:\\Users\\bilba\\Downloads\\Ekstra_Bladet_word2vec\\document_vector.parquet\", engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NRMSModel:\n",
    "    \"\"\"NRMS model(Neural News Recommendation with Multi-Head Self-Attention)\n",
    "    Chuhan Wu, Fangzhao Wu, Suyu Ge, Tao Qi, Yongfeng Huang,and Xing Xie, \"Neural News\n",
    "    Recommendation with Multi-Head Self-Attention\" in Proceedings of the 2019 Conference\n",
    "    on Empirical Methods in Natural Language Processing and the 9th International Joint Conference\n",
    "    on Natural Language Processing (EMNLP-IJCNLP)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hparams: dict,\n",
    "        word2vec_embedding: np.ndarray = None,\n",
    "        word_emb_dim: int = 300,\n",
    "        vocab_size: int = 32000,\n",
    "        seed: int = None,\n",
    "    ):\n",
    "        \"\"\"Initialization steps for NRMS.\"\"\"\n",
    "        self.hparams = hparams\n",
    "        self.seed = seed\n",
    "\n",
    "        # Set seed for reproducibility\n",
    "        tf.random.set_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        # Initialize the word embeddings\n",
    "        if word2vec_embedding is None:\n",
    "            initializer = GlorotUniform(seed=self.seed)\n",
    "            self.word2vec_embedding = initializer(shape=(vocab_size, word_emb_dim))\n",
    "        else:\n",
    "            self.word2vec_embedding = word2vec_embedding\n",
    "\n",
    "        # Build and compile model\n",
    "        self.model, self.scorer = self._build_graph()\n",
    "        data_loss = self._get_loss(self.hparams['loss'])\n",
    "        train_optimizer = self._get_opt(optimizer=self.hparams['optimizer'], lr=self.hparams['learning_rate'])\n",
    "        self.model.compile(loss=data_loss, optimizer=train_optimizer)\n",
    "\n",
    "    def _get_loss(self, loss: str):\n",
    "        \"\"\"Define loss function\"\"\"\n",
    "        if loss == \"cross_entropy_loss\":\n",
    "            data_loss = \"categorical_crossentropy\"\n",
    "        elif loss == \"log_loss\":\n",
    "            data_loss = \"binary_crossentropy\"\n",
    "        else:\n",
    "            raise ValueError(f\"This loss is not defined: {loss}\")\n",
    "        return data_loss\n",
    "\n",
    "    def _get_opt(self, optimizer: str, lr: float):\n",
    "        \"\"\"Define optimizer\"\"\"\n",
    "        if optimizer == \"adam\":\n",
    "            train_opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        else:\n",
    "            raise ValueError(f\"This optimizer is not defined: {optimizer}\")\n",
    "        return train_opt\n",
    "\n",
    "    def _build_graph(self):\n",
    "        \"\"\"Build NRMS model and scorer.\"\"\"\n",
    "        model, scorer = self._build_nrms()\n",
    "        return model, scorer\n",
    "\n",
    "    def _build_userencoder(self, titleencoder):\n",
    "        \"\"\"Create user encoder for NRMS with timestamps.\"\"\"\n",
    "        his_input_title = Input(shape=(self.hparams['history_size'], self.hparams['title_size']), dtype=\"int32\")\n",
    "        timestamp_input = Input(shape=(self.hparams['history_size'],), dtype=\"float16\")\n",
    "\n",
    "        # Process titles\n",
    "        click_title_presents = TimeDistributed(titleencoder)(his_input_title)\n",
    "\n",
    "        # Concatenate timestamps with title representations\n",
    "        timestamp_reshaped = tf.expand_dims(timestamp_input, axis=-1)  # Shape: (batch_size, history_size, 1)\n",
    "        click_title_with_timestamps = tf.concat([click_title_presents, timestamp_reshaped], axis=-1)\n",
    "\n",
    "        # Apply attention\n",
    "        y = SelfAttention(self.hparams['head_num'], self.hparams['head_dim'], seed=self.seed)([click_title_with_timestamps] * 3)\n",
    "        user_present = AttLayer2(self.hparams['attention_hidden_dim'], seed=self.seed)(y)\n",
    "\n",
    "        model = tf.keras.Model([his_input_title, timestamp_input], user_present, name=\"user_encoder\")\n",
    "        return model\n",
    "\n",
    "\n",
    "    def _build_newsencoder(self):\n",
    "        \"\"\"Create news encoder for NRMS.\"\"\"\n",
    "        embedding_layer = Embedding(\n",
    "            input_dim=self.word2vec_embedding.shape[0],\n",
    "            output_dim=self.word2vec_embedding.shape[1],\n",
    "            weights=[self.word2vec_embedding],\n",
    "            trainable=True,\n",
    "        )\n",
    "        sequences_input_title = Input(shape=(self.hparams['title_size'],), dtype=\"int32\")\n",
    "        embedded_sequences_title = embedding_layer(sequences_input_title)\n",
    "\n",
    "        y = Dropout(self.hparams['dropout'])(embedded_sequences_title)\n",
    "        y = SelfAttention(self.hparams['head_num'], self.hparams['head_dim'], seed=self.seed)([y, y, y])\n",
    "\n",
    "        for layer_size in self.hparams['layers']:  # Adjust layer sizes to match user encoder output\n",
    "            y = Dense(units=layer_size, activation=\"relu\")(y)\n",
    "            y = BatchNormalization()(y)\n",
    "            y = Dropout(self.hparams['dropout'])(y)\n",
    "\n",
    "        y = Dropout(self.hparams['dropout'])(y)\n",
    "        pred_title = AttLayer2(self.hparams['attention_hidden_dim'], seed=self.seed)(y)\n",
    "        model = tf.keras.Model(sequences_input_title, pred_title, name=\"news_encoder\")\n",
    "        return model\n",
    "\n",
    "    def _build_nrms(self):\n",
    "        \"\"\"Create NRMS model.\"\"\"\n",
    "        his_input_title = Input(shape=(self.hparams['history_size'], self.hparams['title_size']), dtype=\"int32\")\n",
    "        timestamp_input = Input(shape=(self.hparams['history_size'],), dtype=\"float16\")\n",
    "        pred_input_title = Input(shape=(None, self.hparams['title_size']), dtype=\"int32\")\n",
    "        pred_input_title_one = Input(shape=(1, self.hparams['title_size']), dtype=\"int32\")\n",
    "        pred_title_one_reshape = Reshape((self.hparams['title_size'],))(pred_input_title_one)\n",
    "\n",
    "        titleencoder = self._build_newsencoder()\n",
    "        self.userencoder = self._build_userencoder(titleencoder)\n",
    "        self.newsencoder = titleencoder\n",
    "\n",
    "        #user_present = self.userencoder(his_input_title)\n",
    "        user_present = self.userencoder([his_input_title, timestamp_input]) #newly added for the timestamps\n",
    "        news_present = TimeDistributed(self.newsencoder)(pred_input_title)\n",
    "        news_present_one = self.newsencoder(pred_title_one_reshape)\n",
    "\n",
    "        preds = Dot(axes=-1)([news_present, user_present])\n",
    "        preds = Activation(activation=\"softmax\")(preds)\n",
    "\n",
    "        pred_one = Dot(axes=-1)([news_present_one, user_present])\n",
    "        pred_one = Activation(activation=\"sigmoid\")(pred_one)\n",
    "\n",
    "        #model = tf.keras.Model([his_input_title, pred_input_title], preds)\n",
    "        #scorer = tf.keras.Model([his_input_title, pred_input_title_one], pred_one)\n",
    "        model = tf.keras.Model([his_input_title, timestamp_input, pred_input_title], preds)\n",
    "        scorer = tf.keras.Model([his_input_title, timestamp_input, pred_input_title_one], pred_one)\n",
    "\n",
    "        return model, scorer\n",
    "\n",
    "\n",
    "class AttLayer2(layers.Layer):\n",
    "    \"\"\"Soft alignment attention implement.\n",
    "\n",
    "    Attributes:\n",
    "        dim (int): attention hidden dim\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim=200, seed=0, **kwargs):\n",
    "        \"\"\"Initialization steps for AttLayer2.\n",
    "\n",
    "        Args:\n",
    "            dim (int): attention hidden dim\n",
    "        \"\"\"\n",
    "\n",
    "        self.dim = dim\n",
    "        self.seed = seed\n",
    "        super(AttLayer2, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"Initialization for variables in AttLayer2\n",
    "        There are there variables in AttLayer2, i.e. W, b and q.\n",
    "\n",
    "        Args:\n",
    "            input_shape (object): shape of input tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        assert len(input_shape) == 3\n",
    "        dim = self.dim\n",
    "        self.W = self.add_weight(\n",
    "            name=\"W\",\n",
    "            shape=(int(input_shape[-1]), dim),\n",
    "            initializer=keras.initializers.glorot_uniform(seed=self.seed),\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            name=\"b\",\n",
    "            shape=(dim,),\n",
    "            initializer=keras.initializers.Zeros(),\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.q = self.add_weight(\n",
    "            name=\"q\",\n",
    "            shape=(dim, 1),\n",
    "            initializer=keras.initializers.glorot_uniform(seed=self.seed),\n",
    "            trainable=True,\n",
    "        )\n",
    "        super(AttLayer2, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "    def call(self, inputs, mask=None, **kwargs):\n",
    "        \"\"\"Core implemention of soft attention\n",
    "\n",
    "        Args:\n",
    "            inputs (object): input tensor.\n",
    "\n",
    "        Returns:\n",
    "            object: weighted sum of input tensors.\n",
    "        \"\"\"\n",
    "        print(\"Inside AttLayer2 call\")\n",
    "        \n",
    "        attention = K.tanh(K.dot(inputs, self.W) + self.b)\n",
    "        attention = K.dot(attention, self.q)\n",
    "\n",
    "        attention = K.squeeze(attention, axis=2)\n",
    "\n",
    "        if mask == None:\n",
    "            attention = K.exp(attention)\n",
    "        else:\n",
    "            attention = K.exp(attention) * K.cast(mask, dtype=inputs.dtype)\n",
    "\n",
    "        attention_weight = attention / (\n",
    "            K.sum(attention, axis=-1, keepdims=True) + K.epsilon()\n",
    "        )\n",
    "\n",
    "        attention_weight = K.expand_dims(attention_weight)\n",
    "        weighted_input = inputs * attention_weight\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        \"\"\"Compte output mask value\n",
    "\n",
    "        Args:\n",
    "            input (object): input tensor.\n",
    "            input_mask: input mask\n",
    "\n",
    "        Returns:\n",
    "            object: output mask.\n",
    "        \"\"\"\n",
    "        return None\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\"Compute shape of output tensor\n",
    "\n",
    "        Args:\n",
    "            input_shape (tuple): shape of input tensor.\n",
    "\n",
    "        Returns:\n",
    "            tuple: shape of output tensor.\n",
    "        \"\"\"\n",
    "        return input_shape[0], input_shape[-1]\n",
    "    \n",
    "    def get_config(self):\n",
    "        \"\"\"Override get_config to enable saving/loading.\"\"\"\n",
    "        config = super(AttLayer2, self).get_config()\n",
    "        config.update({\n",
    "            \"dim\": self.dim,\n",
    "            \"seed\": self.seed,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "class SelfAttention(layers.Layer):\n",
    "    \"\"\"Multi-head self attention implement.\n",
    "\n",
    "    Args:\n",
    "        multiheads (int): The number of heads.\n",
    "        head_dim (object): Dimention of each head.\n",
    "        mask_right (boolean): whether to mask right words.\n",
    "\n",
    "    Returns:\n",
    "        object: Weighted sum after attention.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, multiheads, head_dim, seed=0, mask_right=False, **kwargs):\n",
    "        \"\"\"Initialization steps for AttLayer2.\n",
    "\n",
    "        Args:\n",
    "            multiheads (int): The number of heads.\n",
    "            head_dim (object): Dimention of each head.\n",
    "            mask_right (boolean): whether to mask right words.\n",
    "        \"\"\"\n",
    "\n",
    "        self.multiheads = multiheads\n",
    "        self.head_dim = head_dim\n",
    "        self.output_dim = multiheads * head_dim\n",
    "        self.mask_right = mask_right\n",
    "        self.seed = seed\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\"Compute shape of output tensor.\n",
    "\n",
    "        Returns:\n",
    "            tuple: output shape tuple.\n",
    "        \"\"\"\n",
    "\n",
    "        return (input_shape[0][0], input_shape[0][1], self.output_dim)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"Initialization for variables in SelfAttention.\n",
    "        There are three variables in SelfAttention, i.e. WQ, WK ans WV.\n",
    "        WQ is used for linear transformation of query.\n",
    "        WK is used for linear transformation of key.\n",
    "        WV is used for linear transformation of value.\n",
    "\n",
    "        Args:\n",
    "            input_shape (object): shape of input tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        self.WQ = self.add_weight(\n",
    "            name=\"WQ\",\n",
    "            shape=(int(input_shape[0][-1]), self.output_dim),\n",
    "            initializer=keras.initializers.glorot_uniform(seed=self.seed),\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.WK = self.add_weight(\n",
    "            name=\"WK\",\n",
    "            shape=(int(input_shape[1][-1]), self.output_dim),\n",
    "            initializer=keras.initializers.glorot_uniform(seed=self.seed),\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.WV = self.add_weight(\n",
    "            name=\"WV\",\n",
    "            shape=(int(input_shape[2][-1]), self.output_dim),\n",
    "            initializer=keras.initializers.glorot_uniform(seed=self.seed),\n",
    "            trainable=True,\n",
    "        )\n",
    "        super(SelfAttention, self).build(input_shape)\n",
    "\n",
    "    def Mask(self, inputs, seq_len, mode=\"add\"):\n",
    "        \"\"\"Mask operation used in multi-head self attention\n",
    "\n",
    "        Args:\n",
    "            seq_len (object): sequence length of inputs.\n",
    "            mode (str): mode of mask.\n",
    "\n",
    "        Returns:\n",
    "            object: tensors after masking.\n",
    "        \"\"\"\n",
    "\n",
    "        if seq_len is None:\n",
    "            return inputs\n",
    "        else:\n",
    "            mask = K.one_hot(indices=seq_len[:, 0], num_classes=K.shape(inputs)[1])\n",
    "            mask = 1 - K.cumsum(mask, axis=1)\n",
    "\n",
    "            for _ in range(len(inputs.shape) - 2):\n",
    "                mask = K.expand_dims(mask, 2)\n",
    "\n",
    "            if mode == \"mul\":\n",
    "                return inputs * mask\n",
    "            elif mode == \"add\":\n",
    "                return inputs - (1 - mask) * 1e12\n",
    "\n",
    "    def call(self, QKVs):\n",
    "        \"\"\"Core logic of multi-head self attention.\n",
    "\n",
    "        Args:\n",
    "            QKVs (list): inputs of multi-head self attention i.e. qeury, key and value.\n",
    "\n",
    "        Returns:\n",
    "            object: ouput tensors.\n",
    "        \"\"\"\n",
    "        if len(QKVs) == 3:\n",
    "            Q_seq, K_seq, V_seq = QKVs\n",
    "            Q_len, V_len = None, None\n",
    "        elif len(QKVs) == 5:\n",
    "            Q_seq, K_seq, V_seq, Q_len, V_len = QKVs\n",
    "        Q_seq = K.dot(Q_seq, self.WQ)\n",
    "        Q_seq = K.reshape(\n",
    "            Q_seq, shape=(-1, K.shape(Q_seq)[1], self.multiheads, self.head_dim)\n",
    "        )\n",
    "        Q_seq = K.permute_dimensions(Q_seq, pattern=(0, 2, 1, 3))\n",
    "\n",
    "        K_seq = K.dot(K_seq, self.WK)\n",
    "        K_seq = K.reshape(\n",
    "            K_seq, shape=(-1, K.shape(K_seq)[1], self.multiheads, self.head_dim)\n",
    "        )\n",
    "        K_seq = K.permute_dimensions(K_seq, pattern=(0, 2, 1, 3))\n",
    "\n",
    "        V_seq = K.dot(V_seq, self.WV)\n",
    "        V_seq = K.reshape(\n",
    "            V_seq, shape=(-1, K.shape(V_seq)[1], self.multiheads, self.head_dim)\n",
    "        )\n",
    "        V_seq = K.permute_dimensions(V_seq, pattern=(0, 2, 1, 3))\n",
    "        A = tf.matmul(Q_seq, K_seq, adjoint_a=False, adjoint_b=True) / K.sqrt(\n",
    "            K.cast(self.head_dim, dtype=Q_seq.dtype)\n",
    "        )\n",
    "\n",
    "        A = K.permute_dimensions(\n",
    "            A, pattern=(0, 3, 2, 1)\n",
    "        )  # A.shape=[batch_size,K_sequence_length,Q_sequence_length,self.multiheads]\n",
    "\n",
    "        A = self.Mask(A, V_len, \"add\")\n",
    "        A = K.permute_dimensions(A, pattern=(0, 3, 2, 1))\n",
    "\n",
    "        if self.mask_right:\n",
    "            ones = K.ones_like(A[:1, :1])\n",
    "            lower_triangular = K.tf.matrix_band_part(ones, num_lower=-1, num_upper=0)\n",
    "            mask = (ones - lower_triangular) * 1e12\n",
    "            A = A - mask\n",
    "        A = K.softmax(A)\n",
    "\n",
    "        O_seq = tf.matmul(A, V_seq, adjoint_a=True, adjoint_b=False)\n",
    "        O_seq = K.permute_dimensions(O_seq, pattern=(0, 2, 1, 3))\n",
    "\n",
    "        O_seq = K.reshape(O_seq, shape=(-1, K.shape(O_seq)[1], self.output_dim))\n",
    "        O_seq = self.Mask(O_seq, Q_len, \"mul\")\n",
    "        return O_seq\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"add multiheads, multiheads and mask_right into layer config.\n",
    "\n",
    "        Returns:\n",
    "            dict: config of SelfAttention layer.\n",
    "        \"\"\"\n",
    "        config = super(SelfAttention, self).get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"multiheads\": self.multiheads,\n",
    "                \"head_dim\": self.head_dim,\n",
    "                \"mask_right\": self.mask_right,\n",
    "            }\n",
    "        )\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.stack(word2vec_file['document_vector'].values)\n",
    "word2vec_file[\"article_length\"] = word2vec_file[\"document_vector\"].apply(len)\n",
    "max_length = word2vec_file[\"article_length\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"wordEmb_file\": word2vec_file,  # Pre-trained word embeddings\n",
    "    'layers': [max_length, max_length, max_length],\n",
    "    \"title_size\": max_length,                          # Size of news titles\n",
    "    \"npratio\": 4,                              # Negative sampling ratio\n",
    "    \"word_emb_dim\": max_length,                       # Word embedding dimension\n",
    "    \"head_num\": 4,                            # Number of attention heads\n",
    "    \"head_dim\": int(max_length/4),                            # Dimension of each attention head\n",
    "    \"attention_hidden_dim\": 50,               # Hidden layer size for attention mechanism\n",
    "    \"dropout\": 0.2,                            # Dropout rate\n",
    "    \"learning_rate\": 0.01,                    # Learning rate\n",
    "    \"epochs\": 10,                              # Number of epochs\n",
    "    \"batch_size\": 32,                          # Batch size\n",
    "    \"loss\": \"log_loss\",              # Loss function\n",
    "    \"optimizer\": \"adam\",                       # Optimizer\n",
    "    \"history_size\": 5,     #20                    # Number of past news items\n",
    "    \"num_candidate_news\": 10,                 # Number of candidate news items\n",
    "    \"vocab_size\": 500, #125541\n",
    "    \"metrics\": [\"AUC\", \"accuracy\"]\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside AttLayer2 call\n",
      "Inside AttLayer2 call\n",
      "Inside AttLayer2 call\n",
      "Inside AttLayer2 call\n",
      "Inside AttLayer2 call\n",
      "Inside AttLayer2 call\n",
      "Inside AttLayer2 call\n",
      "Inside AttLayer2 call\n",
      "Inside AttLayer2 call\n",
      "Inside AttLayer2 call\n"
     ]
    }
   ],
   "source": [
    "nrms_model = NRMSModel(hparams,word2vec_embedding=embedding_matrix,word_emb_dim=hparams[\"word_emb_dim\"],vocab_size=hparams[\"vocab_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode all list columns into individual rows\n",
    "expanded_history = train_history.explode(\n",
    "    [\"impression_time_fixed\", \"scroll_percentage_fixed\", \"article_id_fixed\", \"read_time_fixed\"],\n",
    "    ignore_index=True,\n",
    ")\n",
    "\n",
    "# Ensure impression_time_fixed is treated as datetime\n",
    "expanded_history[\"impression_time_fixed\"] = pd.to_datetime(expanded_history[\"impression_time_fixed\"])\n",
    "\n",
    "# Sort by user_id and impression_time_fixed (most recent first)\n",
    "expanded_history = expanded_history.sort_values(by=[\"user_id\", \"impression_time_fixed\"], ascending=[True, False])\n",
    "\n",
    "# Truncate to the most recent hparams[\"history_size\"] articles for each user\n",
    "truncated_history = (\n",
    "    expanded_history.groupby(\"user_id\")\n",
    "    .head(hparams[\"history_size\"])  # Keep only the top N most recent articles\n",
    "    .groupby(\"user_id\")[\"article_id_fixed\"]\n",
    "    .apply(list)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Pad histories with 0 for users with fewer than history_size articles\n",
    "truncated_history[\"article_id_fixed\"] = truncated_history[\"article_id_fixed\"].apply(\n",
    "    lambda x: x + [0] * (hparams[\"history_size\"] - len(x))\n",
    ")\n",
    "\n",
    "\n",
    "behaviors_df = train_behaviors[[\"user_id\",\"article_ids_inview\",\"article_ids_clicked\"]]\n",
    "\n",
    "articles_df = articles_[[\"article_id\",\"title\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#### starting off with behaviors_df only and then we add the rest soon\\n\\ndef generate_samples(row, k=hparams[\"num_candidate_news\"]):\\n   user_id = row[\\'user_id\\']\\n   clicked = row[\\'article_ids_clicked\\']\\n   inview = list(set(row[\\'article_ids_inview\\']) - set(clicked))  # Remove clicked from inview\\n   samples = []\\n   \\n   # Positive samples (clicked)\\n   for article_id in clicked:\\n       samples.append((user_id, article_id, 1))\\n   \\n   # Negative samples (random from inview)\\n   if inview:  # Only if there are inview articles left\\n       for article_id in clicked:\\n           n_samples = min(k, len(inview))\\n           negatives = random.sample(inview, n_samples)\\n           samples.extend((user_id, neg_id, 0) for neg_id in negatives)\\n           \\n   return samples\\n\\n# Sample 1000 rows\\nsample_size = 500\\nsample_df = behaviors_df.sample(n=sample_size, random_state=42)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#### starting off with behaviors_df only and then we add the rest soon\n",
    "\n",
    "def generate_samples(row, k=hparams[\"num_candidate_news\"]):\n",
    "   user_id = row['user_id']\n",
    "   clicked = row['article_ids_clicked']\n",
    "   inview = list(set(row['article_ids_inview']) - set(clicked))  # Remove clicked from inview\n",
    "   samples = []\n",
    "   \n",
    "   # Positive samples (clicked)\n",
    "   for article_id in clicked:\n",
    "       samples.append((user_id, article_id, 1))\n",
    "   \n",
    "   # Negative samples (random from inview)\n",
    "   if inview:  # Only if there are inview articles left\n",
    "       for article_id in clicked:\n",
    "           n_samples = min(k, len(inview))\n",
    "           negatives = random.sample(inview, n_samples)\n",
    "           samples.extend((user_id, neg_id, 0) for neg_id in negatives)\n",
    "           \n",
    "   return samples\n",
    "\n",
    "# Sample 1000 rows\n",
    "sample_size = 500\n",
    "sample_df = behaviors_df.sample(n=sample_size, random_state=42)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated and shuffled samples:\n",
      "   user_id  article_id  label\n",
      "0  1014610     9776234      0\n",
      "1  1939489     9772805      1\n",
      "2  2590015     9407487      0\n",
      "3  2452795     9773887      0\n",
      "4   164957     9773873      0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_samples(row, k=hparams[\"num_candidate_news\"]):\n",
    "    \"\"\"Generate samples with labels based on article_ids_inview and article_ids_clicked.\"\"\"\n",
    "    user_id = row['user_id']\n",
    "    clicked = row['article_ids_clicked']\n",
    "    inview = list(set(row['article_ids_inview']) - set(clicked))  # Remove clicked from inview\n",
    "    samples = []\n",
    "    \n",
    "    # Positive samples (clicked)\n",
    "    for article_id in clicked:\n",
    "        samples.append((user_id, article_id, 1))  # Label 1 for clicked articles\n",
    "    \n",
    "    # Negative samples (random from inview)\n",
    "    if inview:  # Only if there are inview articles left\n",
    "        for _ in range(len(clicked)):  # Ensure we generate the same number of negative samples as clicked\n",
    "            n_samples = min(k, len(inview))  # Take k or fewer negative samples\n",
    "            negatives = random.sample(inview, n_samples)\n",
    "            samples.extend((user_id, neg_id, 0) for neg_id in negatives)  # Label 0 for non-clicked (negative)\n",
    "            \n",
    "    return samples\n",
    "\n",
    "# Sample a subset of behaviors_df (for example, 500 samples)\n",
    "sample_size = 500\n",
    "sample_df = behaviors_df.sample(n=sample_size, random_state=42)\n",
    "\n",
    "# Apply generate_samples to each row and create the final DataFrame\n",
    "sample_df['samples'] = sample_df.apply(generate_samples, axis=1)\n",
    "samples = [item for sublist in sample_df['samples'] for item in sublist]\n",
    "\n",
    "# Convert to a DataFrame\n",
    "train_df = pd.DataFrame(samples, columns=['user_id', 'article_id', 'label'])\n",
    "\n",
    "# Shuffle the DataFrame to ensure random order\n",
    "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Check the result\n",
    "print(f\"Generated and shuffled samples:\\n{train_df.head()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 500\n",
      "First few samples: [(956287, 9769432, 1), (956287, 9770283, 0), (956287, 9771224, 0), (956287, 9771168, 0), (956287, 9120051, 0), (956287, 9771626, 0), (956287, 9771151, 0), (956287, 9765759, 0), (956287, 9771113, 0), (956287, 9176912, 0)]\n"
     ]
    }
   ],
   "source": [
    "# Check results\n",
    "print(f\"Sample size: {len(sample_df)}\")\n",
    "print(f\"First few samples: {sample_df['samples'].iloc[0][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_to_index = {article_id: idx for idx, article_id in enumerate(word2vec_file['article_id'])}\n",
    "\n",
    "train_df['title_features'] = train_df['article_id'].apply(\n",
    "    lambda article_id: article_to_index.get(article_id, 0)  # Map single article_id to index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_article_features_with_word2vec(articles_df, word2vec_file, hparams):\n",
    "    \"\"\"\n",
    "    Map article IDs to word embeddings using word2vec_file.\n",
    "\n",
    "    Args:\n",
    "        articles_df (DataFrame): DataFrame with article information (including article_id).\n",
    "        word2vec_file (DataFrame): DataFrame containing article_id and document_vector.\n",
    "        hparams (dict): Hyperparameters with 'title_size' for padding.\n",
    "\n",
    "    Returns:\n",
    "        dict: Mapping of article_id to padded document vectors.\n",
    "    \"\"\"\n",
    "    # Create a mapping from article_id to its embedding\n",
    "    article_embeddings = {\n",
    "        row['article_id']: np.array(row['document_vector'])\n",
    "        for _, row in word2vec_file.iterrows()\n",
    "    }\n",
    "    \n",
    "    article_features = {}\n",
    "    for _, row in articles_df.iterrows():\n",
    "        article_id = row['article_id']\n",
    "        embedding = article_embeddings.get(article_id, np.zeros(hparams['word_emb_dim']))\n",
    "        \n",
    "        # If the embedding is shorter than title_size, pad it\n",
    "        if len(embedding) < hparams['title_size']:\n",
    "            padded_embedding = np.pad(\n",
    "                embedding,\n",
    "                (0, hparams['title_size'] - len(embedding)),\n",
    "                mode='constant',\n",
    "                constant_values=0\n",
    "            )\n",
    "        else:\n",
    "            padded_embedding = embedding[:hparams['title_size']]  # Truncate if too long\n",
    "\n",
    "        article_features[article_id] = padded_embedding\n",
    "\n",
    "    return article_features\n",
    "\n",
    "# Example usage\n",
    "article_features = map_article_features_with_word2vec(articles_df, word2vec_file, hparams)\n",
    "\n",
    "# Map the features to the train_df\n",
    "train_df['title_features'] = train_df['article_id'].map(article_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>label</th>\n",
       "      <th>title_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1014610</td>\n",
       "      <td>9776234</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.039682284, -0.024288755, 0.09457365, 0.0297...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1939489</td>\n",
       "      <td>9772805</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.061793916, -0.0006896765, -0.0156888, 0.026...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2590015</td>\n",
       "      <td>9407487</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.038196277, -0.034157332, 0.051786672, 0.058...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2452795</td>\n",
       "      <td>9773887</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.034705974, -0.0034392518, 0.0070585194, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>164957</td>\n",
       "      <td>9773873</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.044300657, 0.019420944, 0.00011168638, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4287</th>\n",
       "      <td>632021</td>\n",
       "      <td>9779996</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.004843264, 0.026948638, 0.010931431, 0.0127...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4288</th>\n",
       "      <td>39221</td>\n",
       "      <td>9775518</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.017093746, -0.047642525, 0.09617593, 0.050...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4289</th>\n",
       "      <td>1293434</td>\n",
       "      <td>9777505</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.009022846, -0.0091448715, 0.0056488942, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4290</th>\n",
       "      <td>96059</td>\n",
       "      <td>9773962</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.04382465, -0.026772723, -0.02088443, 0.0247...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4291</th>\n",
       "      <td>2447250</td>\n",
       "      <td>9777075</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.01763526, 0.0012722586, -0.002034805, 0.018...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4292 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id  article_id  label  \\\n",
       "0     1014610     9776234      0   \n",
       "1     1939489     9772805      1   \n",
       "2     2590015     9407487      0   \n",
       "3     2452795     9773887      0   \n",
       "4      164957     9773873      0   \n",
       "...       ...         ...    ...   \n",
       "4287   632021     9779996      0   \n",
       "4288    39221     9775518      0   \n",
       "4289  1293434     9777505      0   \n",
       "4290    96059     9773962      0   \n",
       "4291  2447250     9777075      0   \n",
       "\n",
       "                                         title_features  \n",
       "0     [0.039682284, -0.024288755, 0.09457365, 0.0297...  \n",
       "1     [0.061793916, -0.0006896765, -0.0156888, 0.026...  \n",
       "2     [0.038196277, -0.034157332, 0.051786672, 0.058...  \n",
       "3     [0.034705974, -0.0034392518, 0.0070585194, 0.0...  \n",
       "4     [-0.044300657, 0.019420944, 0.00011168638, 0.0...  \n",
       "...                                                 ...  \n",
       "4287  [0.004843264, 0.026948638, 0.010931431, 0.0127...  \n",
       "4288  [-0.017093746, -0.047642525, 0.09617593, 0.050...  \n",
       "4289  [0.009022846, -0.0091448715, 0.0056488942, 0.0...  \n",
       "4290  [0.04382465, -0.026772723, -0.02088443, 0.0247...  \n",
       "4291  [0.01763526, 0.0012722586, -0.002034805, 0.018...  \n",
       "\n",
       "[4292 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_user_groups(df, max_items=hparams[\"num_candidate_news\"]):\n",
    "    \"\"\"Transform to user-level groups while preserving user_id column.\"\"\"\n",
    "    transformed_data = []\n",
    "    \n",
    "    for user_id, group in df.groupby('user_id'):\n",
    "        articles = group['article_id'].tolist()[:max_items]\n",
    "        labels = group['label'].tolist()[:max_items]\n",
    "        features = group['title_features'].tolist()[:max_items]\n",
    "        \n",
    "        # Pad if necessary\n",
    "        if len(articles) < max_items:\n",
    "            padding_length = max_items - len(articles)\n",
    "            articles.extend([0] * padding_length)\n",
    "            labels.extend([0] * padding_length)\n",
    "            features.extend([[0] * len(features[0])] * padding_length)\n",
    "            \n",
    "        transformed_data.append({\n",
    "            'user_id': user_id,\n",
    "            'article_ids': articles,\n",
    "            'labels': labels,\n",
    "            'title_features': features\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(transformed_data)\n",
    "\n",
    "# Usage\n",
    "grouped_df = transform_to_user_groups(train_df)\n",
    "\n",
    "merged_df = pd.merge(grouped_df, truncated_history, on=\"user_id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>article_ids</th>\n",
       "      <th>labels</th>\n",
       "      <th>title_features</th>\n",
       "      <th>article_id_fixed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11132</td>\n",
       "      <td>[9746360, 9767751, 9770882, 9770594, 9770798, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]</td>\n",
       "      <td>[[0.035727814, 0.012963935, 0.015877515, 0.030...</td>\n",
       "      <td>[9756202, 9758464, 9759080, 9758464, 9756202]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19915</td>\n",
       "      <td>[9775518, 9775402, 9774733, 9775347, 9758424, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[-0.017093746, -0.047642525, 0.09617593, 0.05...</td>\n",
       "      <td>[9769679, 9769572, 9768566, 9769572, 9768328]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26391</td>\n",
       "      <td>[9773350, 9769531, 9773857, 9649632, 9774297, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[0.002001684, -0.010715722, 0.067146085, 0.01...</td>\n",
       "      <td>[9770989, 9769553, 9770741, 9769306, 9763284]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34912</td>\n",
       "      <td>[9775352, 9775277, 9774557, 9482380, 9773877, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[0.031933676, 0.0013623529, 0.006322594, 0.00...</td>\n",
       "      <td>[9770882, 9770829, 9769650, 9770798, 9770867]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39221</td>\n",
       "      <td>[9754160, 9770028, 9775419, 9769679, 9775500, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[0.0190506, -0.021137215, -0.006801234, 0.023...</td>\n",
       "      <td>[9759345, 9737023, 9767344, 9763445, 9764640]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>2585224</td>\n",
       "      <td>[9726469, 9775964, 9775855, 9776184, 9776278, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[-0.011301067, -0.028080167, 0.021872781, 0.0...</td>\n",
       "      <td>[9770425, 9770178, 9768820, 9768802, 9760758]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>2587444</td>\n",
       "      <td>[9779648, 9779860, 9779427, 9779777, 9779430, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>[[0.034681696, -0.023107076, 0.014389009, 0.04...</td>\n",
       "      <td>[9770037, 9770030, 9766136, 9768177, 9767507]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>2588527</td>\n",
       "      <td>[9774120, 9770709, 9770491, 9775184, 9775562, ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[0.011999497, -0.015975023, -0.05593742, -0.0...</td>\n",
       "      <td>[9769380, 9766949, 9759345, 9766264, 9724944]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>2588560</td>\n",
       "      <td>[9774074, 9774120, 9774163, 9767665, 9772453, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[0.015317978, -0.004446184, 0.017638985, 0.07...</td>\n",
       "      <td>[9652291, 9676621, 9767697, 9769459, 9768793]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>2590015</td>\n",
       "      <td>[9407487, 9771576, 9771787, 9771919, 9771758, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>[[0.038196277, -0.034157332, 0.051786672, 0.05...</td>\n",
       "      <td>[9766627, 9694022, 9766770, 9766695, 9767027]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>462 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id                                        article_ids  \\\n",
       "0      11132  [9746360, 9767751, 9770882, 9770594, 9770798, ...   \n",
       "1      19915  [9775518, 9775402, 9774733, 9775347, 9758424, ...   \n",
       "2      26391  [9773350, 9769531, 9773857, 9649632, 9774297, ...   \n",
       "3      34912  [9775352, 9775277, 9774557, 9482380, 9773877, ...   \n",
       "4      39221  [9754160, 9770028, 9775419, 9769679, 9775500, ...   \n",
       "..       ...                                                ...   \n",
       "457  2585224  [9726469, 9775964, 9775855, 9776184, 9776278, ...   \n",
       "458  2587444  [9779648, 9779860, 9779427, 9779777, 9779430, ...   \n",
       "459  2588527  [9774120, 9770709, 9770491, 9775184, 9775562, ...   \n",
       "460  2588560  [9774074, 9774120, 9774163, 9767665, 9772453, ...   \n",
       "461  2590015  [9407487, 9771576, 9771787, 9771919, 9771758, ...   \n",
       "\n",
       "                             labels  \\\n",
       "0    [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]   \n",
       "1    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "3    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4    [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "..                              ...   \n",
       "457  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]   \n",
       "458  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]   \n",
       "459  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]   \n",
       "460  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]   \n",
       "461  [0, 1, 0, 0, 0, 0, 0, 1, 0, 0]   \n",
       "\n",
       "                                        title_features  \\\n",
       "0    [[0.035727814, 0.012963935, 0.015877515, 0.030...   \n",
       "1    [[-0.017093746, -0.047642525, 0.09617593, 0.05...   \n",
       "2    [[0.002001684, -0.010715722, 0.067146085, 0.01...   \n",
       "3    [[0.031933676, 0.0013623529, 0.006322594, 0.00...   \n",
       "4    [[0.0190506, -0.021137215, -0.006801234, 0.023...   \n",
       "..                                                 ...   \n",
       "457  [[-0.011301067, -0.028080167, 0.021872781, 0.0...   \n",
       "458  [[0.034681696, -0.023107076, 0.014389009, 0.04...   \n",
       "459  [[0.011999497, -0.015975023, -0.05593742, -0.0...   \n",
       "460  [[0.015317978, -0.004446184, 0.017638985, 0.07...   \n",
       "461  [[0.038196277, -0.034157332, 0.051786672, 0.05...   \n",
       "\n",
       "                                  article_id_fixed  \n",
       "0    [9756202, 9758464, 9759080, 9758464, 9756202]  \n",
       "1    [9769679, 9769572, 9768566, 9769572, 9768328]  \n",
       "2    [9770989, 9769553, 9770741, 9769306, 9763284]  \n",
       "3    [9770882, 9770829, 9769650, 9770798, 9770867]  \n",
       "4    [9759345, 9737023, 9767344, 9763445, 9764640]  \n",
       "..                                             ...  \n",
       "457  [9770425, 9770178, 9768820, 9768802, 9760758]  \n",
       "458  [9770037, 9770030, 9766136, 9768177, 9767507]  \n",
       "459  [9769380, 9766949, 9759345, 9766264, 9724944]  \n",
       "460  [9652291, 9676621, 9767697, 9769459, 9768793]  \n",
       "461  [9766627, 9694022, 9766770, 9766695, 9767027]  \n",
       "\n",
       "[462 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_history_embeddings(df, word2vec_file, hparams):\n",
    "    # Create a mapping from article_id to embedding vector\n",
    "    article_features = {\n",
    "        row['article_id']: row['document_vector']  # Directly use the numpy array\n",
    "        for _, row in word2vec_file.iterrows()\n",
    "    }\n",
    "    \n",
    "    history_embeddings = []\n",
    "    \n",
    "    for history in df['article_id_fixed']:\n",
    "        # Get embeddings for each article in history\n",
    "        embeddings = [\n",
    "            article_features.get(article_id, np.zeros(hparams['word_emb_dim']))  # Default to zero vector\n",
    "            for article_id in history[:hparams['history_size']]\n",
    "        ]\n",
    "        # Pad the embeddings if history is shorter than history_size\n",
    "        if len(embeddings) < hparams['history_size']:\n",
    "            padding = [np.zeros(hparams['word_emb_dim'])] * (hparams['history_size'] - len(embeddings))\n",
    "            embeddings = padding + embeddings\n",
    "        history_embeddings.append(embeddings)\n",
    "    \n",
    "    # Add history_embeddings as a new column\n",
    "    df['history_embeddings'] = history_embeddings\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "tmp = add_history_embeddings(merged_df, word2vec_file, hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_timestamps(df, articles_df, hparams):\n",
    "    # Create a mapping from article_id to article_timestamp\n",
    "    article_timestamps = {\n",
    "        row['article_id']: row['article_timestamp']\n",
    "        for _, row in articles_df.iterrows()\n",
    "    }\n",
    "\n",
    "    # Initialize history_timestamps\n",
    "    history_timestamps = []\n",
    "\n",
    "    for history in df['article_id_fixed']:\n",
    "        # Get timestamps for each article in history\n",
    "        timestamps = [\n",
    "            article_timestamps.get(article_id, 0)  # Default to 0 if timestamp is missing\n",
    "            for article_id in history[:hparams['history_size']]\n",
    "        ]\n",
    "\n",
    "        # Pad the timestamps if history is shorter than history_size\n",
    "        if len(timestamps) < hparams['history_size']:\n",
    "            timestamps = [0] * (hparams['history_size'] - len(timestamps)) + timestamps\n",
    "\n",
    "        history_timestamps.append(timestamps)\n",
    "\n",
    "    # Add history_timestamps as a new column\n",
    "    df['history_timestamps'] = history_timestamps\n",
    "    return df\n",
    "\n",
    "# Example Usage\n",
    "articles_[\"article_timestamp\"] = articles_[\"published_time\"].dt.day\n",
    "tmp = add_timestamps(tmp, articles_, hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, history_size=hparams[\"history_size\"], embedding_dim=hparams[\"word_emb_dim\"]):\n",
    "    # Initialize arrays\n",
    "    history_data = []\n",
    "    timestamp_data = []\n",
    "    target_data = []\n",
    "    label_data = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        # Prepare history embeddings\n",
    "        histories = row['history_embeddings']\n",
    "        if len(histories) > history_size:\n",
    "            histories = histories[-history_size:]\n",
    "        elif len(histories) < history_size:\n",
    "            padding = [np.zeros(embedding_dim) for _ in range(history_size - len(histories))]\n",
    "            histories = padding + histories\n",
    "\n",
    "        history_data.append(histories)  # Shape: (history_size, embedding_dim)\n",
    "\n",
    "        # Prepare timestamps\n",
    "        timestamps = row['history_timestamps']  # Ensure this column exists\n",
    "        if len(timestamps) > history_size:\n",
    "            timestamps = timestamps[-history_size:]\n",
    "        elif len(timestamps) < history_size:\n",
    "            padding = [0] * (history_size - len(timestamps))  # Pad with zeros\n",
    "            timestamps = padding + timestamps\n",
    "\n",
    "        timestamp_data.append(timestamps)  # Shape: (history_size,)\n",
    "\n",
    "        # Prepare target embeddings\n",
    "        targets = row['title_features']\n",
    "        target_data.append(targets)  # Shape: (num_candidates, embedding_dim)\n",
    "\n",
    "        # Prepare labels\n",
    "        labels = row['labels']\n",
    "        label_data.append(labels)  # Shape: (num_candidates,)\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    history_data = np.array(history_data)  # Shape: (num_samples, history_size, embedding_dim)\n",
    "    timestamp_data = np.array(timestamp_data)  # Shape: (num_samples, history_size)\n",
    "    target_data = np.array(target_data)  # Shape: (num_samples, num_candidates, embedding_dim)\n",
    "    label_data = np.array(label_data)  # Shape: (num_samples, num_candidates)\n",
    "\n",
    "    return history_data, timestamp_data, target_data, label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "history_input, timestamp_input, title_input, label_input = prepare_data(tmp, history_size=hparams[\"history_size\"], embedding_dim=hparams[\"word_emb_dim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History Input Shape: (462, 5, 300)\n",
      "timestamp Input Shape: (462, 5)\n",
      "Title Input Shape: (462, 10, 300)\n",
      "Labels Shape: (462, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"History Input Shape:\", history_input.shape)  # (num_samples, history_size, embedding_dim)\n",
    "print(\"timestamp Input Shape:\", timestamp_input.shape)\n",
    "print(\"Title Input Shape:\", title_input.shape)  # (num_samples, num_candidates, embedding_dim)\n",
    "print(\"Labels Shape:\", label_input.shape)  # (num_samples, num_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training data (90% training, 10% validation)\n",
    "#train_df, val_df = train_test_split(tmp, test_size=0.1, random_state=42)\n",
    "\n",
    "# Prepare training data\n",
    "history_input, timestamp_input,title_input, label_input = prepare_data(tmp, history_size=hparams[\"history_size\"], embedding_dim=hparams[\"word_emb_dim\"])\n",
    "\n",
    "# Prepare validation data\n",
    "val_history_input, val_timestamp_input, val_title_input, val_label_input = prepare_data(val_df, history_size=hparams[\"history_size\"], embedding_dim=hparams[\"word_emb_dim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, None, 300)]  0           []                               \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, 5, 300)]     0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 5)]          0           []                               \n",
      "                                                                                                  \n",
      " time_distributed_1 (TimeDistri  (None, None, 300)   38221900    ['input_3[0][0]']                \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      " user_encoder (Functional)      (None, 300)          38507900    ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, None)         0           ['time_distributed_1[0][0]',     \n",
      "                                                                  'user_encoder[0][0]']           \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, None)         0           ['dot[0][0]']                    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 38,507,900\n",
      "Trainable params: 38,506,100\n",
      "Non-trainable params: 1,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nrms_model.model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=hparams[\"learning_rate\"]),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"AUC\", \"accuracy\"],  # Include both AUC and accuracy here\n",
    ")\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "log_dir = \"logs\"  # Directory to save the logs\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Define EarlyStopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=3,  # Stop training after 3 epochs with no improvement\n",
    "    restore_best_weights=True,  # Restore the best weights after stopping\n",
    ")\n",
    "\n",
    "\n",
    "nrms_model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model once\n",
    "nrms_model.model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hparams[\"learning_rate\"]),\n",
    "                          loss='binary_crossentropy',\n",
    "                          metrics=[\"AUC\", \"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Inside AttLayer2 call\n",
      "Inside AttLayer2 call\n",
      "Inside AttLayer2 call\n",
      "Inside AttLayer2 call\n",
      "Inside AttLayer2 call\n",
      "Inside AttLayer2 call\n",
      "Inside AttLayer2 call\n",
      "Inside AttLayer2 call\n",
      "Inside AttLayer2 call\n",
      "Inside AttLayer2 call\n",
      "13/13 [==============================] - ETA: 0s - loss: 3.0324 - auc: 0.5056 - accuracy: 0.1301Inside AttLayer2 call\n",
      "Inside AttLayer2 call\n",
      "Inside AttLayer2 call\n",
      "Inside AttLayer2 call\n",
      "Inside AttLayer2 call\n",
      "13/13 [==============================] - 9s 416ms/step - loss: 3.0324 - auc: 0.5056 - accuracy: 0.1301 - val_loss: 127.8378 - val_auc: 0.5000 - val_accuracy: 0.1489\n",
      "Epoch 2/10\n",
      "13/13 [==============================] - 5s 361ms/step - loss: 0.6934 - auc: 0.5000 - accuracy: 0.1952 - val_loss: 1957.3829 - val_auc: 0.5000 - val_accuracy: 0.1489\n",
      "Epoch 3/10\n",
      "13/13 [==============================] - 5s 360ms/step - loss: 0.6934 - auc: 0.5000 - accuracy: 0.1952 - val_loss: 1135.3617 - val_auc: 0.5000 - val_accuracy: 0.1489\n",
      "Epoch 4/10\n",
      "13/13 [==============================] - 5s 359ms/step - loss: 0.6934 - auc: 0.5000 - accuracy: 0.1952 - val_loss: 332.7287 - val_auc: 0.5000 - val_accuracy: 0.1489\n",
      "Epoch 5/10\n",
      "13/13 [==============================] - 5s 362ms/step - loss: 0.6934 - auc: 0.5000 - accuracy: 0.1952 - val_loss: 99.9162 - val_auc: 0.5000 - val_accuracy: 0.1489\n",
      "Epoch 6/10\n",
      "13/13 [==============================] - 5s 363ms/step - loss: 0.6934 - auc: 0.5000 - accuracy: 0.1952 - val_loss: 42.9395 - val_auc: 0.5000 - val_accuracy: 0.1489\n",
      "Epoch 7/10\n",
      "13/13 [==============================] - 5s 358ms/step - loss: 0.6934 - auc: 0.5000 - accuracy: 0.1952 - val_loss: 24.0103 - val_auc: 0.5000 - val_accuracy: 0.1489\n",
      "Epoch 8/10\n",
      "13/13 [==============================] - 5s 361ms/step - loss: 0.6934 - auc: 0.5000 - accuracy: 0.1952 - val_loss: 18.5831 - val_auc: 0.5000 - val_accuracy: 0.1489\n",
      "Epoch 9/10\n",
      "13/13 [==============================] - 5s 364ms/step - loss: 0.6934 - auc: 0.5000 - accuracy: 0.1952 - val_loss: 0.6934 - val_auc: 0.5000 - val_accuracy: 0.1489\n",
      "Epoch 10/10\n",
      "13/13 [==============================] - 5s 363ms/step - loss: 0.6934 - auc: 0.5000 - accuracy: 0.1952 - val_loss: 0.6934 - val_auc: 0.5000 - val_accuracy: 0.1489\n"
     ]
    }
   ],
   "source": [
    "# Create TensorFlow datasets\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    ({\"input_1\": history_input, \"input_2\": timestamp_input, \"input_3\": title_input}, label_input)\n",
    ").shuffle(buffer_size=1000).batch(hparams[\"batch_size\"]).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    ({\"input_1\": val_history_input, \"input_2\": val_timestamp_input, \"input_3\": val_title_input}, val_label_input)\n",
    ").batch(hparams[\"batch_size\"]).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Train the model with validation\n",
    "history = nrms_model.model.fit(\n",
    "    dataset,\n",
    "    epochs=hparams[\"epochs\"],\n",
    "    verbose=1,\n",
    "    validation_data=val_dataset #,\n",
    "    #callbacks=[early_stopping, tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
